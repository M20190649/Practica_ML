 double path network sequence sequence learn encoder-decoder base sequence sequence learn make remarkable progress recent year different network architecture use encoder decoder among convolutional neural network cnn self attention network san prominent one two architecture achieve similar performance use different way encode decode context cnn use convolutional layer focus local connectivity sequence san us self-attention layer focus global semantics work propose double path network sequence sequence learn dpn-s leverage advantage model use double path information fusion encoding step develop double path architecture maintain information come different path convolutional layer self-attention layer separately effectively use encoded context develop cross attention module gating use automatically pick information need decoding step deeply integrate two path cross attention type information combine well exploit experiment show propose method significantly improve performance sequence sequence learning state-of-the-art system