 language model gated convolutional network pre-dominant approach language model date base recurrent neural network success task often link ability capture unbounded context paper develop finite context approach stacked convolution efficient since allow parallelization sequential token propose novel simplify gate mechanism outperform oord et al investigate impact key architectural decision propose approach achieve state-of-the-art wikitext- benchmark even though feature long-term dependency well competitive result google billion word benchmark model reduce latency score sentence order magnitude compare recurrent baseline knowledge first time non-recurrent approach competitive strong recurrent model large scale language task