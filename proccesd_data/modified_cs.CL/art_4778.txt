 fast simple mixture softmaxes bpe hybrid-lightrnn language generation mixture softmaxes mo show effective address expressiveness limitation softmax-based model despite known advantage mo practically seal large consumption memory computational time due need compute multiple softmaxes work set unleash power mo practical application investigate improve word cod scheme could effectively reduce vocabulary size hence relieve memory computation burden show bpe propose hybrid-lightrnn lead improve encode mechanism halve time memory consumption mo without performance loss mo achieve improvement bleu score iwslt german-to-english corpus improvement cider score image captioning moreover large wmt machine translation dataset mos-boosted transformer yield bleu score english-to-german bleu score english-to-french outperform single-softmax transformer bleu score respectively achieve state-of-the-art result wmt english-to-german task