 bilingual distribute word representation document-aligned comparable data propose new model learn bilingual word representation non-parallel document-aligned data follow recent advance word representation learning model learn dense real-valued word vector bilingual word embeddings bwes unlike prior work induce bwes heavily rely parallel sentence-aligned corpus readily available translation resource dictionary article reveal bwes may learn solely basis document-aligned comparable data without additional lexical resource syntactic information present comparison approach previous state-of-the-art model learn bilingual word representation comparable data rely framework multilingual probabilistic topic model muptm well distributional local context-counting model demonstrate utility induced bwes two semantic task bilingual lexicon extraction suggest word translation context polysemous word simple yet effective bwe-based model significantly outperform muptm-based context-counting representation model comparable data well prior bwe-based model acquire best reported result task three test language pair