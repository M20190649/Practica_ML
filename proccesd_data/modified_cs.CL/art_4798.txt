 zero-training sentence embed via orthogonal basis propose simple robust training-free approach build sentence representation inspire gram-schmidt process geometric theory build orthogonal basis subspace span word surround context sentence model semantic meaning word sentence base two aspect one relatedness word vector subspace already span contextual word word 's novel semantic meaning shall introduce new basis vector perpendicular exist subspace follow motivation develop innovative method base orthogonal basis combine pre-trained word embeddings sentence representation approach require zero training zero parameter along efficient inference performance evaluate approach downstream nlp task experimental result show model outperform exist zero-training alternative task competitive approach rely either large amount label data prolonged training time