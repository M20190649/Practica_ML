 source-side prediction neural headline generation encoder-decoder model widely use natural language generation task however model sometimes suffers repeat redundant generation miss important phrase include irrelevant entity toward solve problem propose novel source-side token prediction module method jointly estimate probability distribution source target vocabulary capture correspondence source target token experiment show propose model outperform current state-of-the-art method headline generation task additionally show method ability learn reasonable token-wise correspondence without know true alignment