 latent topic conversational model latent variable model preferred choice conversational modeling compare sequence-to-sequence seq seq model tend generate generic repetitive response despite train latent variable model remain difficult paper propose latent topic conversational model ltcm augment seq seq neural latent topic component good guide response generation make training easy neural topic component encode information source sentence build global topic distribution word consult seq seq model generation step study detail latent representation learn vanilla model ltcm extensive experiment contribute good understanding training conditional latent model language result show sample learnt latent representation ltcm generate diverse interesting response subjective human evaluation judge also confirm ltcm overall preferred option