 semi-supervised sequence model cross-view training unsupervised representation learn algorithm word vec elmo improve accuracy many supervise nlp model mainly take advantage large amount unlabeled text however supervised model learn task-specific label data main training phase therefore propose cross-view training cvt semi-supervised learning algorithm improve representation bi-lstm sentence encoder use mix label unlabeled data labeled example standard supervise learning use unlabeled example cvt teach auxiliary prediction module see restricted view input e.g. part sentence match prediction full model see whole input since auxiliary module full model share intermediate representation turn improve full model moreover show cvt particularly effective combine multi-task learning evaluate cvt five sequence tagging task machine translation dependency parsing achieve state-of-the-art result