 future word context neural network language model recently bidirectional recurrent network language model bi-rnnlms show outperform standard unidirectional recurrent neural network language model uni-rnnlms range speech recognition task indicate future word context information beyond word history useful however bi-rnnlms pose number challenge make use complete previous future word context information impact training efficiency use within lattice rescoring framework paper issue address propose novel neural network structure succeed word rnnlms su-rnnlms instead use recurrent unit capture complete future word context feedforward unit use model finite number succeed future word model train much efficiently bi-rnnlms also use lattice rescoring experimental result meeting transcription task ami show propose model consistently outperform uni-rnnlms yield slight degradation compare bi-rnnlms n-best rescoring additionally performance improvement obtain use lattice rescoring subsequent confusion network decoding