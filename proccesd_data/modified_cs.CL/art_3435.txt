 deep feed-forward sequential memory network speech synthesis bidirectional lstm blstm rnn base speech synthesis system among best parametric text-to-speech tt system term naturalness generated speech especially naturalness prosody however model complexity inference cost blstm prevents usage many runtime application meanwhile deep feed-forward sequential memory network dfsmn show consistent out-performance blstm word error rate wer runtime computation cost speech recognition task since speech synthesis also require model long-term dependency compare speech recognition paper investigate deep-fsmn dfsmn speech synthesis objective subjective experiment show compare blstm tt method dfsmn system generate synthesize speech comparable speech quality drastically reduce model complexity speech generation time