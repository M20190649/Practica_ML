 syntax-directed attention neural machine translation attention mechanism include global attention local attention play key role neural machine translation nmt global attention attends source word word prediction comparison local attention selectively look fixed-window source word however alignment weight current target word often decrease left right linear distance center align source position neglect syntax-directed distance constraint paper extend local attention syntax-distance constraint focus syntactically relate source word predicted target word thus learn effective context vector word prediction moreover propose double context nmt architecture consist global context vector syntax-directed context vector global attention provide translation performance nmt source representation experiment large-scale chinese-to-english english-to-germen translation task show propose approach achieve substantial significant improvement baseline system