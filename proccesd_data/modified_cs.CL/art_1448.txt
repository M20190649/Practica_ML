 attention-based vocabulary selection nmt decode neural machine translation nmt model usually use large target vocabulary size capture word target language vocabulary size big factor decode new sentence final softmax layer normalize possible target word address problem widely common restrict target vocabulary candidate list base source sentence usually candidate list combination external word-to-word aligner phrase table entry frequent word work propose simple yet novel approach learn candidate list directly attention layer nmt training candidate list highly optimize current nmt model need external computation candidate pool show significant decode speedup compare use entire vocabulary without lose translation quality two language pair