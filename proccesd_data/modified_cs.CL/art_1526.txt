 neural speed read via skim-rnn inspire principle speed reading introduce skim-rnn recurrent neural network rnn dynamically decide update small fraction hidden state relatively unimportant input token skim-rnn give computational advantage rnn always update entire hidden state skim-rnn use input output interface standard rnn easily use instead rnns exist model experiment show skim-rnn achieve significantly reduce computational cost without lose accuracy compare standard rnns across five different natural language task addition demonstrate trade-off accuracy speed skim-rnn dynamically control inference time stable manner analysis also show skim-rnn run single cpu offer low latency compare standard rnns gpus