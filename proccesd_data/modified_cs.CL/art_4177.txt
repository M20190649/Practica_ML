 unsupervised efficient vocabulary expansion recurrent neural network language model asr automatic speech recognition asr system recurrent neural network language model rnnlm use rescore word lattice n-best hypothesis list due expensive training rnnlm 's vocabulary set accommodate small shortlist frequent word lead suboptimal performance input speech contain many out-of-shortlist oos word effective solution increase shortlist size retrain entire network highly inefficient therefore propose efficient method expand shortlist set pretrained rnnlm without incur expensive retraining use additional training data method exploit structure rnnlm decouple three part input projection layer middle layer output projection layer specifically method expand word embed matrix projection layer keep middle layer unchanged approach functionality pretrained rnnlm correctly maintain long oos word properly model two embed space propose model oos word borrow linguistic knowledge appropriate in-shortlist word additionally propose generate list oos word expand vocabulary unsupervised manner automatically extract asr output