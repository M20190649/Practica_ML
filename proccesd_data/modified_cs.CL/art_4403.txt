 sentencepiece simple language independent subword tokenizer detokenizer neural text processing paper describe sentencepiece language-independent subword tokenizer detokenizer design neural-based text processing include neural machine translation provide open-source c python implementation subword unit exist subword segmentation tool assume input pre-tokenized word sequence sentencepiece train subword model directly raw sentence allow u make purely end-to-end language independent system perform validation experiment nmt english-japanese machine translation find possible achieve comparable accuracy direct subword training raw sentence also compare performance subword training segmentation various configuration sentencepiece available apache license http github.com google sentencepiece