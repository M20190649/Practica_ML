 efficient graph-based word sense induction distributional inclusion vector embeddings word sense induction wsi address polysemy unsupervised discovery multiple word sens resolve ambiguity downstream nlp task also make word representation interpretable paper propose accurate efficient graph-based method wsi build global non-negative vector embed basis interpretable like topic cluster basis index ego network polysemous word adopt distributional inclusion vector embeddings basis formation model avoid expensive step near neighbor search plague graph-based method without sacrifice quality sense cluster experiment three datasets show propose method produce similar good sense cluster embeddings compare previous state-of-the-art method significantly efficient