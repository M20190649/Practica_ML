 stochastic language generation dialogue use recurrent neural network convolutional sentence reranking natural language generation nlg component speak dialogue system sds usually need substantial amount handcrafting well-labeled dataset train limitation add significantly development cost make cross-domain multi-lingual dialogue system intractable moreover human language context-aware natural response directly learn data rather depend predefined syntax rule paper present statistical language generator base joint recurrent convolutional neural network structure train dialogue act-utterance pair without semantic alignment predefined grammar tree objective metric suggest new model outperform previous method experimental condition result evaluation human judge indicate produce high quality linguistically vary utterance prefer compare n-gram rule-based system