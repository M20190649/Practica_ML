 cross-lingual distillation text classification cross-lingual text classification cltc task classify document write different language taxonomy category paper present novel approach cltc build model distillation adapt extend framework originally propose model compression use soft probabilistic prediction document label-rich language induced supervisory label parallel corpus document train classifier successfully new language label training data available adversarial feature adaptation technique also apply model training reduce distribution mismatch conduct experiment two benchmark cltc datasets treat english source language german french japan chinese unlabeled target language propose approach advantageous comparable performance state-of-art method