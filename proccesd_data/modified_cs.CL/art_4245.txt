 recurrent neural network linguistic theory revisiting pinker prince past tense debate advance nlp help advance cognitive modeling examine role artificial neural network current state art many common nlp task return classic case study rumelhart mcclelland famously introduce neural architecture learn transduce english verb stem past tense form shortly thereafter pinker prince present comprehensive rebuttal many rumelhart mcclelland 's claim much force attack center empirical inadequacy rumelhart mcclelland model today however model severely outmode show encoder-decoder network architecture use modern nlp system obviate pinker prince 's criticism without require simplication past tense mapping problem suggest empirical performance modern network warrant re-examination utility linguistic cognitive modeling