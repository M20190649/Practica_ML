 recurrent neural network language model adaptation derive document vector many natural language processing nlp task document commonly model bag word use term frequency-inverse document frequency tf-idf vector one major shortcoming frequency-based tf-idf feature vector ignore word order carry syntactic semantic relationship among word document important nlp task genre classification paper propose novel distribute vector representation document simple recurrent-neural-network language model rnn-lm long short-term memory rnn language model lstm-lm first create document task lm parameter adapt document adapted parameter vectorized represent document new document vector label dv-rnn dv-lstm respectively believe new document vector capture high-level sequential information document current document representation fail capture new document vector evaluate genre classification document three corpus brown corpus bnc baby corpus artificially create penn treebank dataset classification performance compare performance tf-idf vector state-of-the-art distributed memory model paragraph vector pv-dm result show dv-lstm significantly outperform tf-idf pv-dm case combination propose document vector tf-idf pv-dm may improve performance