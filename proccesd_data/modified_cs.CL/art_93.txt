 entropy-based pruning backoff language model criterion prune parameter n-gram backoff language model develop base relative entropy original pruned model show relative entropy result prune single n-gram compute exactly efficiently backoff model relative entropy measure express relative change training set perplexity lead simple pruning criterion whereby n-grams change perplexity less threshold remove model experiment show production-quality hub lm reduce original size without increase recognition error also compare approach heuristic pruning criterion seymore rosenfeld show approach interpret approximation relative entropy criterion experimentally approach select similar set n-grams overlap exact relative entropy criterion give marginally good performance