 encoder-decoder focus-mechanism sequence labelling base spoken language understand paper investigate framework encoder-decoder attention sequence labelling base spoken language understanding introduce bidirectional long short term memory long short term memory network blstm-lstm encoder-decoder model fully utilize power deep learning sequence label task input output sequence align word word attention mechanism provide exact alignment address limitation propose novel focus mechanism encoder-decoder framework experiment standard atis dataset show blstm-lstm focus mechanism define new state-of-the-art outperform standard blstm attention base encoder-decoder experiment also show propose model robust speech recognition error