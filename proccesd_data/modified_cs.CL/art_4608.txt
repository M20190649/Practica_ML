 multi-level structure self-attentions distantly supervise relation extraction attention mechanism often use deep neural network distantly supervise relation extraction ds-re distinguish valid noisy instance however traditional -d vector attention model insufficient learning different context selection valid instance predict relationship entity pair alleviate issue propose novel multi-level structure -d matrix self-attention mechanism ds-re multi-instance learning mil framework use bidirectional recurrent neural network propose method structured word-level self-attention mechanism learn -d matrix row vector represent weight distribution different aspect instance regard two entity target mil issue structured sentence-level attention learn -d matrix row vector represent weight distribution selection different valid in-stances experiment conduct two publicly available ds-re datasets show propose framework multi-level structured self-attention mechanism significantly outperform state-of-the-art baseline term pr curve p n f measure