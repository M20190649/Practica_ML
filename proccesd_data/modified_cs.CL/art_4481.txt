 dissect contextual word embeddings architecture representation contextual word representation derive pre-trained bidirectional language model bilms recently show provide significant improvement state art wide range nlp task however many question remain model effective paper present detailed empirical study choice neural architecture e.g lstm cnn self attention influence end task accuracy qualitative property representation learn show tradeoff speed accuracy architecture learn high quality contextual representation outperform word embeddings four challenge nlp task additionally architecture learn representation vary network depth exclusively morphological base word embed layer local syntax base low contextual layer long range semantics coreference upper layer together result suggest unsupervised bilms independent architecture learn much structure language previously appreciate