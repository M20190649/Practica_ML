 neural machine translation training multi-domain scenario paper explore alternative way train neural machine translation system multi-domain scenario investigate data concatenation fine tuning model stack multi-level fine tuning data selection multi-model ensemble finding show best translation quality achieve build initial system concatenation available out-of-domain data fine-tune in-domain data model stack work best training begin furthest out-of-domain data model incrementally fine-tuned next furthest domain data selection give best result consider decent compromise training time translation quality weighted ensemble different individual model perform good data selection beneficial scenario time fine-tune already trained model