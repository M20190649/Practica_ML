 effective use pretraining natural language inference neural network excel many nlp task remain open question performance pretrained distributed word representation interaction weight initialization hyperparameters address question empirically use attention-based sequence-to-sequence model natural language inference nli specifically compare three type embeddings random pretrained glove word vec retrofit pretrained plus wordnet information show pretrained embeddings outperform random retrofit one large nli corpus experiment controlled data set shed light context retrofit embeddings useful also explore two principled approach initialize rest model parameter gaussian orthogonal show latter yield gain nli task