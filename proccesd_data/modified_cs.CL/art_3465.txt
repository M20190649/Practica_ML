 self-attention relative position representation rely entirely attention mechanism transformer introduce vaswani et al achieves state-of-the-art result machine translation contrast recurrent convolutional neural network explicitly model relative absolute position information structure instead require add representation absolute position input work present alternative approach extend self-attention mechanism efficiently consider representation relative position distance sequence element wmt english-to-german english-to-french translation task approach yield improvement bleu bleu absolute position representation respectively notably observe combine relative absolute position representation yield improvement translation quality describe efficient implementation method cast instance relation-aware self-attention mechanism generalize arbitrary graph-labeled input