 variational attention sequence-to-sequence model variational encoder-decoder ved encodes source information set random variable use neural network turn decode target data use another neural network natural language processing sequence-to-sequence seq seq model typically serve encoder-decoder network combine traditional deterministic attention mechanism variational latent space may bypass attention model thus becomes ineffective paper propose variational attention mechanism ved attention vector also model gaussian distributed random variable result two experiment show without loss quality propose method alleviate bypassing phenomenon increase diversity generated sentence