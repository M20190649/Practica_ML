 train tip transformer model article describe experiment neural machine translation use recent tensor tensor framework transformer sequence-to-sequence model vaswani et al. examine critical parameter affect final translation quality memory usage train stability training time conclude experiment set recommendation fellow researcher addition confirm general mantra data large model address scale multiple gpus provide practical tip improved training regard batch size learn rate warmup step maximum sentence length checkpoint averaging hope observation allow others get good result give particular hardware data constraint