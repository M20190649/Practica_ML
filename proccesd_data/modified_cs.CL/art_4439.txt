 attention-guided answer distillation machine reading comprehension despite current reading comprehension system achieve significant advancement promising performance often obtain cost make ensemble numerous model besides exist approach also vulnerable adversarial attack paper tackle problem leverage knowledge distillation aim transfer knowledge ensemble model single model first demonstrate vanilla knowledge distillation apply answer span prediction effective read comprehension system propose two novel approach penalize prediction confuse answer also guide training alignment information distil ensemble experiment show best student model slight drop f squad test set compare ensemble teacher run x faster inference even outperform teacher adversarial squad datasets narrativeqa benchmark