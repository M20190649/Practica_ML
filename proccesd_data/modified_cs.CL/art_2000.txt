 cseq seq cyclic sequence-to-sequence learn vanilla sequence-to-sequence learn seq seq read encode source sequence fixed-length vector suffer insufficiency model structural correspondence source target sequence instead handle insufficiency linearly weighted attention mechanism paper propose use recurrent neural network rnn alternative cseq seq-i decoding cseq seq-i cyclically feed previous decoding state back encoder initial state rnn reencodes source representation produce context vector surprisingly find introduced rnn succeed dynamically detect translationrelated source token accord partial target sequence base finding hypothesize partial target sequence act feedback improve understanding source sequence test hypothesis propose cyclic sequence-to-sequence learn cseq seq-ii differs seq seq reintroduction previous decode state encoder perform parameter share cseq seq-ii reduce parameter redundancy enhance regularization particular share weight encoder decoder two targetside word embeddings make cseq seq-ii equivalent single conditional rnn model parameter prune even good performance cseq seq-ii preserve simplicity seq seq also yield comparable promising result machine translation task experiment chinese- english english-german translation show cseq seq achieves significant consistent improvement seq seq competitive attention-based seq seq model