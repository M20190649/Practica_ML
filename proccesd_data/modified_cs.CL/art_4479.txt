 unsupervised multilingual word embeddings multilingual word embeddings mwes represent word multiple language single distributional vector space unsupervised mwe umwe method acquire multilingual embeddings without cross-lingual supervision significant advantage traditional supervised approach open many new possibility low-resource language prior art learn umwes however merely rely number independently train unsupervised bilingual word embeddings ubwes obtain multilingual embeddings method fail leverage interdependency exist among many language address shortcoming propose fully unsupervised framework learn mwes directly exploit relation language pair model substantially outperform previous approach experiment multilingual word translation cross-lingual word similarity addition model even beat supervise approach train cross-lingual resource