 recurrent dropout without memory loss paper present novel approach recurrent neural network rnn regularization differently widely adopt dropout method apply textit forward connection feed-forward architecture rnns propose drop neuron directly textit recurrent connection way cause loss long-term memory approach easy implement apply regular feed-forward dropout demonstrate effectiveness long short-term memory network popular type rnn cell experiment nlp benchmark show consistent improvement even combine conventional feed-forward dropout