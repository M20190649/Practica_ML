 query focus abstractive summarization incorporating query relevance multi-document coverage summary length constraint seq seq model query focused summarization qfs address mostly use extractive method method however produce text suffers low coherence investigate abstractive method apply qfs overcome limitation recent development neural-attention base sequence-to-sequence model lead state-of-the-art result task abstractive generic single document summarization model train end end method large amount training data address three aspect make abstractive summarization applicable qfs since training data incorporate query relevance pre-trained abstractive model b since exist abstractive model train single-document setting design iterated method embed abstractive model within multi-document requirement qfs c abstractive model adapt train generate text specific length word aim generate output different size word design way adapt target size generated summary give size ratio compare method relevance sensitive attention qfs extractive baseline various way combine abstractive model duc qfs datasets demonstrate solid improvement rouge performance