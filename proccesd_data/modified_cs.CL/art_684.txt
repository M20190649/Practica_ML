 heval yet another human evaluation metric machine translation evaluation important activity machine translation development automatic evaluation metric propose literature inadequate require one human reference translation compare output produce machine translation always give accurate result text several different translation human evaluation metric hand lack inter-annotator agreement repeatability paper propose new human evaluation metric address issue moreover metric also provide solid ground make sound assumption quality text produce machine translation