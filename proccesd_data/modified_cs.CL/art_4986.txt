 learn unsupervised word mapping maximize mean discrepancy cross-lingual word embeddings aim capture common linguistic regularity different language benefit various downstream task range machine translation transfer learning recently show embeddings effectively learn align two disjoint monolingual vector space linear transformation word mapping work focus learn word mapping without supervision signal previous work task adopt parametric metric measure distribution difference typically require sophisticated alternate optimization process either form emph minmax game intermediate emph density estimation alternate optimization process relatively hard unstable order avoid sophisticated alternate optimization propose learn unsupervised word mapping directly maximize mean discrepancy distribution transfer embedding target embedding extensive experimental result show propose model outperform competitive baseline large margin