 model order neural word embeddings scale natural language processing nlp system commonly leverage bag-of-words co-occurrence technique capture semantic syntactic word relationship result word-level distributed representation often ignore morphological information though character-level embeddings prove valuable nlp task propose new neural language model incorporate word order character order embedding model produce several vector space meaningful substructure evidence performance recent word-analogy task exceed best publish syntactic word-analogy score error margin furthermore model include several parallel train method notably allow skip-gram network billion parameter train overnight multi-core cpu x large previous large neural network