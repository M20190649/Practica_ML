 language-independent representor neural machine translation current neural machine translation nmt employ language-specific encoder represent source sentence adopt language-specific decoder generate target translation language-dependent design lead large-scale network parameter make duality parallel data underutilized address problem propose paper language-independent representor replace encoder decoder use weight sharing share representor reduce large portion network parameter also facilitate u fully explore language duality jointly train source-to-target target-to-source left-to-right right-to-left translation within multi-task learning framework experiment show propose framework obtain significant improvement conventional nmt model resource-rich low-resource translation task quarter parameter