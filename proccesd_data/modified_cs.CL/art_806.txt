 evaluate neural word representation tensor-based compositional setting provide comparative study neural word representation traditional vector space base co-occurrence count number compositional task use three different semantic space implement seven tensor-based compositional model test together simpler additive multiplicative approach task involve verb disambiguation sentence similarity check scalability additionally evaluate space use simple compositional method larger-scale task less constrained language paraphrase detection dialogue act tagging constrained task co-occurrence vector competitive although choice compositional method important larger-scale task outperform neural word embeddings show robust stable performance across task