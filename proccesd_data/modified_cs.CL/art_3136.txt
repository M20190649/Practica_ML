 word translation without parallel data state-of-the-art method learn cross-lingual word embeddings rely bilingual dictionary parallel corpus recent study show need parallel data supervision alleviate character-level information method show encouraging result par supervise counterpart limit pair language share common alphabet work show build bilingual dictionary two language without use parallel corpus align monolingual word embed space unsupervised way without use character information model even outperform exist supervise method cross-lingual task language pair experiment demonstrate method work well also distant language pair like english-russian english-chinese finally describe experiment english-esperanto low-resource language pair exist limited amount parallel data show potential impact method fully unsupervised machine translation code embeddings dictionary publicly available