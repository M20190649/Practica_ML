 slim embed layer recurrent neural language model recurrent neural language model state-of-the-art model language modeling vocabulary size large space take store model parameter become bottleneck use recurrent neural language model paper introduce simple space compression method randomly share structured parameter input output embed layer recurrent neural language model significantly reduce size model parameter still compactly represent original input output embed layer method easy implement tune experiment several data set show new method get similar perplexity bleu score result use tiny fraction parameter