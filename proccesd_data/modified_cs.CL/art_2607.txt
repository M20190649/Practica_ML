 ontology-aware token embeddings prepositional phrase attachment type-level word embeddings use set parameter represent instance word regardless context ignore inherent lexical ambiguity language instead embed semantic concept synset define wordnet represent word token particular context estimate distribution relevant semantic concept use new context-sensitive embeddings model predict prepositional phrase pp attachment jointly learn concept embeddings model parameter show use context-sensitive embeddings improve accuracy pp attachment model absolute point amount relative reduction error