 leverage sentence-level information encoder lstm semantic slot fill recurrent neural network rnn one specific architecture long short-term memory lstm widely use sequence labeling paper first enhance lstm-based sequence label explicitly model label dependency propose another enhancement incorporate global information spanning whole input sequence latter propose method encoder-labeler lstm first encode whole input sequence fixed length vector encoder lstm use encoded vector initial state another lstm sequence labeling combine method predict label sequence consider label dependency information whole input sequence experiment slot fill task essential component natural language understanding use standard atis corpus achieve state-of-the-art f -score