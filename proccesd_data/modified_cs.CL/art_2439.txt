 n-gram language model use recurrent neural network estimation investigate effective memory depth rnn model use n -gram language model lm smoothing experiment small corpus upenn treebank one million word training data k vocabulary find lstm cell dropout best model encode n -gram state compare feed-forward vanilla rnn model preserve sentence independence assumption lstm n -gram match lstm lm performance n slightly outperform n allow dependency across sentence boundary lstm -gram almost match perplexity unlimited history lstm lm lstm n -gram smoothing also desirable property improve increase n -gram order unlike katz kneser-ney back-off estimator use multinomial distribution target train instead usual one-hot target slightly beneficial low n -gram order experiment one billion word benchmark show result hold large scale lstm smooth short n -gram context provide significant advantage classic n-gram model become effective long context n depend task amount data match fully recurrent lstm model n may implication model short-format text e.g voice search query lm building lstm n -gram lm may appeal practical situation state n -gram lm succinctly represent n- byte store identity word context batch n -gram context process parallel downside n -gram context encode compute lstm discard make model expensive regular recurrent lstm lm