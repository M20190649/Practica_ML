 accelerate neural transformer via average attention network parallelizable attention network neural transformer fast train however due auto-regressive architecture self-attention decoder decoding procedure become slow alleviate issue propose average attention network alternative self-attention network decoder neural transformer average attention network consist two layer average layer model dependency previous position gating layer stack average layer enhance expressiveness propose attention network apply network decoder part neural transformer replace original target-side self-attention model mask trick dynamic programming model enable neural transformer decode sentence four time faster original version almost loss training time translation performance conduct series experiment wmt translation task different language pair obtain robust consistent speed-ups decoding