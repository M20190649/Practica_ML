 natural language generation hierarchical decode linguistic pattern natural language generation nlg critical component spoken dialogue system classic nlg divide two phase sentence planning deciding overall sentence structure surface realization determine specific word form flatten sentence structure string many simple nlg model base recurrent neural network rnn sequence-to-sequence seq seq model basically contain encoder-decoder structure nlg model generate sentence scratch jointly optimize sentence planning surface realization use simple cross entropy loss training criterion however simple encoder-decoder architecture usually suffer generate complex long sentence decoder learn grammar diction knowledge paper introduce hierarchical decoding nlg model base linguistic pattern different level show propose method outperform traditional one small model size furthermore design hierarchical decoding flexible easily-extensible various nlg system