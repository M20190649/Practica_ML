 multilingual extractive reading comprehension runtime machine translation despite recent work read comprehension rc progress mostly limit english due lack large-scale datasets language work introduce first rc system language without rc training data give target language without rc training data pivot language rc train data e.g english method leverage exist rc resource pivot language combine competitive rc model pivot language attentive neural machine translation nmt model first translate data target pivot language obtain answer use rc model pivot language finally recover correspond answer original language use soft-alignment attention score nmt model create evaluation set rc data two non-english language namely japanese french evaluate method experimental result datasets show method significantly outperform back-translation baseline state-of-the-art product-level machine translation system