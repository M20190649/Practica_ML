 redefine context window word embedding model experimental study distributional semantic model learn vector representation word context occur although choice context often take form sliding window direct influence resulting embeddings exact role model component still fully understood paper present systematic analysis context window base set four distinct hyper-parameters train continuous skip-gram model two english-language corpus various combination hyper-parameters evaluate lexical similarity analogy task notable experimental result positive impact cross-sentential context surprisingly good performance right-context window