 neural machine transliteration preliminary result machine transliteration process automatically transform script word source language target language preserve pronunciation sequence sequence learning recently emerge new paradigm supervised learning paper character-based encoder-decoder model propose consist two recurrent neural network encoder bidirectional recurrent neural network encode sequence symbol fixed-length vector representation decoder generate target sequence use attention-based recurrent neural network encoder decoder attention mechanism jointly train maximize conditional probability target sequence give source sequence experiment different datasets show propose encoder-decoder model able achieve significantly high transliteration quality traditional statistical model