 decomposable attention model natural language inference propose simple neural architecture natural language inference approach use attention decompose problem subproblems solve separately thus make trivially parallelizable stanford natural language inference snli dataset obtain state-of-the-art result almost order magnitude few parameter previous work without rely word-order information add intra-sentence attention take minimum amount order account yield improvement