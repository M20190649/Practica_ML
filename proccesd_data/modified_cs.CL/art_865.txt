 rehabilitation count-based model word vector representation recent work word representation mostly rely predictive model distribute word representation aka word embeddings train optimally predict context corresponding word tend appear model succeed capture word similarties well semantic syntactic regularity instead aim revive interest model base count present systematic study use hellinger distance extract semantic representation word co-occurence statistic large text corpus show distance give good performance word similarity analogy task proper type size context dimensionality reduction base stochastic low-rank approximation besides simple intuitive method also provide encode function use infer unseen word phrase become clear advantage compare predictive model must train new word