 linguistically-informed self-attention semantic role label current state-of-the-art semantic role label srl use deep neural network explicit linguistic feature however prior work show gold syntax tree dramatically improve srl decoding suggest possibility increased accuracy explicit modeling syntax work present linguistically-informed self-attention lisa neural network model combine multi-head self-attention multi-task learn across dependency parsing part-of-speech tagging predicate detection srl unlike previous model require significant pre-processing prepare linguistic feature lisa incorporate syntax use merely raw token input encode sequence simultaneously perform parsing predicate detection role labeling predicate syntax incorporate train one attention head attend syntactic parent token moreover high-quality syntactic parse already available beneficially inject test time without re-training srl model experiment conll- srl lisa achieve new state-of-the-art performance model use predict predicate standard word embeddings attain f absolute high previous state-of-the-art newswire f out-of-domain data nearly reduction error conll- english srl also show improvement f lisa also out-performs state-of-the-art contextually-encoded elmo word representation nearly f news f out-of-domain text