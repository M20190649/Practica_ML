 greedy search probabilistic n-gram matching neural machine translation neural machine translation nmt model usually train word-level loss use teacher force algorithm evaluate translation improperly also suffers exposure bias sequence-level training reinforcement framework mitigate problem word-level loss performance unstable due high variance gradient estimation ground present method differentiable sequence-level training objective base probabilistic n-gram match avoid reinforcement framework addition method perform greedy search training use predicted word context inference alleviate problem exposure bias experiment result nist chinese-to-english translation task show method significantly outperform reinforcement-based algorithm achieve improvement bleu point average strong baseline system