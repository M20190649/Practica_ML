 non-projective dependency parse via latent head representation lhr paper introduce novel approach base bidirectional recurrent autoencoder perform globally optimize non-projective dependency parse via semi-supervised learning syntactic analysis complete end neural process generate latent head representation lhr without algorithmic constraint linear complexity result latent syntactic structure use directly semantic task lhr transform usual dependency tree compute simple vector similarity believe model potential compete much complex state-of-the-art parsing architecture