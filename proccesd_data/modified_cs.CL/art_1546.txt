 interpretable probabilistic embeddings bridge gap topic model neural network consider probabilistic topic model recent word embed technique perspective learn hidden semantic representation inspire striking similarity two approach merge learn probabilistic embeddings online em-algorithm word co-occurrence data resulting embeddings perform par skip-gram negative sampling sgns word similarity task benefit interpretability component next learn probabilistic document embeddings outperform paragraph vec document similarity task require less memory time training finally employ multimodal additive regularization topic model artm obtain high sparsity learn embeddings modality timestamps category observe improvement word similarity performance meaningful inter-modality similarity