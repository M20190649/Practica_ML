 learn simpler language model differential state framework learn useful information across long time lag critical difficult problem temporal neural model task language modeling exist architecture address issue often complex costly train differential state framework dsf simple high-performing design unify previously introduce gated neural model dsf model maintain longer-term memory learn interpolate fast-changing data-driven representation slowly changing implicitly stable state require hardly parameter classical simple recurrent network within dsf framework new architecture present delta-rnn language modeling word character level delta-rnn outperforms popular complex architecture long short term memory lstm gated recurrent unit gru regularize perform comparably several state-of-the-art baseline subword level delta-rnn 's performance comparable complex gate architecture