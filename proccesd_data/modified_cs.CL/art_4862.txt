 self-attentive autoencoder-based universal language representation machine translation universal language representation holy grail machine translation mt thanks new neural mt approach seem good perspective towards goal paper propose new architecture base combine variational autoencoders encoder-decoders introduce interlingual loss additional training objective add force interlingual loss able train multiple encoders decoder language share common universal representation since final objective universal representation produce close result similar input sentence language propose evaluate encode sentence two different language decode latent representation language compare output preliminary result wmt turkish english task show propose architecture capable learn universal language representation simultaneously train translation direction state-of-the-art result