 gaussian mixture embeddings multiple word prototypes recently word representation increasingly focus excellent property represent word semantics previous work mainly suffer problem polysemy phenomenon address problem previous model represent word multiple distributed vector however reflect rich relation word represent word point embedded space paper propose gaussian mixture skip-gram gmsg model learn gaussian mixture embeddings word base skip-gram framework word regard gaussian mixture distribution embedded space gaussian component represent word sense since number sens varies word word propose dynamic gmsg d-gmsg model adaptively increase sense number word training experiment four benchmark show effectiveness propose model