 towards linear time neural machine translation capsule network study first investigate novel capsule network dynamic rout linear time neural machine translation nmt refer textsc capsnmt textsc capsnmt use aggregation mechanism map source sentence matrix pre-determined size apply deep lstm network decode target sequence source representation unlike previous work cite sutskever sequence store source sentence passive bottom-up way dynamic routing policy encode source sentence iterative process decide credit attribution node low high layer textsc capsnmt two core property run time linear length sequence provide flexible way select represent aggregate part-whole information source sentence wmt english-german task large wmt english-french task textsc capsnmt achieves comparable result state-of-the-art nmt system best knowledge first work capsule network empirically investigate sequence sequence problem