 improve language model use densely connect recurrent neural network paper introduce novel concept densely connect layer recurrent neural network evaluate propose architecture penn treebank language model task show obtain similar perplexity score six time few parameter compare standard stack -layer lstm model train dropout zaremba et al contrast current usage skip connection show densely connect stacked layer skip connection already yield significant perplexity reduction