 continuous space reordering model phrase-based mt bilingual sequence model improve phrase-based translation reordering overcome phrasal independence assumption handle long range reordering however due data sparsity model often fall back small context size problem previously address learn sequence generalized representation po tag word cluster paper explore alternative base neural network model concretely train neuralized version lexicalized reordering operation sequence model use feed-forward neural network result show improvement bleu point top baseline german- english english- german system also observe improvement compare system use po tag word cluster train model modify bilingual corpus integrate reorder operation allow u also train sequence-to-sequence neural mt model explicit reorder trigger motivation directly enable reorder information encoder-decoder framework otherwise rely solely attention model handle long range reordering try coarser fine-grained reordering operation however experiment yield improvement baseline neural mt system