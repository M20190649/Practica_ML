 generalized recurrent neural architecture text classification multi-task learn multi-task learning leverage potential correlation among related task extract common feature yield performance gain however previous work consider simple weak interaction thereby fail model complex correlation among three task paper propose multi-task learning architecture four type recurrent neural layer fuse information across multiple related task architecture structurally flexible considers various interaction among task regard generalized case many previous work extensive experiment five benchmark datasets text classification show model significantly improve performance related task additional information others