 distil knowledge search-based structured prediction many natural language processing task model structure prediction solve search problem paper distill ensemble multiple model train different initialization single model addition learn match ensemble 's probability output reference state also use ensemble explore search space learn encountered state exploration experimental result two typical search-based structure prediction task -- transition-based dependency parsing neural machine translation show distillation effectively improve single model 's performance final model achieve improvement la bleu score two task respectively strong baseline outperform greedy structure prediction model previous literature