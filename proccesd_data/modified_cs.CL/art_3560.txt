 learn general purpose distribute sentence representation via large scale multi-task learn lot recent success natural language processing nlp drive distributed vector representation word train large amount text unsupervised manner representation typically use general purpose feature word across range nlp problem however extend success learn representation sequence word sentence remain open problem recent work explore unsupervised well supervise learn technique different train objective learn general purpose fixed-length sentence representation work present simple effective multi-task learning framework sentence representation combine inductive bias diverse train objective single model train model several data source multiple train objective million sentence extensive experiment demonstrate share single recurrent sentence encoder across weakly relate task lead consistent improvement previous method present substantial improvement context transfer learning low-resource setting use learned general-purpose representation