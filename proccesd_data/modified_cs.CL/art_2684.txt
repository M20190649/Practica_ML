 multilingual hierarchical attention network document classification hierarchical attention network recently achieve remarkable performance document classification give language however multilingual document collection consider train model separately language entail linear parameter growth lack cross-language transfer learn single multilingual model few parameter therefore challenging potentially beneficial objective end propose multilingual hierarchical attention network learn document structure share encoders share attention mechanism across language use multi-task learning align semantic space input evaluate propose model multilingual document classification disjoint label set large dataset provide k news document language k label multilingual model outperform monolingual one low-resource well full-resource setting use few parameter thus confirm computational efficiency utility cross-language transfer