 recurrent neural network grammar learn syntax recurrent neural network grammar rnng recently propose probabilistic generative model family natural language show state-of-the-art language modeling parse performance investigate information learn linguistic perspective various ablation model data augment model attention mechanism ga-rnng enable close inspection find explicit modeling composition crucial achieve best performance attention mechanism find headedness play central role phrasal representation model 's latent attention largely agree prediction make hand-crafted head rule albeit important difference train grammar without nonterminal label find phrasal representation depend minimally nonterminals provide support endocentricity hypothesis