 attention-based word-level interaction model relation detection knowledge base question answer relation detection play crucial role knowledge base question answer kbqa high variance relation expression question traditional deep learning method follow encoding-comparing paradigm question candidate relation represent vector compare semantic similarity max- average- pooling operation compress sequence word fixed-dimensional vector become bottleneck information paper propose learn attention-based word-level interaction question relation alleviate bottleneck issue similar traditional model question relation firstly represent sequence vector instead merge sequence single vector pooling operation soft alignment word question relation learn aligned word subsequently compare convolutional neural network cnn comparison result merge finally perform comparison low-level representation attention-based word-level interaction model abwim relieve information loss issue cause merge sequence fixed-dimensional vector comparison experimental result relation detection simplequestions webquestions datasets show abwim achieves state-of-the-art accuracy demonstrate effectiveness