 learn universal sentence representation mean-max attention autoencoder order learn universal sentence representation previous method focus complex recurrent neural network supervise learning paper propose mean-max attention autoencoder mean-max aae within encoder-decoder framework autoencoder rely entirely multihead self-attention mechanism reconstruct input sequence encoding propose mean-max strategy apply mean max pooling operation hidden vector capture diverse information input enable information steer reconstruction process dynamically decoder performs attention mean-max representation train model large collection unlabelled data obtain high-quality representation sentence experimental result broad range transfer task demonstrate model outperform state-of-the-art unsupervised single method include classical skip-thoughts advanced skip-thoughts ln model furthermore compare traditional recurrent neural network mean-max aae greatly reduce training time