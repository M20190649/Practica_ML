 unsupervised cross-lingual transfer word embed space cross-lingual transfer word embeddings aim establish semantic mapping among word different language learn transformation function corresponding word embed space successfully solve problem would benefit many downstream task translate text classification model resource-rich language e.g english low-resource language supervise method problem rely availability cross-lingual supervision either use parallel corpus bilingual lexicon label data training may available many low resource language paper propose unsupervised learning approach require cross-lingual label data give two monolingual word embedding space language pair algorithm optimize transformation function direction simultaneously base distributional matching well minimize back-translation loss use neural network implementation calculate sinkhorn distance well-defined distributional similarity measure optimize objective back-propagation evaluation benchmark datasets bilingual lexicon induction cross-lingual word similarity prediction show strong competitive performance propose method compare state-of-the-art supervise unsupervised baseline method many language pair