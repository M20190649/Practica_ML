 shape shared-private encoder-decoder text style adaptation supervise training abstractive language generation model result learn conditional probability language sequence base supervised training signal training signal contain variety write style model may end learn 'average style directly influence training data make-up control need application describe family model architecture capable capture generic language characteristic via share model parameter well particular style characteristic via private model parameter model able generate language accord specific learned style still take advantage power model generic language phenomenon furthermore describe extension use mixture output distribution learn style perform on-the fly style adaptation base textual input alone experimentally find propose model consistently outperform model encapsulate single-style average-style language generation capability