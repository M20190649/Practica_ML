 learn well internal structure word sequence label character-based neural model recently prove useful many nlp task however gap sophistication method learn representation sentence word character model learn representation sentence deep complex model learn representation word shallow simple also spite considerable research learn character embeddings still clear kind architecture best capture character-to-word representation address question first investigate gap method learn word sentence representation conduct detailed experiment comparison different state-of-the-art convolutional model also investigate advantage disadvantage constituent furthermore propose intnet funnel-shaped wide convolutional neural architecture down-sampling learn representation internal structure word compose character limited supervise training corpus evaluate propose model six sequence label datasets include name entity recognition part-of-speech tagging syntactic chunking in-depth analysis show intnet significantly outperform character embed model obtain new state-of-the-art performance without rely external knowledge resource