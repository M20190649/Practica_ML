 local monotonic attention mechanism end-to-end speech language processing recently encoder-decoder neural network show impressive performance many sequence-related task architecture commonly use attentional mechanism allow model learn alignment source target sequence attentional mechanism use today base global attention property require computation weighted summarization whole input sequence generate encoder state however computationally expensive often produce misalignment long input sequence furthermore fit monotonous left-to-right nature several task automatic speech recognition asr grapheme-to-phoneme g p etc paper propose novel attention mechanism local monotonic property various way control property also explore experimental result asr g p machine translation two language similar sentence structure demonstrate propose encoder-decoder model local monotonic attention could achieve significant performance improvement reduce computational complexity comparison one use standard global attention architecture