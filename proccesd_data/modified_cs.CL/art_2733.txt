 towards bidirectional hierarchical representation attention-based neural machine translation paper propose hierarchical attentional neural translation model focus enhance source-side hierarchical representation cover local global semantic information use bidirectional tree-based encoder maximize predictive likelihood target word weighted variant attention mechanism use balance attentive information lexical phrase vector use tree-based rare word encoding propose model extend sub-word level alleviate out-of-vocabulary oov problem empirical result reveal propose model significantly outperform sequence-to-sequence attention-based tree-based neural translation model english-chinese translation task