 recurrent stacking layer compact neural machine translation model neural machine translation nmt common practice stack number recurrent feed-forward layer encoder decoder result addition new layer improve translation quality significantly however also lead significant increase number parameter paper propose share parameter across layer thereby lead recurrently stack nmt model empirically show translation quality model recurrently stack single layer time comparable translation quality model stack separate layer also show use pseudo-parallel corpus back-translation lead significant improvement translation quality