 graph convolutional encoders syntax-aware neural machine translation present simple effective approach incorporate syntactic structure neural attention-based encoder-decoder model machine translation rely graph-convolutional network gcns recent class neural network develop model graph-structured data gcns use predict syntactic dependency tree source sentence produce representation word i.e hidden state encoder sensitive syntactic neighborhood gcns take word representation input produce word representation output easily incorporate layer standard encoders e.g. top bidirectional rnns convolutional neural network evaluate effectiveness english-german english-czech translation experiment different type encoders observe substantial improvement syntax-agnostic version consider setup