 back-translation-style data augmentation end-to-end asr paper propose novel data augmentation method attention-based end-to-end automatic speech recognition e e-asr utilize large amount text pair speech signal inspire back-translation technique propose field machine translation build neural text-to-encoder model predict sequence hidden state extract pre-trained e e-asr encoder sequence character use hidden state target instead acoustic feature possible achieve fast attention learning reduce computational cost thanks sub-sampling e e-asr encoder also use hidden state avoid model speaker dependency unlike acoustic feature training text-to-encoder model generate hidden state large amount unpaired text e e-asr decoder retrain use generate hidden state additional training data experimental evaluation use librispeech dataset demonstrate propose method achieve improvement asr performance reduce number unknown word without need pair data