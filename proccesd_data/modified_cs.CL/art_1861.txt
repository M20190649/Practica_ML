 gate word-character recurrent language model introduce recurrent neural network language model rnn-lm long short-term memory lstm unit utilize character-level word-level input model gate adaptively find optimal mixture character-level word-level input gate create final vector representation word combine two distinct representation word character-level input convert vector representation word use bidirectional lstm word-level input project another high-dimensional space word lookup table final vector representation word use lstm language model predict next word give preceding word model gate mechanism effectively utilize character-level input rare out-of-vocabulary word outperforms word-level language model several english corpus