 explore hyper-parameter optimization neural machine translation gpu architecture neural machine translation nmt accelerate deep learn neural network statistical-based approach due plethora programmability commodity heterogeneous compute architecture fpgas gpus massive amount training corpus generate news outlet government agency social medium train learning classifier neural network entail tune hyper-parameters would yield best performance unfortunately number parameter machine translation include discrete category well continuous option make combinatorial explosive problem research explore optimize hyper-parameters train deep learn neural network machine translation specifically work investigate train language model marian nmt result compare nmt various hyper-parameter setting across variety modern gpu architecture generation single node multi-node setting reveal insight hyper-parameters matter term performance word process per second convergence rate translation accuracy provide insight best achieve high-performing nmt system