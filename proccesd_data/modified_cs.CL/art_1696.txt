 probabilistic modelling morphologically rich languages thesis investigate sub-structure word account probabilistic model language model play important role natural language process task translation speech recognition often rely simplistic assumption word opaque symbol assumption fit morphologically complex language well word rich internal structure sub-word element share across distinct word form approach encode basic notion morphology assumption three different type language model intention leverage share sub-word structure improve model performance help overcome data sparsity arise morphological process context n-gram language modelling formulate new bayesian model rely decomposition compound word attain good smoothing develop new distributed language model learn vector representation morpheme leverage link together morphologically related word case show account word sub-structure improve model intrinsic performance provide benefit apply task include machine translation shift focus beyond modelling word sequence consider model automatically learn sub-word element give language give unannotated list word formulate novel model learn discontiguous morpheme addition conventional contiguous morpheme previous model limit approach demonstrate semitic language find model discontiguous sub-word structure lead improvement task segment word contiguous morpheme