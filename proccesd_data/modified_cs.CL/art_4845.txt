 sequence-to-sequence model data-to-text natural language generation word- vs. character-based processing output diversity present comparison word-based character-based sequence-to-sequence model data-to-text natural language generation generate natural language description structured input datasets two recent generation challenge model achieve comparable well automatic evaluation result best challenge submission subsequent detail statistical human analysis shed light difference two input representation diversity generated text controlled experiment synthetic training data generate template demonstrate ability neural model learn novel combination template thereby generalize beyond linguistic structure train