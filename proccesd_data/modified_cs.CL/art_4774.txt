 neural speech synthesis transformer network although end-to-end neural text-to-speech tt method tacotron propose achieve state-of-the-art performance still suffer two problem low efficiency training inference hard model long dependency use current recurrent neural network rnns inspire success transformer network neural machine translation nmt paper introduce adapt multi-head attention mechanism replace rnn structure also original attention mechanism tacotron help multi-head self-attention hidden state encoder decoder construct parallel improve training efficiency meanwhile two input different time connect directly self-attention mechanism solve long range dependency problem effectively use phoneme sequence input transformer tt network generates mel spectrogram follow wavenet vocoder output final audio result experiment conduct test efficiency performance new network efficiency transformer tt network speed training time faster compare tacotron performance rigorous human test show propose model achieve state-of-the-art performance outperforms tacotron gap close human quality v mo