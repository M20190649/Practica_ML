 natural language generation neural variational model thesis explore use deep neural network generation natural language specifically implement two sequence-to-sequence neural variational model variational autoencoders vae variational encoder-decoders ved vaes text generation difficult train due issue associate kullback-leibler kl divergence term loss function vanish zero successfully train vaes implement optimization heuristic kl weight anneal word dropout also demonstrate effectiveness continuous latent space experiment random sampling linear interpolation sample neighborhood input argue vaes design appropriately may lead bypass connection result latent space ignore training show experimentally example decoder hidden state initialization bypassing connection degrade vae deterministic model thereby reduce diversity generated sentence discover traditional attention mechanism use sequence-to-sequence ved model serve bypassing connection thereby deteriorate model 's latent space order circumvent issue propose variational attention mechanism attention context vector model random variable sample distribution show empirically use automatic evaluation metric namely entropy distinct measure variational attention model generate diverse output sentence deterministic attention model qualitative analysis human evaluation study prove model simultaneously produce sentence high quality equally fluent one generate deterministic attention counterpart