 neural machine translation word prediction encoder-decoder architecture neural machine translation nmt hidden state recurrent structure encoder decoder carry crucial information sentence.these vector generate parameter update back-propagation translation error time argue propagate error end-to-end recurrent structure direct way control hidden vector paper propose use word prediction mechanism direct supervision specifically require vector able predict vocabulary target sentence simple mechanism ensure good representation encoder decoder without use extra data annotation also helpful reduce target side vocabulary improve decoding efficiency experiment chinese-english german-english machine translation task show bleu improvement respectively