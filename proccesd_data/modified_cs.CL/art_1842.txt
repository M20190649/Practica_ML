 learn natural language inference use bidirectional lstm model inner-attention paper propose sentence encoding-based model recognize text entailment approach encoding sentence two-stage process firstly average pooling use word-level bidirectional lstm bilstm generate first-stage sentence representation secondly attention mechanism employ replace average pooling sentence good representation instead use target sentence attend word source sentence utilize sentence 's first-stage representation attend word appear call inner-attention paper experiment conduct stanford natural language inference snli corpus prove effectiveness inner-attention mechanism less number parameter model outperform exist best sentence encoding-based approach large margin