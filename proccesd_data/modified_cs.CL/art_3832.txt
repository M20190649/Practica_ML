 deep rnns encode soft hierarchical syntax present set experiment demonstrate deep recurrent neural network rnns learn internal representation capture soft hierarchical notion syntax highly vary supervision consider four syntax task different depth parse tree word predict part speech well first parent second grandparent third level great-grandparent constituent label appear prediction make representation produce different depth network pretrained one four objective dependency parsing semantic role labeling machine translation language modeling every case find correspondence network depth syntactic depth suggest soft syntactic hierarchy emerges effect robust across condition indicate model encode significant amount syntax even absence explicit syntactic training supervision