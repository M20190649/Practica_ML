 linguistic knowledge memory recurrent neural network train recurrent neural network model long term dependency difficult hence propose use external linguistic knowledge explicit signal inform model memories utilize specifically external knowledge use augment sequence typed edge arbitrarily distant element result graph decompose direct acyclic subgraphs introduce model encode graph explicit memory recurrent neural network use model coreference relation text apply model several text comprehension task achieve new state-of-the-art result consider benchmark include cnn babi lambada babi qa task model solve task train example per task analysis learned representation demonstrate ability model encode fine-grained entity information across document