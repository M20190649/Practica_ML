 efficient attention use fixed-size memory representation standard content-based attention mechanism typically use sequence-to-sequence model computationally expensive require comparison large encoder decoder state time step work propose alternative attention mechanism base fixed size memory representation efficient technique predict compact set k attention context encode let decoder compute efficient lookup need consult memory show approach perform on-par standard attention mechanism yield inference speedup real-world translation task task long sequence visualize attention score demonstrate model learn distinct meaningful alignment