 fine line linguistic generalization failure seq seq-attention model seq seq base neural architecture become go-to architecture apply sequence sequence language task despite excellent performance task recent work note model usually fully capture linguistic structure require generalize beyond dense section data distribution cite ettinger towards likely fail sample tail end distribution input noisy citep belkinovnmtbreak different length citep bentivoglinmtlength paper look model 's ability generalize simple symbol rewrite task clearly defined structure find model 's ability generalize structure beyond training distribution depend greatly chosen random seed even performance standard test set remain suggest model 's ability capture generalizable structure highly sensitive moreover sensitivity may apparent evaluate standard test set