 deep multimodal learn emotion recognition spoken language paper present novel deep multimodal framework predict human emotion base sentence-level spoken language architecture two distinctive characteristic first extract high-level feature text audio via hybrid deep multimodal structure consider spatial information text temporal information audio high-level association low-level handcrafted feature second fuse feature use three-layer deep neural network learn correlation across modality train feature extraction fusion module together allow optimal global fine-tuning entire structure evaluate propose framework iemocap dataset result show promise performance achieve weighted accuracy five emotion category