 ensemble distillation neural machine translation knowledge distillation describe method train student network perform good learn strong teacher network translate sentence neural machine translation nmt engine time expensive small model speed process demonstrate transfer translation quality ensemble oracle bleu teacher network single nmt system present translation improvement teacher network architecture dimension student network training student model still expensive introduce data filtering method base knowledge teacher model speed training also lead better translation quality technique need code change easily reproduce nmt architecture speed decoding process