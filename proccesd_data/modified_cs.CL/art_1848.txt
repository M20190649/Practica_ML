 generalizing hybridize count-based neural language model language model lm statistical model calculate probability sequence word discrete symbol currently two major paradigm language model exist count-based n-gram model advantage scalability test-time speed neural lm often achieve superior modeling performance demonstrate variety model unify single modeling framework define set probability distribution vocabulary word dynamically calculate mixture weight distribution formulation allow u create novel hybrid model combine desirable feature count-based neural lm experiment demonstrate advantage approach