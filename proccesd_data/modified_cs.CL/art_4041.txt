 source-side monolingual word embeddings impact neural machine translation use pre-trained word embeddings input layer common practice many natural language processing nlp task largely neglect neural machine translation nmt paper conduct systematic analysis effect use pre-trained source-side monolingual word embed nmt compare several strategy fix update embeddings nmt training vary amount data also propose novel strategy call dual-embedding blend fixing updating strategy result suggest pre-trained embeddings helpful properly incorporate nmt especially parallel data limited additional in-domain monolingual data readily available