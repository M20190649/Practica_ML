 attention-based memory selection recurrent network language model recurrent neural network rnns achieve great success language modeling however since rnns fix size memory memory store information word see sentence thus useful long-term information may ignore predict next word paper propose attention-based memory selection recurrent network amsrn model review information store memory previous time step select relevant information help generate output amsrn attention mechanism find time step store relevant information memory memory selection determines dimension memory involve compute attention weight information extracted.in experiment amsrn outperform long short-term memory lstm base language model english chinese corpus moreover investigate use entropy regularizer attention weight visualize attention mechanism help language modeling