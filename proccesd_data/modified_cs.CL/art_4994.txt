 multilingual nmt language-independent attention bridge paper propose multilingual encoder-decoder architecture capable obtain multilingual sentence representation mean incorporate intermediate em attention bridge share across language train model language-specific encoders decoder connect via self-attention share layer call attention bridge layer exploit semantics language perform translation develop language-independent meaning representation efficiently use transfer learning present new framework efficient development multilingual nmt use model scheduled training test approach systematic way multi-parallel data set show model achieve substantial improvement strong bilingual model also work well zero-shot translation demonstrate ability abstraction transfer learning