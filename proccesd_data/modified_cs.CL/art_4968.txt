 may need attention nmt far get without attention without separate encoding decoding answer question introduce recurrent neural translation model use attention separate encoder decoder eager translation model low-latency write target token soon read first source token use constant memory decoding perform par standard attention-based model bahdanau et al good long sentence