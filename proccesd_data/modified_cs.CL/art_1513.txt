 compress word embeddings via deep compositional code learn natural language processing nlp model often require massive number parameter word embeddings result large storage memory footprint deploy neural nlp model mobile device require compress word embeddings without significant sacrifice performance purpose propose construct embeddings basis vector word composition basis vector determine hash code maximize compression rate adopt multi-codebook quantization approach instead binary cod scheme code compose multiple discrete number value component limit fixed range propose directly learn discrete code end-to-end neural network apply gumbel-softmax trick experiment show compression rate achieves sentiment analysis task machine translation task without performance loss task propose method improve model performance slightly lower compression rate compare approach character-level segmentation propose method language-independent require modification network architecture