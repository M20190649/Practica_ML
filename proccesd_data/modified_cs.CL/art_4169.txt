 prior attention style-aware sequence-to-sequence model extend sequence-to-sequence model possibility control characteristic style generated output via attention generate priori decode latent code vector train initial attention-based sequence-to-sequence model use variational auto-encoder condition representation input sequence latent code vector space generate attention matrix sample code vector specific region latent space decode impose prior attention generate seq seq model output steer towards certain attribute demonstrate task sentence simplification latent code vector allow control output length lexical simplification enable fine-tuning optimize different evaluation metric