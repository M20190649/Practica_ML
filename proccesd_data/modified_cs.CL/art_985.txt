 sum part joint learning word phrase representation autoencoders recently lot effort represent word continuous vector space representation show capture semantic syntactic information word however distribute representation phrase remain challenge introduce novel model jointly learn word vector representation summation word representation learnt use word co-occurrence statistical information embed sequence word i.e phrase different size common semantic space propose average word vector representation contrast previous method report posteriori compositionality aspect simple summation simultaneously train word sum keep maximum information original vector evaluate quality word representation several classical word evaluation task introduce novel task evaluate quality phrase representation distributed representation compete method learn word representation word evaluation show give good performance phrase evaluation representation phrase could interest many task natural language processing