 lightweight adaptive mixture neural n-gram language model often case best performing language model ensemble neural language model n-grams work propose method improve two model combine use small network predict mixture weight two model adapt relative importance time step gating network small train quickly small amount hold data add overhead score time experiment carry one billion word benchmark show significant improvement state art ensemble without retrain basic module