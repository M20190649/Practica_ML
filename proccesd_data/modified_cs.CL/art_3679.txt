 efficient contextualized representation language model prune sequence label many effort make facilitate natural language processing task pre-trained language model lms bring significant improvement various application fully leverage nearly unlimited corpus capture linguistic information multifarious level large-size lm require specific task part information useful large-sized lm even inference stage may cause heavy computation workload make time-consuming large-scale application propose compress bulky lm preserve useful information regard specific task different layer model keep different information develop layer selection method model prune use sparsity-inducing regularization introduce dense connectivity detach layer without affect others stretch shallow wide lm deep narrow model training lm learn layer-wise dropout good robustness experiment two benchmark datasets demonstrate effectiveness method