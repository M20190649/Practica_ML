 self-attentive residual decoder neural machine translation neural sequence-to-sequence network attention achieve remarkable performance machine translation one reason effectiveness ability capture relevant source-side contextual information time-step prediction attention mechanism however target-side context solely base sequence model practice prone recency bias lack ability capture effectively non-sequential dependency among word address limitation propose target-side-attentive residual recurrent network decoding attention previous word contribute directly prediction next word residual learning facilitate flow information distant past able emphasize previously translate word hence gain access wider context propose model outperform neural mt baseline well memory self-attention network three language pair analysis attention learn decoder confirm emphasize wider context capture syntactic-like structure