 empirical gaussian prior cross-lingual transfer learn sequence model learn algorithm typically maximize log-likelihood minus norm model minimize hamming loss norm cross-lingual part-of-speech po tagging target language training data consists sequence sentence word-by-word label project translation k language label data via word alignment training data therefore noisy rademacher complexity high learning algorithm prone overfit norm-based regularization assume constant width zero mean prior instead propose use k source language model estimate parameter gaussian prior learn new po tagger lead significantly good performance multi-source transfer set-ups also present drop-out version inject empirical gaussian noise online learning finally note use empirical gaussian prior lead much low rademacher complexity superior optimally weighted model interpolation