 multimodal affective analysis use hierarchical attention strategy word-level alignment multimodal affective computing learn recognize interpret human affect subjective information multiple data source still challenging hard extract informative feature represent human affect heterogeneous input ii current fusion strategy fuse different modality abstract level ignore time-dependent interaction modality address issue introduce hierarchical multimodal architecture attention word-level fusion classify utter-ance-level sentiment emotion text audio data introduced model outperform state-of-the-art approach publish datasets demonstrate model able visualize interpret synchronized attention modality