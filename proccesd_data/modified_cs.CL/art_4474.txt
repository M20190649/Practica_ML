 improve cross-lingual word embeddings meet middle cross-lingual word embeddings become increasingly important multilingual nlp recently show embeddings effectively learn align two disjoint monolingual vector space linear transformation use small bilingual dictionary supervision work propose apply additional transformation initial alignment step move cross-lingual synonym towards middle point apply transformation aim obtain good cross-lingual integration vector space addition perhaps surprisingly monolingual space also improve transformation contrast original alignment typically learn structure monolingual space preserve experiment confirm result cross-lingual embeddings outperform state-of-the-art model monolingual cross-lingual evaluation task