 semantic projection recover human knowledge multiple distinct object feature word embeddings word language reflect structure human mind allow u transmit thought individual however language represent subset rich detailed cognitive architecture ask kind common knowledge semantic memory capture word meaning lexical semantics examine prominent computational model represent word vector multidimensional space proximity word-vectors approximates semantic relatedness related word appear similar context space call word embeddings learn pattern lexical co-occurrence natural language despite popularity fundamental concern word embeddings appear semantically rigid inter-word proximity capture overall similarity yet human judgment object similarity highly context-dependent involve multiple distinct semantic feature example dolphin alligator appear similar size differ intelligence aggressiveness could context-dependent relationship recover word embeddings address issue introduce powerful domain-general solution semantic projection word-vectors onto line represent various object feature like size line extend word small big intelligence dumb smart danger safe dangerous method intuitively analogous place object mental scale two extreme recovers human judgment across range object category property thus show word embeddings inherit wealth common knowledge word co-occurrence statistic flexibly manipulate express context-dependent meaning