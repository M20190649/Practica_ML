 dual long short-term memory network sub-character representation learn character commonly regard minimal processing unit natural language processing nlp many non-latin language hieroglyphic write system involve big alphabet thousand million character character compose even small part often ignore previous work paper propose novel architecture employ two stack long short-term memory network lstms learn sub-character level representation capture deep level semantic meaning build concrete study substantiate efficiency neural architecture take chinese word segmentation research case example among language chinese typical case every character contain several component call radical network employ shared radical level embed solve simplify traditional chinese word segmentation without extra traditional simplify chinese conversion highly end-to-end way word segmentation significantly simplify compare previous work radical level embeddings also capture deep semantic meaning character level improve system performance learning tie radical character embeddings together parameter count reduce whereas semantic knowledge share transfer two level boost performance largely bakeoff datasets method surpass state-of-the-art result result reproducible source code corpus available github