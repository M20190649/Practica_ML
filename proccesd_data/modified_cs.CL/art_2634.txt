 subregular complexity deep learning paper argue judicial use formal language theory grammatical inference invaluable tool understand deep neural network represent learn long-term dependency temporal sequence learn experiment conduct two type recurrent neural network rnns six formal language draw strictly local sl strictly piecewise sp class network simple rnns s-rnns long short-term memory rnns lstms vary size sl sp class among simple mathematically well-understood hierarchy subregular class encode local long-term dependency respectively grammatical inference algorithm regular positive negative inference rpni provide baseline accord earlier research lstm architecture capable learn long-term dependency outperform s-rnns result experiment challenge narrative first lstms performance generally bad sp experiment sl one second s-rnns out-performed lstms complex sp experiment perform comparably others