 dragnn transition-based framework dynamically connect neural network work present compact modular framework construct novel recurrent neural architecture basic module new generic unit transition base recurrent unit tbru addition hide layer activation tbrus discrete state dynamic allow network connection build dynamically function intermediate activation connect multiple tbrus extend combine commonly used architecture sequence-to-sequence attention mechanism re-cursive tree-structured model tbru also serve encoder downstream task decoder task simultaneously result accurate multi-task learning call approach dynamic recurrent acyclic graphical neural network dragnn show dragnn significantly accurate efficient seq seq attention syntactic dependency parse yield accurate multi-task learning extractive summarization task