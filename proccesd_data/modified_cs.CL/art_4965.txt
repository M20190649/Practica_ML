 convolutional self-attention network self-attention network san recently attract increase interest due fully parallelize computation flexibility model dependency enhance multi-headed attention mechanism allow model jointly attend information different representation subspace different position vaswani et al. work propose novel convolutional self-attention network csan offer san ability capture neighboring dependency model interaction multiple attention head experimental result wmt english-to-german translation task demonstrate propose approach outperform strong transformer baseline exist work enhance locality san compare previous work model introduce new parameter