 battrae bidimensional attention-based recursive autoencoders learn bilingual phrase embeddings paper propose bidimensional attention base recursive autoencoder battrae integrate clue sourcetarget interaction multiple level granularity bilingual phrase representation employ recursive autoencoders generate tree structure phrase embeddings different level granularity e.g. word sub-phrases phrase embeddings source target side introduce bidimensional attention network learn interaction encode bidimensional attention matrix extract two soft attention weight distribution simultaneously weight distribution enable battrae generate compositive phrase representation via convolution base learned phrase representation use bilinear neural model train via max-margin method measure bilingual semantic similarity evaluate effectiveness battrae incorporate semantic similarity additional feature state-of-the-art smt system extensive experiment nist chinese-english test set show model achieve substantial improvement bleu point average baseline