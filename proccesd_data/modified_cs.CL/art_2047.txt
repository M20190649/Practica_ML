 strong baseline learn cross-lingual word embeddings sentence alignment cross-lingual word embeddings study extensively recent year qualitative difference different algorithm remain vague observe whether algorithm use particular feature set sentence id account significant performance gap among algorithm feature set also use traditional alignment algorithm ibm model- demonstrate similar performance state-of-the-art embed algorithm variety benchmark overall observe different algorithmic approach utilize sentence id feature space result similar performance paper draw empirical theoretical parallel embedding alignment literature suggest add additional source information go beyond traditional signal bilingual sentence-aligned corpus may substantially improve cross-lingual word embeddings future baseline least take feature account