 statistical model compression small-footprint natural language understanding paper investigate statistical model compression apply natural language understand nlu model small-footprint nlu model important enable offline system hardware restrict device decrease on-demand model load latency cloud-based system compress nlu model present two main technique parameter quantization perfect feature hashing technique complementary exist model prune strategy l regularization perform experiment large scale nlu system result show approach achieve -fold reduction memory usage compare original model minimal predictive performance impact