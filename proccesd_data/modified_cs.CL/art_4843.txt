 bert pre-training deep bidirectional transformer language understanding introduce new language representation model call bert stand bidirectional encoder representation transformer unlike recent language representation model bert design pre-train deep bidirectional representation unlabeled text jointly condition leave right context layer result pre-trained bert model fine-tuned one additional output layer create state-of-the-art model wide range task question answering language inference without substantial task-specific architecture modification bert conceptually simple empirically powerful obtain new state-of-the-art result eleven natural language processing task include push glue score point absolute improvement multinli accuracy absolute improvement squad v question answer test f point absolute improvement squad v test f point absolute improvement