 asynchronous bidirectional decoding neural machine translation dominant neural machine translation nmt model apply unified attentional encoder-decoder neural network translation traditionally nmt decoder adopt recurrent neural network rnns perform translation left-toright manner leave target-side context generate right leave unexploited translation paper equip conventional attentional encoder-decoder nmt framework backward decoder order explore bidirectional decoding nmt attend hidden state sequence produce encoder backward decoder first learn generate target-side hidden state sequence right leave forward decoder performs translation forward direction translation prediction timestep simultaneously apply two attention model consider source-side reverse target-side hidden state respectively new architecture model able fully exploit source- target-side context improve translation quality altogether experimental result nist chinese-english wmt english-german translation task demonstrate model achieve substantial improvement conventional nmt bleu point respectively source code work obtain http github.com deeplearnxmu abdnmt