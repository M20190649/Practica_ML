 state art evaluation neural language model ongoing innovation recurrent neural network architecture provide steady influx apparently state-of-the-art result language modelling benchmark however evaluate use differ code base limited computational resource represent uncontrolled source experimental variation reevaluate several popular architecture regularisation method large-scale automatic black-box hyperparameter tuning arrive somewhat surprising conclusion standard lstm architecture properly regularise outperform recent model establish new state art penn treebank wikitext- corpus well strong baseline hutter prize dataset