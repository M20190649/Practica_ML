 sequence-to-sequence neural net model grapheme-to-phoneme conversion sequence-to-sequence translation method base generation side-conditioned language model recently show promising result several task machine translation model condition source side word use produce target-language text image captioning model condition image use generate caption text past work approach focus large vocabulary task measure quality term bleu paper explore applicability model qualitatively different grapheme-to-phoneme task input output side vocabulary small plain n-gram model well credit give output exactly correct find simple side-conditioned generation approach able rival state-of-the-art able significantly advance stat-of-the-art bi-directional long short-term memory lstm neural network use alignment information use conventional approach