 near-lossless binarization word embeddings word embeddings commonly use start point many nlp model achieve state-of-the-art performance however large vocabulary many dimension floating-point representation expensive term memory calculation make unsuitable use low-resource device method propose paper transform real-valued embeddings binary embeddings preserve semantic information require bit vector lead small memory footprint fast vector operation model base autoencoder architecture also allow reconstruct original vector binary one experimental result semantic similarity text classification sentiment analysis task show binarization word embeddings lead loss accuracy vector size reduce furthermore top-k benchmark demonstrate use binary vector time faster use real-valued vector