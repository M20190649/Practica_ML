 correlational encoder decoder architecture pivot base sequence generation interlingua base machine translation mt aim encode multiple language common linguistic representation decode sentence multiple target language representation work explore idea context neural encoder decoder architecture albeit small scale without mt end goal specifically consider case three language modality x z wherein interested generate sequence start information available x. however parallel training data available x train data available x z z often case many real world application z thus act pivot bridge obvious solution perhaps less elegant work well practice train two stage model first convert x z z y. instead explore interlingua inspire solution jointly learn following encode x z common representation ii decode common representation evaluate model two task bridge transliteration ii bridge captioning report promise result application believe right step towards truly interlingua inspired encoder decoder architecture