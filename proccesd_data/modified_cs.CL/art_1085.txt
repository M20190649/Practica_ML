 multinomial loss held-out data sparse non-negative matrix language model describe sparse non-negative matrix snm language model estimation use multinomial loss held-out data able train held-out data important practical situation training data usually mismatch held-out test data also less constrained previous training algorithm use leave-one-out training data allow use rich meta-features adjustment model e.g diversity count use kneser-ney smoothing would difficult deal correctly leave-one-out training experiment one billion word language model benchmark able slightly improve previous result use different loss function employ leave-one-out training subset main training set surprisingly adjustment model meta-features discard lexical information perform well lexicalize meta-features find fairly small amount held-out data order thousand word sufficient train adjustment model real-life scenario training data mix data source imbalanced size different degree relevance held-out test data take account data source give skip- n-gram feature combine best performance held-out test data improves skip- n-gram snm model train pool data smt setup much asr ime setup ability mix various data source base relevant mismatched held-out set probably attractive feature new estimation method snm lm