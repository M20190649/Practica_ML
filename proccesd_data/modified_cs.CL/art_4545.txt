 revisit character-based neural machine translation capacity compression translate character instead word word-fragments potential simplify processing pipeline neural machine translation nmt improve result eliminate hyper-parameters manual feature engineering however result long sequence symbol contain less information create modeling computational challenge paper show modeling problem solve standard sequence-to-sequence architecture sufficient depth deep model operate character level outperform identical model operate word fragment result imply alternative architecture handle character input well view method reduce computation time improve way model long sequence perspective evaluate several technique character-level nmt verify match performance deep character baseline model evaluate performance versus computation time tradeoff offer within framework also perform first evaluation nmt conditional computation time model learn timesteps skip rather dictate fixed schedule specify training begin