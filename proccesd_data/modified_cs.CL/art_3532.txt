 pay attention neural architecture question-answering machine comprehension representative task natural language understanding typically give context paragraph objective answer question depend context problem require model complex interaction context paragraph question lately attention mechanism find quite successful task particular attention mechanism attention flow context-to-question question-to-context prove quite useful paper study two state-of-the-art attention mechanism call bi-directional attention flow bidaf dynamic co-attention network dcn propose hybrid scheme combine two architecture give good overall performance moreover also suggest new simpler attention mechanism call double cross attention dca provide good result compare bidaf co-attention mechanism provide similar performance hybrid scheme objective paper focus particularly attention layer suggest improvement experimental evaluation show propose model achieve superior result stanford question answer dataset squad compare bidaf dcn attention mechanism