 comparative study word embeddings read comprehension focus past machine learning research read comprehension task primarily design novel deep learning architecture show seemingly minor choice make use pre-trained word embeddings representation out-of-vocabulary token test time turn large impact architectural choice final performance systematically explore several option choice provide recommendation researcher work area