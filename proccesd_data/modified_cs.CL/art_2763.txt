 optimal hyperparameters deep lstm-networks sequence labeling task select optimal parameter neural network architecture often make difference mediocre state-of-the-art performance however little publish parameter design choice evaluate select make correct hyperparameter optimization often black art require expert experience snoek et al. paper evaluate importance different network design choice hyperparameters five common linguistic sequence tag task po chunk ner entity recognition event detection evaluate different setup find parameter like pre-trained word embeddings last layer network large impact performance parameter example number lstm layer number recurrent unit minor importance give recommendation configuration perform well among different task