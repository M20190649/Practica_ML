 jointly train sequential labeling classification sparse attention neural network sentence-level classification sequential labeling two fundamental task language understanding two task usually model separately reality often correlate example intent classification slot filling topic classification named-entity recognition order utilize potential benefit correlation propose jointly train model learn two task simultaneously via long short-term memory lstm network model predict sentence-level category word-level label sequence stepwise output hidden representation lstm also introduce novel mechanism sparse attention weigh word differently base semantic relevance sentence-level classification propose method outperform baseline model atis trec datasets