 language model teach syntax translation lesson learn auxiliary task analysis recent work use auxiliary prediction task classifier investigate property lstm representation begin shed light pretrained representation like elmo peter et al. cove mccann et al. beneficial neural language understanding model still though yet clear understanding choice pretraining objective affect type linguistic information model learn mind compare four objective -- -language modeling translation skip-thought autoencoding -- -on ability induce syntactic part-of-speech information make fair comparison task hold constant quantity genre training data well lstm architecture find representation language model consistently perform best syntactic auxiliary prediction task even train relatively small amount data result suggest language modeling may best data-rich pretraining task transfer learning application require syntactic information also find representation randomly-initialized frozen lstms perform strikingly well syntactic auxiliary task effect disappear amount training data auxiliary task reduce