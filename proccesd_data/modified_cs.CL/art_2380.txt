 exponential move average model parallel speech recognition training training data rapid growth large-scale parallel training multi-gpus cluster widely apply neural network model learn currently.we present new approach apply exponential move average method large-scale parallel training neural network model non-interference strategy exponential moving average model broadcast distributed worker update local model model synchronization training process implement final model training system fully-connected feed-forward neural network dnns deep unidirectional long short-term memory lstm recurrent neural network rnns successfully train proposed method large vocabulary continuous speech recognition shenma voice search data mandarin character error rate cer mandarin speech recognition degrades state-of-the-art approach parallel training