 self-attention targeted evaluation neural machine translation architectures recently non-recurrent architecture convolutional self-attentional outperform rnns neural machine translation cnns self-attentional network connect distant word via short network path rnns speculate improve ability model long-range dependency however theoretical argument test empirically alternative explanation strong performance explore in-depth hypothesize strong performance cnns self-attentional network could also due ability extract semantic feature source text evaluate rnns cnns self-attention network two task subject-verb agreement capture long-range dependency require word sense disambiguation semantic feature extraction require experimental result show self-attentional network cnns outperform rnns model subject-verb agreement long distance self-attentional network perform distinctly good rnns cnns word sense disambiguation