 hierarchical attention really count various nlp task attention mechanism sequence sequence model show great ability wonderful performance various natural language processing nlp task sentence embedding text generation machine translation machine reading comprehension etc unfortunately exist attention mechanism learn either high-level low-level feature paper think lack hierarchical mechanism bottleneck improve performance attention mechanism propose novel hierarchical attention mechanism ham base weighted sum different layer multi-level attention ham achieve state-of-the-art bleu score chinese poem generation task nearly average improvement compare exist machine read comprehension model bidaf match-lstm furthermore experiment theorem reveal ham great generalization representation ability exist attention mechanism