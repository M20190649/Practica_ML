 recurrent neural network-based semantic variational autoencoder sequence-to-sequence learn sequence-to-sequence seq seq model play important role recent success various natural language processing method machine translation text summarization speech recognition however current seq seq model trouble preserve global latent information long sequence word variational autoencoder vae alleviate problem learn continuous semantic space input sentence however solve problem completely paper propose new recurrent neural network rnn -based seq seq model rnn semantic variational autoencoder rnn -- svae well capture global latent information sequence word reflect meaning word sentence properly without regard position within sentence construct document information vector use attention information final state encoder every prior hidden state mean standard deviation continuous semantic space learn use vector take advantage variational method use document information vector find semantic space sentence become possible well capture global latent feature sentence experimental result three natural language task i.e. language modeling miss word imputation paraphrase identification confirm propose rnn -- svae yield high performance two benchmark model