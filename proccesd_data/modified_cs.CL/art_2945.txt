 look-ahead attention generation neural machine translation attention model become standard component neural machine translation nmt guide translation process selectively focus part source sentence predict target word however find generation target word depend source sentence also rely heavily previous generate target word especially distant word difficult model use recurrent neural network solve problem propose paper novel look-ahead attention mechanism generation nmt aim directly capture dependency relationship target word design three pattern integrate look-ahead attention conventional attention model experiment nist chinese-to-english wmt english-to-german translation task show propose look-ahead attention mechanism achieve substantial improvement state-of-the-art baseline