 improve machine read comprehension general reading strategy read strategy show improve comprehension level especially reader lack adequate prior knowledge process knowledge accumulation time-consuming human reader resource-demanding impart rich general domain knowledge deep language model via pre-training inspire read strategy identify cognitive science give limited computational resource -- pre-trained model fixed number training instance -- propose three general strategy aim improve non-extractive machine read comprehension mrc back forth reading consider original reverse order input sequence ii highlighting add trainable embedding text embedding token relevant question candidate answer iii self-assessment generate practice question candidate answer directly text unsupervised manner fine-tune pre-trained language model radford et al. propose strategy large general domain multiple-choice mrc dataset race obtain absolute increase accuracy previous best result achieve pre-trained model fine-tuned race without use strategy fine-tune result model target mrc task lead absolute improvement average accuracy previous state-of-the-art approach six representative non-extractive mrc datasets different domain i.e. arc openbookqa mctest semeval- task rocstories multirc result demonstrate effectiveness propose strategy versatility general applicability fine-tuned model incorporate strategy core code available http github.com nlpdata strategy