 hard non-monotonic attention character-level transduction character-level string-to-string transduction important component various nlp task goal map input string output string string may different length character take different alphabet recent approach use sequence-to-sequence model attention mechanism learn part input string model focus generation output string soft attention hard monotonic attention use hard non-monotonic attention use sequence model task image captioning require stochastic approximation compute gradient work introduce exact polynomial-time algorithm marginalize exponential number non-monotonic alignment two string show hard attention model view neural reparameterizations classical ibm model compare soft hard non-monotonic attention experimentally find exact algorithm significantly improve performance stochastic approximation outperforms soft attention