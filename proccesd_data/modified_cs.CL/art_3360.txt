 densely connect bidirectional lstm application sentence classification deep neural network recently show achieve highly competitive performance many computer vision task due ability explore much large hypothesis space however since deep architecture like stacked rnns tend suffer vanishing-gradient overfitting problem effect still understudy many nlp task inspire propose novel multi-layer rnn model call densely connected bidirectional long short-term memory dc-bi-lstm paper essentially represent layer concatenation hidden state precede layer hidden state follow recursively pass layer 's representation subsequent layer evaluate propose model five benchmark datasets sentence classification dc-bi-lstm depth successfully trained obtain significant improvement traditional bi-lstm even less parameter moreover model promising performance compare state-of-the-art approach