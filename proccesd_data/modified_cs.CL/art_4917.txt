 language modeling scale show zipf 's law use scale language model lm take advantage training data gpus lm play key role many important natural language application speech recognition machine translation scale lm important since widely accept community data like data eventually would like train terabyte tb text trillion word modern training method far goal various bottleneck especially memory within gpus communication across gpus paper show zipf 's law address bottleneck group parameter common word character sequence u n u number unique word type n size training set token local batch size k g gpus -dimension embed matrix reduce original per-gpu memory communication asymptotic complexity theta gkd theta gk ud empirically find u propto gk four publicly available large datasets scale number gpus factor train time speed factor time character lm time word lm negligible loss accuracy weak scaling gpus tieba dataset show improvement lm prediction accuracy train gb data time large publicly available sota dataset take time increase training time compare gb dataset run gpus