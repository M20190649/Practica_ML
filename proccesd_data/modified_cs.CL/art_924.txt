 leverage twitter low-resource conversational speech language modeling application involve conversational speech data sparsity limiting factor build good language model propose simple language-independent method quickly harvest large amount data twitter supplement small training set closely match domain technique lead significant reduction perplexity four low-resource language even though presence twitter language relatively small also find twitter text useful learn word class in-domain text use word class lead reduction perplexity additionally introduce method use social textual information prioritize download queue twitter crawling maximize amount useful data collect impact perplexity vocabulary coverage