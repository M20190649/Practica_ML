 restrict recurrent neural tensor network exploit word frequency compositionality increase capacity recurrent neural network rnn usually involve augment size hidden layer significant increase computational cost recurrent neural tensor network rntn increase capacity use distinct hidden layer weight word great cost memory usage paper introduce restrict recurrent neural tensor network r-rntn reserve distinct hidden layer weight frequent vocabulary word share single set weight infrequent word perplexity evaluation show fixed hidden layer size r-rntns improve language model performance rnns use small fraction parameter unrestricted rntns result hold r-rntns use gated recurrent unit long short-term memory