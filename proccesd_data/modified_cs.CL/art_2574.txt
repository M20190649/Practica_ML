 semi-supervised sequence tag bidirectional language model pre-trained word embeddings learn unlabeled text become standard component neural network architecture nlp task however case recurrent network operate word-level representation produce context sensitive representation train relatively little labeled data paper demonstrate general semi-supervised approach add pre- trained context embeddings bidirectional language model nlp system apply sequence label task evaluate model two standard datasets name entity recognition ner chunking case achieve state art result surpass previous system use form transfer joint learning additional label data task specific gazetteer