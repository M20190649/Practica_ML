 distributional representation ready real world evaluate word vector grounded perceptual meaning distributional word representation method exploit word co-occurrence build compact vector encoding word representation enjoy widespread use modern natural language processing unclear whether accurately encode necessary facet conceptual meaning paper evaluate well representation predict perceptual conceptual feature concrete concept draw two semantic norm datasets source human participant find several standard word representation fail encode many salient perceptual feature concept show deficit correlate word-word similarity prediction error analysis provide motivation grounded embodied language learning approach may help remedy deficit