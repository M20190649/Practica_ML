 n-gram-based low-dimensional representation document classification bag-of-words bow model common approach classify document word use feature train classifier generally involve huge number feature technique latent semantic analysis lsa latent dirichlet allocation lda design summarize document low dimension least semantic information loss semantic information nevertheless always lose since word consider instead aim use information come n-grams overcome limitation remain low-dimension space many approach skip-gram model provide good word vector representation quickly propose average representation obtain representation n-grams n-grams thus embed semantic space k-means clustering group semantic concept number feature therefore dramatically reduce document represent bag semantic concept show model outperform lsa lda sentiment classification task yield similar result traditional bow-model far less feature