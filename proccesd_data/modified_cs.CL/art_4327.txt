 word-level loss extension neural temporal relation classification unsupervised pre-trained word embeddings use effectively many task natural language processing leverage unlabeled textual data often embeddings either use initialization fixed word representation task-specific classification model work extend classification model 's task loss unsupervised auxiliary loss word-embedding level model ensure learned word representation contain task-specific feature learn supervised loss component general feature learn unsupervised loss component evaluate approach task temporal relation extraction particular narrative containment relation extraction clinical record show continue training embeddings unsupervised objective together task objective give well task-specific embeddings result improvement state art thyme dataset use general-domain part-of-speech tagger linguistic resource