 phase conductor multi-layered attention machine comprehension attention model intensively study improve nlp task machine comprehension via question-aware passage attention model self-matching attention model research propose phase conductor phasecond attention model two meaningful way first phasecond architecture multi-layered attention model consists multiple phase implement stack attention layer produce passage representation stack inner out fusion layer regulate information flow second extend improve dot-product attention function phasecond simultaneously encode multiple question passage embed layer different perspective demonstrate effectiveness propose model phasecond squad dataset show model significantly outperform state-of-the-art single-layered multiple-layered attention model deepen result new finding via detailed qualitative analysis visualized example show dynamic change multi-layered attention model