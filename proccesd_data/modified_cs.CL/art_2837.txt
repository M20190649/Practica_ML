 low-rank hidden state embeddings viterbi sequence labeling textual information extraction sequence label task common use recurrent neural network lstm form rich embedded representation long-term input co-occurrence pattern representation output co-occurrence pattern typically limited hand-designed graphical model linear-chain crf represent short-term markov dependency among successive label paper present method learn embedded representation latent output structure sequence data model take form finite-state machine large number latent state per label latent variable crf state-transition matrix factorize -- -effectively form embedded representation state-transitions capable enforce long-term label dependency support exact viterbi inference output label demonstrate accuracy improvement interpretable latent structure synthetic complex task base conll name entity recognition