 efficient summarization read-again copy mechanism encoder-decoder model widely use solve sequence sequence prediction task however current approach suffer two shortcoming first encoders compute representation word take account history word read far yield suboptimal representation second current decoder utilize large vocabulary order minimize problem unknown word result slow decoding time paper address shortcoming towards goal first introduce simple mechanism first read input sequence commit representation word furthermore propose simple copy mechanism able exploit small vocabulary handle out-of-vocabulary word demonstrate effectiveness approach gigaword dataset duc competition outperform state-of-the-art