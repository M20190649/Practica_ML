 morphosyntactic tagging meta-bilstm model context sensitive token encode rise neural network particularly recurrent neural network produce significant advance part-of-speech tagging accuracy one characteristic common among model presence rich initial word encoding encoding typically compose recurrent character-based representation learned pre-trained word embeddings however encoding consider context wider single word subsequent recurrent layer word sub-word information interacts paper investigate model use recurrent neural network sentence-level context initial character word-based representation particular show optimal result obtain integrate context sensitive representation synchronized training meta-model learn combine state present result part-of-speech morphological tagging state-of-the-art performance number language