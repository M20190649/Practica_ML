 lattice-based recurrent neural network encoders neural machine translation neural machine translation nmt heavily relies word-level modelling learn semantic representation input sentence however language without natural word delimiters e.g. chinese input sentence tokenized first conventional nmt confront two issue difficult find optimal tokenization granularity source sentence modelling error -best tokenizations may propagate encoder nmt handle issue propose word-lattice base recurrent neural network rnn encoders nmt generalize standard rnn word lattice topology proposed encoders take input word lattice compactly encode multiple tokenizations learn generate new hidden state arbitrarily many input hidden state precede time step word-lattice base encoders alleviate negative impact tokenization error also expressive flexible embed input sentence experiment result chinese-english translation demonstrate superiority propose encoders conventional encoder