 revisit recurrent network paraphrastic sentence embeddings consider problem learn general-purpose paraphrastic sentence embeddings revisit setting wieting et al b find lstm recurrent network underperform word averaging present several development together produce opposite conclusion include training sentence pair rather phrase pair average state represent sequence regularize aggressively improve lstms transfer learning supervised setting also introduce new recurrent architecture gated recurrent average network inspire average lstms outperform analyze learned model find evidence preference particular part speech dependency relation