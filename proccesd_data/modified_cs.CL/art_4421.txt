 gaussian word embed wasserstein distance loss compare word embed base point representation distribution-based word embedding show flexibility express uncertainty therefore embeds richer semantic information represent word wasserstein distance provide natural notion dissimilarity probability measure closed-form solution measure distance two gaussian distribution therefore aim represent word highly efficient way propose operate gaussian word embed model loss function base wasserstein distance also external information conceptnet use semi-supervise result gaussian word embedding thirteen datasets word similarity task together one word entailment task six datasets downstream document classification task evaluate paper test hypothesis