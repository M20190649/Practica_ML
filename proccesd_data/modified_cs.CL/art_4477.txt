 accelerate asynchronous stochastic gradient descent neural machine translation order extract best possible performance asynchronous stochastic gradient descent one must increase mini-batch size scale learning rate accordingly order achieve speedup introduce technique delay gradient update effectively increase mini-batch size unfortunately increase mini-batch size worsen stale gradient problem asynchronous stochastic gradient descent sgd make model convergence poor introduce local optimizers mitigate stale gradient problem together fine tune momentum able train shallow machine translation system faster optimized baseline negligible penalty bleu