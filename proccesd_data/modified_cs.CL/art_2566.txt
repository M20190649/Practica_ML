 gru-gated attention model neural machine translation neural machine translation nmt heavily relies attention network produce context vector target word prediction practice find context vector different target word quite similar one another therefore insufficient discriminatively predict target word reason might context vector produce vanilla attention network weighted sum source representation invariant decoder state paper propose novel gru-gated attention model gatt nmt enhance degree discrimination context vector enable source representation sensitive partial translation generate decoder gatt use gated recurrent unit gru combine two type information treat source annotation vector originally produce bidirectional encoder history state correspond previous decoder state input gru gru-combined information form new source annotation vector way obtain translation-sensitive source representation fee attention network generate discriminative context vector propose variant regard source annotation vector current input previous decoder state history experiment nist chinese-english translation task show gatt-based model achieve significant improvement vanilla attentionbased nmt analysis attention weight context vector demonstrate effectiveness gatt improve discrimination power representation handle challenging issue over-translation