 deep neural machine translation weakly-recurrent unit recurrent neural network rnns represent year state art neural machine translation recently new architecture propose leverage parallel computation gpus well classical rnns faster training inference combine different sequence-to-sequence modeling also lead performance improvement new model completely depart original recurrent architecture decide investigate make rnns efficient work propose new recurrent nmt architecture call simple recurrent nmt build class fast weakly-recurrent unit use layer normalization multiple attention experiment wmt english-to-german wmt english-romanian benchmark show model represent valid alternative lstms achieve good result significantly low computational cost