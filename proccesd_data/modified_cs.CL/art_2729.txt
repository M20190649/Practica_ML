 neural net learn statistical law behind natural language performance deep learning natural language processing spectacular reason success remain unclear inherent complexity deep learning paper provide empirical evidence effectiveness limitation neural network language engineering precisely demonstrate neural language model base long short-term memory lstm effectively reproduce zipf 's law heap law two representative statistical property underlie natural language discuss quality reproducibility emergence zipf 's law heap law training progress also point neural language model limitation reproduce long-range correlation another statistical property natural language understanding could provide direction improve architecture neural network