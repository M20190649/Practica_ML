 multi-task learning universal sentence embeddings thorough evaluation use transfer auxiliary task learn distributed sentence representation one key challenge natural language processing previous work demonstrate recurrent neural network rnns base sentence encoder train large collection annotated natural language inference data efficient transfer learn facilitate related task paper show joint learning multiple task result good generalizable sentence representation conduct extensive experiment analysis compare multi-task single-task learned sentence encoders quantitative analysis use auxiliary task show multi-task learning help embed well semantic information sentence representation compare single-task learning addition compare multi-task sentence encoders contextualized word representation show combine boost performance transfer learning