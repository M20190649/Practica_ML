 multimodal relational tensor network sentiment emotion classification understand affect video segment bring researcher language audio video domains together current multimodal research area deal various technique fuse modality mostly treat segment video independently motivate work zadeh et al. poria et al. present architecture relational tensor network use inter-modal interaction within segment intra-segment also consider sequence segment video model inter-segment inter-modal interaction also generate rich representation text audio modality leverage rich audio linguistic context alongwith fuse fine-grained knowledge base polarity score text present result model cmu-mosei dataset show model outperform many baseline state art method sentiment classification emotion recognition