 hybrid self-attention network machine translation encoder-decoder typical framework neural machine translation nmt different structure develop improve translation performance transformer one promising structure leverage self-attention mechanism capture semantic dependency global view however distinguish relative position different token well token locate left right current token focus local information around current token either alleviate problem propose novel attention mechanism name hybrid self-attention network hysan accommodate specific-designed mask self-attention network extract various semantic global local information left right part context finally squeeze gate introduce combine different kind sans fusion experimental result three machine translation task show propose framework outperform transformer baseline significantly achieve superior result state-of-the-art nmt system