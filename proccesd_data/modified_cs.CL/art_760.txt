 generalized language model combination skipped n-grams modify kneser-ney smoothing introduce novel approach building language model base systematic recursive exploration skip n-gram model interpolate use modify kneser-ney smoothing approach generalize language model contain classical interpolation low order model special case paper motivate formalize present approach extensive empirical experiment english text corpus demonstrate generalized language model lead substantial reduction perplexity comparison traditional language model use modified kneser-ney smoothing furthermore investigate behaviour three language domain specific corpus observe consistent improvement finally also show strength approach lie ability cope particular sparse training data use small training data set kb text yield improvement even reduction perplexity