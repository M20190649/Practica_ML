 learningword embeddings low-resource language pu learning word embedding key component many downstream application process natural language exist approach often assume existence large collection text learn effective word embedding however corpus may available low-resource language paper study effectively learn word embed model corpus million token situation co-occurrence matrix sparse co-occurrence many word pair unobserved contrast exist approach often sample unobserved word pair negative sample argue zero entry co-occurrence matrix also provide valuable information design positive-unlabeled learning pu-learning approach factorize co-occurrence matrix validate propose approach four different language