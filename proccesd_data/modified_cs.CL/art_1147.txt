 abstractive text summarization use sequence-to-sequence rnns beyond work model abstractive text summarization use attentional encoder-decoder recurrent neural network show achieve state-of-the-art performance two different corpus propose several novel model address critical problem summarization adequately model basic architecture model key-words capture hierarchy sentence-to-word structure emit word rare unseen training time work show many propose model contribute improvement performance also propose new dataset consisting multi-sentence summary establish performance benchmark research