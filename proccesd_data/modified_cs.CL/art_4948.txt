 simplify neural machine translation addition-subtraction twin-gated recurrent network paper propose additionsubtraction twin-gated recurrent network atr simplify neural machine translation recurrent unit atr heavily simplify small number weight matrix among unit exist gate rnns simple addition subtraction operation introduce twin-gated mechanism build input forget gate highly correlate despite simplification essential non-linearities capability model long-distance dependency preserve additionally propose atr transparent lstm gru due simplification forward self-attention easily establish atr make propose network interpretable experiment wmt translation task demonstrate atr-based neural machine translation yield competitive performance english- german english-french language pair term translation quality speed experiment nist chinese-english translation natural language inference chinese word segmentation verify generality applicability atr different natural language processing task