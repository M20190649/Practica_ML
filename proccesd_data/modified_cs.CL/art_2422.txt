 sequential recurrent neural network language model feedforward neural network fnn -based language model estimate probability next word base history last n word whereas recurrent neural network rnn perform task base last word context information cycle network paper present novel approach bridge gap two category network particular propose architecture take advantage explicit sequential enumeration word history fnn structure enhance word representation projection layer recurrent context information evolve network context integration perform use additional word-dependent weight matrix also learn training extensive experiment conduct penn treebank ptb large text compression benchmark ltcb corpus show significant reduction perplexity compare state-of-the-art feedforward well recurrent neural network architecture