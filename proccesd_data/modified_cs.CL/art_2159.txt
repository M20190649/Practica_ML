 embed word sense together via joint knowledge-enhanced training word embeddings widely use natural language processing mainly due success capture semantic information massive corpus however creation process allow different meaning word automatically separate conflate single vector address issue propose new model learn word sense embeddings jointly model exploit large corpus knowledge semantic network order produce unified vector space word sense embeddings evaluate main feature approach qualitatively quantitatively variety task highlight advantage propose method comparison state-of-the-art word- sense-based model