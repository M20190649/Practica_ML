 high-dimensional dynamic generalization error neural network perform average case analysis generalization dynamic large neural network train use gradient descent study practically-relevant high-dimensional regime number free parameter network order even large number example dataset use random matrix theory exact solution linear model derive generalization error training error dynamic learning analyze depend dimensionality data signal noise ratio learning problem find dynamic gradient descent learn naturally protect overtraining overfitting large network overtraining worst intermediate network size effective number free parameter equal number sample thus reduce make network small large additionally high-dimensional regime low generalization error require start small initial weight turn non-linear neural network show make network large harm generalization performance contrary fact reduce overtraining even without early stopping regularization sort identify two novel phenomenon underlie behavior overcomplete model first frozen subspace weight learning occur gradient descent second statistical property high-dimensional regime yield better-conditioned input correlation protect overtraining demonstrate naive application worst-case theory rademacher complexity inaccurate predict generalization performance deep neural network derive alternative bound incorporate frozen subspace conditioning effect qualitatively match behavior observe simulation