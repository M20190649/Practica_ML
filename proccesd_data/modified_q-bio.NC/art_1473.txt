 sampling-based probabilistic inference emerges learn neural circuit cost reliability neural response cortex change time systematically due ongoing plasticity learning seemingly randomly due various source noise variability previous work consider process learn variability isolation -- study neural network exhibit show interaction lead emergence powerful computational property train neural network classical unsupervised learning task objective represent input efficient easily decodable form additional cost neural reliability derive basic biophysical consideration cost reliability introduce tradeoff energetically cheap inaccurate representation energetically costly accurate one despite learning task non-probabilistic network solve tradeoff develop probabilistic representation neural variability represent sample statistically appropriate posterior distribution would result perform probabilistic inference input provide analytical understanding result reveal connection cost reliability objective state-of-the-art bayesian inference strategy variational autoencoders show cost lead emergence increasingly accurate probabilistic representation network become complex single-layer feed-forward multi-layer feed-forward recurrent architecture result provide insight neural response sensory area show signature sampling-based probabilistic representation may inform future deep learning algorithm implementation stochastic low-precision compute system