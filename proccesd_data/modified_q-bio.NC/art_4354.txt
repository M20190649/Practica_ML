 hebbian anti-hebbian neural network linear subspace learning derivation multidimensional scaling stream data neural network model early sensory processing typically reduce dimensionality stream input data network learn principal subspace sense principal component analysis pca adjust synaptic weight accord activity-dependent learning rule derive principled cost function rule nonlocal hence biologically implausible time biologically plausible local rule postulate rather derive principled cost function bridge gap derive biologically plausible network subspace learn stream data minimize principled cost function departure previous work cost quantify representation reconstruction error adopt multidimensional scaling md cost function stream data result algorithm relies biologically plausible hebbian anti-hebbian local learning rule stochastic setting synaptic weight converge stationary state project input data onto principal subspace data generate nonstationary distribution network track principal subspace thus result make step towards algorithmic theory neural computation