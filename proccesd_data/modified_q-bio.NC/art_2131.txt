 hebbian learning recurrent connection geometrical perspective show hopfield network modifiable recurrent connection undergo slow hebbian learning extract underlying geometry input space first use slow fast analysis derive averaged system whose dynamic derive energy function therefore always converge equilibrium point equilibrium reflect correlation structure input global object extract local recurrent interaction second use numerical method illustrate learning extract hidden geometrical structure input indeed multidimensional scaling method make possible project final connectivity matrix distance matrix high-dimensional space neuron label spatial position within space resulting network structure turn roughly convolutional residual projection define non-convolutional part connectivity minimize process finally show restrict dimension space neuron live give rise pattern similar cortical map motivate use energy efficiency argument base wire length minimization finally show approach lead emergence ocular dominance orientation column primary visual cortex addition establish non-convolutional long-range connectivity patchy co-aligned case orientation learning