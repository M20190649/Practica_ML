 unsupervised feature learn finite data message passing discontinuous versus continuous phase transition unsupervised neural network learn extract hidden feature unlabeled training data use pretraining step supervise learn deep network hence understand unsupervised learning fundamental importance study unsupervised learning finite number data base restrict boltzmann machine learning study inspire efficient message pass algorithm infer hidden feature estimate entropy candidate feature consistent data analysis reveals learning require data feature salient extensively many feature weak moreover entropy candidate feature monotonically decrease data size become negative i.e. entropy crisis message pass becomes unstable suggest discontinuous phase transition term convergence time message pass algorithm unsupervised learning exhibit easy-hard-easy phenomenon training data size increase property reproduce approximate hopfield model exception entropy crisis absent continuous phase transition observe key difference also confirm handwritten digit dataset study deepen understanding unsupervised learning finite number data may provide insight role train deep network