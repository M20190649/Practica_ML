 pesky learning rate performance stochastic gradient descent sgd depend critically learning rate tune decrease time propose method automatically adjust multiple learn rate minimize expected error one time method relies local gradient variation across sample approach learn rate increase well decrease make suitable non-stationary problem use number convex non-convex learning task show result algorithm match performance sgd adaptive approach best setting obtain systematic search effectively remove need learn rate tuning