 fast asynchronous parallel stochastic gradient decent stochastic gradient descent sgd variant become popular machine learning due efficiency effectiveness handle large-scale problem researcher recently propose several parallel sgd method multicore system however exist parallel sgd method achieve satisfactory performance real application paper propose fast asynchronous parallel sgd method call asysvrg design asynchronous strategy parallelize recently propose sgd variant call stochastic variance reduce gradient svrg theoretical empirical result show asysvrg outperform exist state-of-the-art parallel sgd method like hogwild term convergence rate computation cost