 bridge information criterion parameter shrinkage model selection model selection base classical information criterion bic generally computationally demand property well study hand model selection base parameter shrinkage ell -type penalty computationally efficient paper make attempt combine strength propose simple approach penalize likelihood data-dependent ell penalty adaptive lasso exploit fixed penalization parameter even finite sample model selection result approximately coincide base information criterion particular show special case approach corresponding information criterion produce exactly model one also consider approach way directly determine penalization parameter adaptive lasso achieve information criteria-like model selection extension apply idea complex model include gaussian mixture model mixture factor analyzer whose model selection traditionally difficult adopt suitable penalty provide continuous approximators corresponding information criterion easy optimize enable efficient model selection