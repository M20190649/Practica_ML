 deep distributed random sampling supervised learning alternative random forest cite zhang nonlinear zhang nonlinear view machine learning coding dimensionality reduction problem propose simple unsupervised dimensionality reduction method entitle deep distributed random sampling ddrs paper extend supervise learn incrementally key idea incorporate label information cod process reformulate center ddrs multiple output unit indicate class center belong supervised learning method seem somewhat similar random forest cite breiman random emphasize difference follow layer method consider relationship part data point train data training data point random forest focus building decision tree part train data point independently ii method build gradually-narrowed network sample less less data point random forest build gradually-narrowed network merge subclass iii method train straightforward bottom layer top layer random forest build tree top layer bottom layer split iv method encode output target implicitly sparse code random forest encode output target remember class attribute activated node therefore method simpler straightforward maybe well alternative choice though method use two basic element -- -randomization near neighbor optimization -- -as core preprint use protect incremental idea cite zhang nonlinear zhang nonlinear full empirical evaluation announce carefully later