 m gd mini-batch semi-stochastic gradient descent proximal setting propose mini-batching scheme improve theoretical complexity practical performance semi-stochastic gradient descent apply problem minimize strongly convex composite function represent sum average large number smooth convex function simple nonsmooth convex function method first perform deterministic step computation gradient objective function start point follow large number stochastic step process repeat time last iterate become new start point novelty method introduction mini-batching computation stochastic step step instead choose single function sample b function compute gradient compute direction base analyze complexity method show method benefit two speedup effect first prove long b certain threshold reach predefined accuracy less overall work without mini-batching second mini-batching scheme admit simple parallel implementation hence suitable acceleration parallelization