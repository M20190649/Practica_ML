 beat minimax rate active learning prior knowledge active learn refers learning protocol learner allow choose subset instance labeling previous study show compare passive learning active learning able reduce label complexity exponentially data linearly separable satisfy tsybakov noise condition parameter kappa paper propose novel active learning algorithm use convex surrogate loss goal broaden case active learning achieve exponential improvement make use convex loss reduce computational cost importantly lead tight bound empirical process i.e. difference empirical estimation expectation current solution close optimal one assumption norm optimal classifier minimize convex risk available analysis show introduction convex surrogate loss yield exponential reduction label complexity even parameter kappa tsybakov noise large best knowledge first work improve minimax rate active learning utilize certain priori knowledge