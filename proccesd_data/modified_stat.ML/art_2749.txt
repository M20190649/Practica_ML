 stochastic optimization importance sample uniform sampling training data commonly use traditional stochastic optimization algorithm proximal stochastic gradient descent prox-sgd proximal stochastic dual coordinate ascent prox-sdca although uniform sampling guarantee sample stochastic quantity unbiased estimate corresponding true quantity result estimator may rather high variance negatively affect convergence underlying optimization procedure paper study stochastic optimization importance sampling improve convergence rate reduce stochastic variance specifically study prox-sgd actually stochastic mirror descent importance sampling prox-sdca importance sampling prox-sgd instead adopt uniform sample throughout training process propose algorithm employ importance sample minimize variance stochastic gradient prox-sdca propose importance sample scheme aim achieve high expect dual value dual coordinate ascent step provide extensive theoretical analysis show convergence rate propose importance sample method significantly improve suitable condition prox-sgd prox-sdca experiment provide verify theoretical analysis