 efficient marginal likelihood computation gaussian process regression bayesian learning setting posterior distribution predictive model arises trade-off prior distribution conditional likelihood observed data distribution function usually rely additional hyperparameters need tune order achieve optimum predictive performance operation efficiently perform empirical bayes fashion maximize posterior marginal likelihood observed data since score function optimization problem general characterize presence local optimum necessary resort global optimization strategy require large number function evaluation give evaluation usually computationally intensive badly scale respect dataset size maximum number observation treat simultaneously quite limited paper consider case hyperparameter tune gaussian process regression straightforward implementation posterior log-likelihood model require n operation every iteration optimization procedure n number example input dataset derive novel set identity allow initial overhead n evaluation score function well jacobian hessian matrix n operation prove propose identity follow eigendecomposition kernel matrix yield reduction several order magnitude computation time hyperparameter optimization problem notably propose solution provide computational advantage even respect state art approximation rely sparse kernel matrix