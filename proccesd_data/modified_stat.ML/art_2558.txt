 fast gradient descent drift least square regression application bandits online learn algorithms require often recompute least square regression estimate parameter study improve computational complexity algorithm use stochastic gradient descent sgd type scheme place classic regression solver show sgd scheme efficiently track true solution regression problem even presence drift finding couple improvement complexity dimension data make attractive implementation big data setting case strong convexity regression problem guarantee provide bound error expectation high probability latter often need provide theoretical guarantee high level algorithm despite drift least squares solution example case prove regret performance sgd version pege linear bandit algorithm rusmevichientong tsitsiklis bad pege factor log n strong convexity regression problem guarantee investigate use adaptive regularisation make empirical study adaptively regularise sgd version linucb li et al news article recommendation application use large scale news recommendation dataset yahoo front page experiment show large gain computational complexity consistently low track error click-through-rate ctr performance close