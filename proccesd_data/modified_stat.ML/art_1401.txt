 convergence analysis class practical variance-reduction stochastic gradient mcmc stochastic gradient markov chain monte carlo sg-mcmc develop flexible family scalable bayesian sample algorithm however little theoretical analysis impact minibatch size algorithm 's convergence rate paper prove limited computational budget time large minibatch size lead fast decrease mean square error bound thus fast one correspond use full gradient motivate necessity variance reduction sg-mcmc consequently borrow idea stochastic optimization propose practical variance-reduction technique sg-mcmc efficient computation storage develop theory prove algorithm induce fast convergence rate standard sg-mcmc number large-scale experiment range bayesian learning logistic regression deep neural network validate theory demonstrate superiority propose variance-reduction sg-mcmc framework