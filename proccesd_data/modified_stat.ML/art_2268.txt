 learn rank bregman divergence monotone retargeting paper introduce novel approach learn rank letor base notion monotone retargeting involve minimize divergence monotonic increase transformation training score parameterized prediction function minimization transformation well parameter apply bregman divergence large class distance like function recently show unique class statistically consistent normalized discount gain ndcg criterion algorithm us alternate projection style update one set simultaneous projection compute independent bregman divergence reduces parameter estimation generalized linear model result easily implement efficiently parallelizable algorithm letor task enjoy global optimum guarantee mild condition present empirical result benchmark datasets show approach outperform state art ndcg consistent technique