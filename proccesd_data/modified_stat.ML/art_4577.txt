 learn kernel matrix via predictive low-rank approximation efficient accurate low-rank approximation multiple data source essential era big data scaling kernel-based learn algorithm large datasets limit n computation storage complexity full kernel matrix require recent kernel learn algorithm present mklaren algorithm approximate multiple kernel matrix learn regression model entirely base geometrical concept algorithm require access full kernel matrix yet account correlation kernel use incomplete cholesky decomposition pivot selection base least-angle regression combined low-dimensional feature space algorithm linear complexity number data point kernel explicit feature space induce kernel construct mapping dual primal ridge regression weight use model interpretation mklaren algorithm test eight standard regression datasets outperform contemporary kernel matrix approximation approach learn multiple kernel identify relevant kernel achieve high explain variance multiple kernel learn method number iteration test accuracy equivalent one use full kernel matrix achieve significantly low approximation rank difference run time two order magnitude observe either number sample kernel exceeds