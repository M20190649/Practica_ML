 optimal learning rate localized svms one limiting factor use support vector machine svms large scale application super-linear computational requirement term number training sample address issue several approach train svms many small chunk large data set separately propose literature far however almost approach empirically investigate addition motivation always base computational requirement work consider localized svm approach base upon partition input space local svm derive general oracle inequality apply oracle inequality least square regression use gaussian kernel deduce local learn rate essentially minimax optimal standard smoothness assumption regression function give first motivation use local svms base computational requirement theoretical prediction generalization performance introduce data-dependent parameter selection method local svm approach show method achieve learning rate finally present large scale experiment localized svm show achieve essentially test performance global svm fraction computational requirement addition turn computational requirement local svms similar vanilla random chunk approach achieved test error significantly well