 sparse partially collapse mcmc parallel inference topic model topic model specifically class latent dirichlet allocation lda widely use probabilistic modeling text mcmc sample posterior distribution typically perform use collapsed gibbs sampler propose parallel sparse partially collapse gibbs sampler compare speed efficiency state-of-the-art sampler topic model five well-known text corpus differ size property particular propose compare two different strategy sample parameter block latent topic indicator experiment show increase statistical inefficiency partial collapsing small commonly assume compensate speedup parallelization sparsity large corpus also prove partially collapsed sampler scale well size corpus propose algorithm fast efficient exact use modeling situation ordinary collapse sampler