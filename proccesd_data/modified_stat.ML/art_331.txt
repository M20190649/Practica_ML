 combine l greedy l penalize least square linear model selection introduce computationally effective algorithm linear model selection consist three step screening -- order -- selection so screening predictor base thresholded lasso l penalize least square screened predictor fit use least square ls order respect statistic finally model select use greedy generalize information criterion gic l penalized l nested family induce ordering give non-asymptotic upper bound error probability step so algorithm term penalty obtain selection consistency different n p scenario condition need screen consistency lasso traditional setting n p give sanov-type bound error probability ordering -- selection algorithm surprising consequence selection error greedy gic asymptotically large exhaustive gic also obtain new bound prediction estimation error lasso prove parallel algorithm use practice formal version