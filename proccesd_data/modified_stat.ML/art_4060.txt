 zipml framework train model end-to-end low precision can cannots little bit deep learning recently significant interest train machine-learning model low precision reduce precision one reduce computation communication one order magnitude examine train reduce precision theoretical practical perspective ask possible train model end-to-end low precision provable guarantee lead consistent order-of-magnitude speedup present framework call zipml answer question linear model answer yes develop simple framework base one simple novel strategy call double sampling framework able execute train low precision bias guarantee convergence whereas naive quantization would introduce significant bias validate framework across range application show enable fpga prototype x faster implementation use full -bit precision develop variance-optimal stochastic quantization strategy show make significant difference variety setting apply linear model together double sampling save another x data movement compare uniform quantization train deep network quantized model achieve high accuracy state-of-the-art xnor-net finally extend framework approximation non-linear model svm show although use low-precision data induces bias appropriately bind control bias find practice -bit precision often sufficient converge correct solution interestingly however practice notice framework always outperform naive rounding approach discuss negative result detail