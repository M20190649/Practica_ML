 accelerate stochastic composition optimization consider stochastic composition optimization problem objective composition two expected-value function propose new stochastic first-order method namely accelerated stochastic compositional proximal gradient asc-pg method update base query sample oracle use two different timescales asc-pg first proximal gradient method stochastic composition problem deal nonsmooth regularization penalty show asc-pg exhibit faster convergence best known algorithm achieve optimal sample-error complexity several important special case demonstrate application asc-pg reinforcement learning conduct numerical experiment