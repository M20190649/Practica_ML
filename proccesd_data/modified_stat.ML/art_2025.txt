 constrain approximate maximum entropy learning markov random field parameter estimation markov random field mrfs difficult task inference network run inner loop gradient descent procedure replace exact inference approximate method loopy belief propagation lbp suffer poor convergence paper provide different approach combine mrf learning bethe approximation consider dual maximum likelihood markov network learn maximize entropy moment match constraint approximate objective constraint result optimization problem unlike previous work along line teh welling formulation allow parameter share feature general log-linear model parameter regularization conditional training show piecewise training sutton mccallum restricted special case formulation study two optimization strategy one base single convex approximation one use repeat convex approximation show result several real-world network demonstrate algorithm significantly outperform learn loopy piecewise result also provide framework analyze trade-off different relaxation entropy objective constraint