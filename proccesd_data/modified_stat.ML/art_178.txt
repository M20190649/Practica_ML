 fast learn rate non-sparse multiple kernel learn optimal regularization strategy paper give new generalization error bound multiple kernel learn mkl general class regularization discuss kind regularization give favorable predictive accuracy main target paper dense type regularization include ellp-mkl accord recent numerical experiment sparse regularization necessarily show good performance compare dense type regularization motivate fact paper give general theoretical tool derive fast learning rate mkl applicable arbitrary mixed-norm-type regularization unifying manner enable u compare generalization performance various type regularization consequence observe homogeneity complexity candidate reproducing kernel hilbert space rkhss affect regularization strategy ell dense prefer fact homogeneous complexity setting complexity rkhss evenly ell -regularization optimal among isotropic norm hand inhomogeneous complexity setting dense type regularization show well learn rate sparse ell -regularization also show learn rate achieve minimax low bound homogeneous complexity setting