 unified convergence analysis stochastic momentum method convex non-convex optimization recently stochastic momentum method widely adopt train deep neural network however convergence analysis still underexplored moment particular non-convex optimization paper fill gap practice theory develop basic convergence analysis two stochastic momentum method namely stochastic heavy-ball method stochastic variant nesterov 's accelerated gradient method hope basic convergence result develop paper serve reference convergence stochastic momentum method also serve baseline comparison future development stochastic momentum method novelty convergence analysis present paper unified framework reveal insight similarity difference different stochastic momentum method stochastic gradient method unified framework exhibit continuous change gradient method nesterov 's accelerated gradient method finally heavy-ball method incur free parameter help explain similar change observe test error convergence behavior deep learning furthermore empirical result optimize deep neural network demonstrate stochastic variant nesterov 's accelerated gradient method achieve good tradeoff speed convergence train error robustness convergence test error among three stochastic method