 sharp convergence rate support consistency multiple kernel learn sparse dense regularization theoretically investigate convergence rate support consistency i.e. correctly identify subset non-zero coefficient large sample limit multiple kernel learn mkl focus mkl block-l regularization induce sparse kernel combination block-l regularization induce uniform kernel combination elastic-net regularization include block-l block-l regularization case true kernel combination sparse show sharp convergence rate block-l elastic-net mkl method exist rate block-l mkl show elastic-net mkl require milder condition consistent block-l mkl case optimal kernel combination exactly sparse prove elastic-net mkl achieve fast convergence rate block-l block-l mkl method carefully control balance block-l block-l regularizers thus theoretical result overall suggest use elastic-net regularization mkl