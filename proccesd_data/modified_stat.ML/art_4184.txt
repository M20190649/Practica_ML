 stochastic training neural network via successive convex approximation paper propose new family algorithm train neural network nns base recent development field non-convex optimization go general name successive convex approximation sca technique basic idea iteratively replace original non-convex highly dimensional learning problem sequence strongly convex approximation accurate simple optimize differently similar idea e.g. quasi-newton algorithm approximation construct use first-order information neural network function stochastic fashion exploit overall structure learning problem fast convergence discuss several use case base different choice loss function e.g. square loss cross-entropy loss regularization nn 's weight experiment several medium-sized benchmark problem large-scale dataset involve simulated physical data result show algorithm outperform state-of-the-art technique provide faster convergence good minimum additionally show algorithm easily parallelize multiple computational unit without hinder performance particular computational unit optimize tailored surrogate function define randomly assign subset input variable whose dimension select depend entirely available computational power