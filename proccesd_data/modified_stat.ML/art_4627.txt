 stabilized sparse online learn sparse data stochastic gradient descent sgd commonly use optimization large-scale machine learning problem langford et al introduce sparse online learn method induce sparsity via truncated gradient high-dimensional sparse data however method suffers slow convergence high variance due heterogeneity feature sparsity mitigate issue introduce stabilize truncate stochastic gradient descent algorithm employ soft-thresholding scheme weight vector imposed shrinkage adaptive amount information available feature variability resulted sparse weight vector control stability selection integrate informative truncation facilitate good convergence adopt anneal strategy truncation rate lead balance trade-off exploration exploitation learn sparse weight vector numerical experiment show algorithm compare favorably original algorithm term prediction accuracy achieve sparsity stability