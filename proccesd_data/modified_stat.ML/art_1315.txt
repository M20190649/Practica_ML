 structured bayesian prune via log-normal multiplicative noise dropout-based regularization method regard inject random noise pre-defined magnitude different part neural network training recently show bayesian dropout procedure improve generalization also lead extremely sparse neural architecture automatically set individual noise magnitude per weight however sparsity hardly use acceleration since unstructured paper propose new bayesian model take account computational structure neural network provide structured sparsity e.g remove neuron convolutional channel cnns inject noise neuron output keep weight unregularized establish probabilistic model proper truncate log-uniform prior noise truncate log-normal variational approximation ensure kl-term evidence low bound compute closed-form model lead structure sparsity remove element low snr computation graph provide significant acceleration number deep neural architecture model easy implement formulate separate dropout-like layer