 bellman residual bad proxy paper aim theoretically empirically compare two standard optimization criterion reinforcement learning maximization mean value ii minimization bellman residual purpose place framework policy search algorithm usually design maximize mean value derive method minimize residual v pi v pi nu policy theoretical analysis show good proxy policy optimization notably well value-based counterpart also propose experiment randomly generate generic markov decision process specifically design study influence involved concentrability coefficient show bellman residual generally bad proxy policy optimization directly maximize mean value much well despite current lack deep theoretical analysis might seem obvious directly address problem interest usually well give prevalence project bellman residual minimization value-based reinforcement learning believe question worth consider