 regularization nonlinearities neural language model need neural language model lms base recurrent neural network rnn successful word character-level lm work well particular good linear neural lm possible explanation rnns implicitly good regularization rnns high capacity store pattern due nonlinearities argue first explanation limit little training data second explanation large amount text data show state-of-the-art performance popular small penn dataset rnn lm regularize random dropout nonetheless show even good performance simplify much less expressive linear rnn model without off-diagonal entry recurrent matrix call model impulse-response lm irlm use random dropout column normalization anneal learning rate irlms develop neuron keep memory word past achieve perplexity penn dataset two large datasets however regularization method unsuccessful model rnn 's expressivity allow overtake irlm percent perplexity respectively despite perplexity gap irlms still outperform rnns microsoft research sentence completion mrsc task develop slightly modify irlm separate long-context unit lcus short-context unit show lcus alone achieve state-of-the-art performance mrsc task analysis indicate fruitful direction research neural lm lie develop accessible internal representation suggest optimization regime high momentum term effectively train model