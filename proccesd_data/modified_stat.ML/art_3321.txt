 efficient non-parametric estimation multiple embeddings per word vector space rise interest vector-space word embeddings use nlp especially give recent method fast estimation large scale nearly work however assume single vector per word type ignore polysemy thus jeopardize usefulness downstream task present extension skip-gram model efficiently learn multiple embeddings per word type differ recent relate work jointly perform word sense discrimination embed learning non-parametrically estimate number sens per word type efficiency scalability present new state-of-the-art result word similarity context task demonstrate scalability train one machine corpus nearly billion token less hour