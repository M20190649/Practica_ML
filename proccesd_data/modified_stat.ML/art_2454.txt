 marginal likelihood distributed parameter estimation gaussian graphical model consider distributed estimation inverse covariance matrix also call concentration precision matrix gaussian graphical model traditional centralized estimation often require global inference covariance matrix computationally intensive large dimension approximate inference base message-passing algorithm hand lead unstable biased estimation loopy graphical model paper propose general framework distribute estimation base maximum marginal likelihood mml approach approach compute local parameter estimate maximize marginal likelihood define respect data collect local neighborhood due non-convexity mml problem introduce solve convex relaxation local estimate combine global estimate without need iterative message-passing neighborhood propose algorithm naturally parallelizable computationally efficient thereby make suitable high-dimensional problem classical regime number variable p fix number sample increase infinity propose estimator show asymptotically consistent improve monotonically local neighborhood size increase high-dimensional scaling regime p increase infinity convergence rate true parameter derive see comparable centralize maximum likelihood estimation extensive numerical experiment demonstrate improved performance two-hop version propose estimator suffice almost close gap centralize maximum likelihood estimator reduced computational cost