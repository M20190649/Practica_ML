 group sparse regularization deep neural network paper consider joint task simultaneously optimize weight deep neural network ii number neuron hidden layer iii subset active input feature i.e. feature selection problem generally deal separately present simple regularized formulation allow solve three parallel use standard optimization routine specifically extend group lasso penalty originate linear regression literature order impose group-level sparsity network 's connection group define set outgo weight unit depend specific case weight relate input variable hidden neuron bias unit thus perform simultaneously aforementioned task order obtain compact network perform extensive experimental evaluation compare classical weight decay lasso penalty show sparse version group lasso penalty able achieve competitive performance time result extremely compact network small number input feature evaluate toy dataset handwritten digit recognition multiple realistic large-scale classification problem