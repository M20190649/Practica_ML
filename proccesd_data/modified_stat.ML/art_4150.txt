 self-normalizing neural network deep learning revolutionize vision via convolutional neural network cnns natural language processing via recurrent neural network rnns however success story deep learning standard feed-forward neural network fnns rare fnns perform well typically shallow therefore exploit many level abstract representation introduce self-normalizing neural network snns enable high-level abstract representation batch normalization require explicit normalization neuron activation snns automatically converge towards zero mean unit variance activation function snns scale exponential linear unit selus induce self-normalizing property use banach fixed-point theorem prove activation close zero mean unit variance propagate many network layer converge towards zero mean unit variance -- even presence noise perturbation convergence property snns allows train deep network many layer employ strong regularization make learn highly robust furthermore activation close unit variance prove upper low bound variance thus vanish explode gradient impossible compare snns task uci machine learn repository b drug discovery benchmark c astronomy task standard fnns machine learn method random forest support vector machine snns significantly outperform compete fnn method uci task outperform compete method tox dataset set new record astronomy data set win snn architecture often deep implementation available github.com bioinf-jku snns