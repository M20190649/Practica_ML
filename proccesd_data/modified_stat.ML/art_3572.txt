 distil model knowledge top-performing machine learn system deep neural network large ensemble complex probabilistic graphical model expensive store slow evaluate hard integrate large system ideally would like replace cumbersome model simpler model perform equally well thesis study knowledge distillation idea extract knowledge contain complex model inject convenient model present general framework knowledge distillation whereby convenient model choose learn mimic complex model observe latter 's behaviour penalize whenever fail reproduce develop framework within context three distinct machine learn application model compression compress large discriminative model ensemble neural network model much small size b compact predictive distribution bayesian inference distil large bag mcmc sample compact predictive distribution closed form c intractable generative model distil unnormalizable model rbms tractable model nades contribute state art novel technique idea model compression describe implement derivative matching allow good distillation data scarce compact predictive distribution introduce online distillation allow significant saving memory finally intractable generative model show use distilled model robustly estimate intractable quantity original model intractable partition function