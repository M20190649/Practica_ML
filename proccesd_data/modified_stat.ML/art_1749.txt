 generalize boosting algorithm convex optimization boosting popular way derive powerful learner simple hypothesis class follow previous work mason et al. friedman general boosting framework analyze gradient-based descent algorithm boost respect convex objective introduce new measure weak learner performance setting generalize exist work present weak strong learning guarantee exist gradient boost work strongly-smooth strongly-convex objective new measure performance also demonstrate work fail non-smooth objective address issue present new algorithm extend boost approach arbitrary convex loss function give correspond weak strong convergence result addition demonstrate experimental result support analysis demonstrate need new algorithm present