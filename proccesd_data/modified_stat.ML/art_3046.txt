 regularize recurrent network injected noise norm-based method advancement parallel processing lead surge multilayer perceptrons mlp application deep learning past decade recurrent neural network rnns give additional representational power feedforward mlps provide way treat sequential data however rnns hard train use conventional error backpropagation method difficulty relate input many time-steps regularization approach mlp sphere like dropout noisy weight training insufficiently apply test simple rnns moreover solution propose improve convergence rnns enough improve long term dependency remember capability thereof study aim empirically evaluate remembering generalization ability rnns polyphonic musical datasets model train injected noise random dropout norm-based regularizers respective performance compare well-initialized plain rnns advanced regularization method like fast-dropout conclude evidence train noise improve performance conjecture work rnn optimization