 information-theoretic semi-supervised metric learn via entropy regularization propose general information-theoretic approach call seraph semi-supervised metric learn paradigm hyper-sparsity metric learning rely upon manifold assumption give probability parameterized mahalanobis distance maximize entropy probability label data minimize unlabeled data follow entropy regularization allow supervised unsupervised part integrate natural meaningful way furthermore seraph regularize encourage low-rank projection induce metric optimization seraph solve efficiently stably em-like scheme analytical e-step convex m-step experiment demonstrate seraph compare favorably many well-known global local metric learn method