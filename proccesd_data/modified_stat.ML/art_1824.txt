 krylov subspace descent deep learning paper propose second order optimization method learn model dimensionality parameter space number training sample high method construct iteration krylov subspace form gradient approximation hessian matrix use subset training data sample optimize subspace hessian free hf method hessian matrix never explicitly construct compute use subset data practice hf typically use positive definite substitute hessian matrix gauss-newton matrix investigate effectiveness propose method deep neural network compare performance widely use method stochastic gradient descent conjugate gradient descent l-bfgs also hf method lead faster convergence either l-bfgs hf generally performs good either cross-validation accuracy also simple general hf require positive semi-definite approximation hessian matrix work well setting damping parameter chief drawback versus hf need memory store basis krylov subspace