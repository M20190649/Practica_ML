 layer normalization train state-of-the-art deep neural network computationally expensive one way reduce training time normalize activity neuron recently introduce technique call batch normalization use distribution summed input neuron mini-batch training case compute mean variance use normalize summed input neuron training case significantly reduce training time feed-forward neural network however effect batch normalization dependent mini-batch size obvious apply recurrent neural network paper transpose batch normalization layer normalization compute mean variance use normalization sum input neuron layer single training case like batch normalization also give neuron adaptive bias gain apply normalization non-linearity unlike batch normalization layer normalization performs exactly computation training test time also straightforward apply recurrent neural network compute normalization statistic separately time step layer normalization effective stabilize hidden state dynamic recurrent network empirically show layer normalization substantially reduce training time compare previously publish technique