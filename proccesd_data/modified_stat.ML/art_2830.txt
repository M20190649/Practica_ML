 proximal stochastic gradient method progressive variance reduction consider problem minimize sum two convex function one average large number smooth component function general convex function admit simple proximal mapping assume whole objective function strongly convex problem often arise machine learning know regularized empirical risk minimization propose analyze new proximal stochastic gradient method use multi-stage scheme progressively reduce variance stochastic gradient iteration algorithm similar cost classical stochastic gradient method incremental gradient method show expect objective value converges optimum geometric rate overall complexity method much low proximal full gradient method standard proximal stochastic gradient method