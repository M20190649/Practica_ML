 hamsi parallel incremental optimization algorithm use quadratic approximation solve partially separable problem propose hamsi hessian approximate multiple subset iteration provably convergent second order incremental algorithm solve large-scale partially separable optimization problem algorithm base local quadratic approximation hence allow incorporate curvature information speed-up convergence hamsi inherently parallel scale nicely number processor combine technique effectively utilizing modern parallel computer architecture illustrate propose method converge rapidly parallel stochastic gradient descent method use solve large-scale matrix factorization problem performance gain come expense use memory scale linearly total size optimization variable conclude hamsi may consider viable alternative many large scale problem first order method base variant stochastic gradient descent applicable