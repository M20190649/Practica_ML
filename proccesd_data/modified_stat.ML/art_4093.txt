 adaptive accelerate gradient converge method holderian error bound condition recent study show proximal gradient pg method accelerate gradient method apg restart enjoy linear convergence weak condition strong convexity namely quadratic growth condition qgc however fast convergence restart apg method relies potentially unknown constant qgc appropriately restart apg restrict applicability address issue develop novel adaptive gradient converge method i.e. leverage magnitude proximal gradient criterion restart termination analysis extend much general condition beyond qgc namely h lderian error bound heb condition key technique development novel synthesis adaptive regularization conditional restarting scheme extend previous work focus strongly convex problem much broad family problem furthermore demonstrate result important implication application machine learning objective function coercive semi-algebraic pg 's convergence speed essentially frac total number iteration ii objective function consist ell ell infty ell infty huber norm regularization convex smooth piecewise quadratic loss e.g. square loss square hinge loss huber loss propose algorithm parameter-free enjoy faster linear convergence pg without assumption e.g. restrict eigen-value condition notable linear convergence result aforementioned problem global instead local best knowledge improve result first show work