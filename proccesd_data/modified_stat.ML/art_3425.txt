 stochastic expectation propagation expectation propagation ep deterministic approximation algorithm often use perform approximate bayesian parameter learning ep approximate full intractable posterior distribution set local approximation iteratively refine datapoint ep offer analytic computational advantage approximation variational inference vi method choice number model local nature ep appear make ideal candidate perform bayesian learn large model large-scale dataset setting however ep crucial limitation context number approximate factor need increase number data-points n often entail prohibitively large memory overhead paper present extension ep call stochastic expectation propagation sep maintain global posterior approximation like vi update local way like ep experiment number canonical learning problem use synthetic real-world datasets indicate sep performs almost well full ep reduce memory consumption factor n sep therefore ideally suit perform approximate bayesian learning large model large dataset setting