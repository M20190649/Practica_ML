 sparse multiple kernel learn geometric convergence rate paper study problem sparse multiple kernel learn mkl goal efficiently learn combination fix small number kernel large pool could lead kernel classifier small prediction error develop efficient algorithm base greedy coordinate descent algorithm able achieve geometric convergence rate appropriate condition convergence rate achieve measure size functional gradient empirical ell norm depend empirical data distribution contrast previous algorithm use functional norm measure size gradient independent data sample also establish generalization error bound learned sparse kernel classifier use technique local rademacher complexity