 fast rate statistical online learn speed learning algorithm converge present data central problem machine learning -- fast rate convergence mean less data need level performance pursuit fast rate online statistical learning lead discovery many condition learn theory fast learning possible show condition special case single unifying condition come two form central condition 'proper learning algorithm always output hypothesis give model stochastic mixability online algorithm may make prediction outside model show surprisingly weak assumption condition certain sense equivalent central condition re-interpretation term convexity set pseudoprobabilities link density estimation misspecification bounded loss show central condition enable direct proof fast rate prove equivalence bernstein condition generalization tsybakov margin condition play central role obtain fast rate statistical learning yet bernstein condition two-sided central condition one-sided make suitable deal unbounded loss stochastic mixability form condition generalize stochastic exp-concavity condition identify juditsky rigollet tsybakov vovk 's notion mixability unifying condition thus provide substantial step towards characterization fast rate statistical learning similar classical mixability characterize constant regret sequential prediction expert advice setting