 variance reduction stochastic gradient descent asynchronous variant study optimization algorithm base variance reduction stochastic gradient descent sgd remarkable recent progress make direction development algorithm like sag svrg saga algorithm show outperform sgd theoretically empirically however asynchronous version algorithm -- -a crucial requirement modern large-scale application -- -have study bridge gap present unifying framework many variance reduction technique subsequently propose asynchronous algorithm ground framework prove fast convergence important consequence general approach yield asynchronous version variance reduction algorithm svrg saga byproduct method achieves near linear speedup sparse setting common machine learning demonstrate empirical performance method concrete realization asynchronous svrg