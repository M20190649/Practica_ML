 explore improve high-probability regret bound non-stochastic bandit work address problem regret minimization non-stochastic multi-armed bandit problem focus performance guarantee hold high probability result rather scarce literature since prove require large deal technical effort significant modification standard intuitive algorithm come guarantee hold expectation one modification force learner sample arm uniform distribution least omega sqrt time round adversely affect performance many arm suboptimal widely conjecture property essential prove high-probability regret bound show paper possible achieve strong result without undesirable exploration component result relies simple intuitive loss-estimation strategy call implicit exploration ix allow remarkably clean analysis demonstrate flexibility technique derive several improve high-probability bound various extension standard multi-armed bandit framework finally conduct simple experiment illustrate robustness implicit exploration technique