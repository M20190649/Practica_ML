 importance weight without importance weight efficient algorithm combinatorial semi-bandits propose sample-efficient alternative importance weighting situation one sample access probability distribution generate observation new method call geometric resampling gr describe analyze context online combinatorial optimization semi-bandit feedback learner sequentially select action combinatorial decision set minimize cumulative loss particular show well-known follow-the-perturbed-leader fpl prediction method couple geometric resampling yield first computationally efficient reduction offline online optimization setting provide thorough theoretical analysis result algorithm show performance par previous inefficient solution main contribution show despite relatively large variance induce gr procedure performance guarantee hold high probability rather expectation side result also improve best known regret bound fpl online combinatorial optimization full feedback close perceived performance gap fpl exponential weight setting