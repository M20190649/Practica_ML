 searnn training rnns global-local loss propose searnn novel training algorithm recurrent neural network rnns inspire learning search l approach structure prediction rnns widely successful structured prediction application machine translation parsing commonly trained use maximum likelihood estimation mle unfortunately training loss always appropriate surrogate test error maximize ground truth probability fail exploit wealth information offer structured loss introduce discrepancy training predict exposure bias may hurt test performance instead searnn leverage test-alike search space exploration introduce global-local loss close test error first demonstrate improve performance mle two different task ocr spell correction propose subsampling strategy enable searnn scale large vocabulary size allow u validate benefit approach machine translation task