 distribute delayed stochastic optimization analyze convergence gradient-based optimization algorithm base update delayed stochastic gradient information main application result development gradient-based distributed optimization algorithm master node performs parameter update worker node compute stochastic gradient base local information parallel may give rise delay due asynchrony take motivation statistical problem size data large fit one computer advent huge datasets biology astronomy internet problem common main contribution show smooth stochastic problem delay asymptotically negligible achieve order-optimal convergence result application distribute optimization develop procedure overcome communication bottleneck synchronization requirement show n -node architecture whose optimization error stochastic problem -- -in spite asynchronous delay -- -scales asymptotically order sqrt nt iteration rate know optimal distributed system n node even absence delay additionally complement theoretical result numerical experiment statistical machine learn task