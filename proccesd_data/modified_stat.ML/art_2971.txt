 fastfood approximate kernel expansion loglinear time despite success make kernel method difficult use many large scale problem fact store compute decision function typically expensive especially prediction time paper overcome difficulty propose fastfood approximation accelerate computation significantly key fastfood observation hadamard matrix combine diagonal gaussian matrix exhibit property similar dense gaussian random matrix yet unlike latter hadamard diagonal matrix inexpensive multiply store two matrix use lieu gaussian matrix random kitchen sink propose rahimi recht thereby speed computation large range kernel function specifically fastfood require n log time n storage compute n non-linear basis function dimension significant improvement nd computation storage without sacrifice accuracy method applies translation invariant dot-product kernel popular rbf kernel polynomial kernel prove approximation unbiased low variance experiment show achieve similar accuracy full kernel expansion random kitchen sink x faster use x less memory improvement especially term memory usage make kernel method practical application large train set require real-time prediction