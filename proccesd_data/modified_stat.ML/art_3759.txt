 improve dropout shallow deep learning dropout witness great success train deep neural network independently zero output neuron random also receive surge interest shallow learning e.g. logistic regression however independent sampling dropout could suboptimal sake convergence paper propose use multinomial sampling dropout i.e. sample feature neuron accord multinomial distribution different probability different feature neuron exhibit optimal dropout probability analyze shallow learn multinomial dropout establish risk bound stochastic optimization minimize sampling dependent factor risk bound obtain distribution-dependent dropout sample probability dependent second order statistic data distribution tackle issue evolve distribution neuron deep learning propose efficient adaptive dropout name textbf evolutional dropout compute sample probability on-the-fly mini-batch example empirical study several benchmark datasets demonstrate propose dropout achieve much fast convergence also small test error standard dropout example cifar- data evolutional dropout achieves relative improvement prediction performance convergence speed compare standard dropout