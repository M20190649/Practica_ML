 scalable adaptive stochastic optimization use random projection adaptive stochastic gradient method adagrad gain popularity particular train deep neural network commonly use study variant maintain diagonal matrix approximation second order information accumulate past gradient use tune step size adaptively certain situation full-matrix variant adagrad expect attain good performance however high dimension computationally impractical present ada-lr radagrad two computationally efficient approximation full-matrix adagrad base randomized dimensionality reduction able capture dependency feature achieve similar performance full-matrix adagrad much small computational cost show regret ada-lr close regret full-matrix adagrad up-to exponentially small dependence dimension diagonal variant empirically show ada-lr radagrad perform similarly full-matrix adagrad task train convolutional neural network well recurrent neural network radagrad achieves faster convergence diagonal adagrad