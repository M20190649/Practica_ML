 scale large minibatch sgd residual network train imagenet- k improved accuracy reduced time train past year ilsvrc competition imagenet dataset attract lot interest computer vision community allow state-of-the-art accuracy grow tremendously credit use deep artificial neural network design become complex storage bandwidth compute requirement increase mean non-distributed approach even use high-density server available training process may take week make prohibitive furthermore datasets grow representation learn potential deep network grow well use complex model synchronicity trigger sharp increase computational requirement motivate u explore scale behaviour petaflop scale supercomputer paper describe challenge novel solution need order train resnet- large scale environment demonstrate scale efficiency training time minute use k x core support software tool intel 's ecosystem moreover show regular epoch train run achieve top- accuracy high unmodified resnet- topology also introduce novel collapse ensemble ce technique allow u obtain top- accuracy similar resnet- train unmodified resnet- topology fixed training budget resnet- model well script need replicate post shortly