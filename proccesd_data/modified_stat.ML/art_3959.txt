 framework parallel distributed training neural network aim paper develop general framework train neural network nns distributed environment train data partition set agent communicate sparse possibly time-varying connectivity pattern distribute scenario training problem formulate regularized optimization non-convex social cost function give sum local non-convex cost agent contribute single error term define respect local dataset devise flexible efficient solution customize recently propose framework non-convex optimization network hinge primal convexification-decomposition technique handle non-convexity dynamic consensus procedure diffuse information among agent several typical choice training criterion e.g. square loss cross entropy etc regularization e.g. ell norm sparsity induce penalty etc include framework explore along paper convergence stationary solution social non-convex problem guarantee mild assumption additionally show principled way allow agent exploit possible multi-core architecture e.g. local cloud order parallelize local optimization step result strategy distribute across agent parallel inside agent nature comprehensive set experimental result validate propose approach