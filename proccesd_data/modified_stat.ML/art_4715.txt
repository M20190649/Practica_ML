 merge mcmc subposteriors gaussian-process approximation markov chain monte carlo mcmc algorithm become powerful tool bayesian inference however scale well large-data problem divide-and-conquer strategy split data batch batch run independent mcmc algorithm target corresponding subposterior spread computational burden across number separate worker challenge strategy recombine subposteriors approximate full posterior create gaussian-process approximation log-subposterior density create tractable approximation full posterior approximation exploit three methodology firstly hamiltonian monte carlo algorithm target expectation posterior density provide sample approximation posterior secondly evaluate true posterior sampled point lead importance sampler asymptotically target true posterior expectation finally alternative importance sampler use full gaussian-process distribution approximation log-posterior density re-weight initial sample provide estimate posterior expectation measure uncertainty