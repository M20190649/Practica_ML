 analysis gradient descent method non-diminishing bounded error main aim paper provide analysis gradient descent gd algorithm gradient error necessarily vanish asymptotically particular sufficient condition present stability almost sure boundedness iterates convergence gd bound possibly non-diminishing gradient error addition ensure stability algorithm show converge small neighborhood minimum set depend gradient error worth note main result paper use show gd asymptotically vanish error indeed converges minimum set result present herein general compare previous result analysis gd error new literature best knowledge work extend contribution mangasarian solodov bertsekas tsitsiklis tadic doucet use framework simple yet effective implementation gd use simultaneous perturbation stochastic approximation sp sa constant sensitivity parameter present another important improvement many previous result additional restriction impose step-sizes machine learning application step-sizes relate learn rate assumption unlike paper affect learn rate finally present experimental result validate theory