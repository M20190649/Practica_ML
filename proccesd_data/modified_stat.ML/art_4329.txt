 weightless lossy weight encode deep neural network compression large memory requirement deep neural network limit deployment adoption many device model compression method effectively reduce memory requirement model usually apply transformation weight pruning quantization paper present novel scheme lossy weight encode complement conventional compression technique encoding base bloomier filter probabilistic data structure save space cost introduce random error leverage ability neural network tolerate imperfection re-training around error propose technique weightless compress dnn weight x model accuracy result x improvement state-of-the-art