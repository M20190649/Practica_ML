 temporal autoencoding improves generative model time series restrict boltzmann machine rbms generative model learn useful representation sample dataset unsupervised fashion widely employ unsupervised pre-training method machine learning rbms modify model time series two main way temporal rbm stack number rbms laterally introduce temporal dependency hidden layer unit conditional rbm hand considers past sample dataset conditional bias learn representation take account propose new training method trbm crbm enforce dynamic structure temporal datasets treat temporal model denoising autoencoders consider past frame dataset corrupted version present frame minimize reconstruction error present data model call approach temporal autoencoding lead significant improvement performance model filling-in-frames task across number datasets error reduction motion capture data crbm trbm take posterior mean prediction instead single sample improve model 's estimate decrease error much crbm motion capture data also train model perform forecasting large number datasets find ta pretraining consistently improve performance forecast furthermore look prediction error across time see improvement reflect good representation dynamic data oppose bias towards reconstruct observe data short time scale