 optimal weighting multi-view data low dimensional hidden state natural language processing nlp task data often follow two property first data chop multi-views successfully use dimension reduction purpose example topic classification every paper chop title main text reference however common view less noisier view supervised learning problem second unlabeled data easy obtain label data relatively rare example article occur new york time recent year easy grab classify 'politics 'finance 'sports need human labor hence less noisy feature prefer run supervise learn method paper propose unsupervised algorithm optimally weight feature different view view generate low dimensional hidden state occur widely use model like mixture gaussian model hidden markov model hmm latent dirichlet allocation lda