 convergence contrastive divergence algorithm exponential family contrastive divergence cd algorithm achieve notable success train energy-based model include restricted boltzmann machine play key role emergence deep learning idea algorithm approximate intractable term exact gradient log-likelihood function use short markov chain monte carlo mcmc run approximate gradient computationally-cheap bias whether cd algorithm provide asymptotically consistent estimate still open question paper study asymptotic property cd algorithm canonical exponential family special case energy-based model suppose cd algorithm run mcmc transition step iteration iteratively generate sequence parameter estimate theta ge give i.i.d data sample x n sim p theta star condition commonly obey cd algorithm practice prove existence bound limit point time average leave sum t- theta right infty consistent estimate true parameter theta star proof base fact theta ge homogenous markov chain conditional data sample x n chain meet foster-lyapunov drift criterion converges random walk around maximum likelihood estimate range random walk shrink zero rate mathcal sqrt n sample size n infty