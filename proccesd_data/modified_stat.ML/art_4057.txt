 net-trim convex pruning deep neural network performance guarantee introduce analyze new technique model reduction deep neural network large network theoretically capable learn arbitrarily complex model overfitting model redundancy negatively affect prediction accuracy model variance net-trim algorithm prune sparsifies trained network layer-wise remove connection layer solve convex optimization program program seek sparse set weight layer keep layer input output consistent originally train model algorithm associate analysis applicable neural network operate rectified linear unit relu nonlinear activation present parallel cascade version algorithm latter achieve slightly simple model generalization performance former compute distributed manner case net-trim significantly reduce number connection network also provide enough regularization slightly reduce generalization error also provide mathematical analysis consistency initial network retrained model analyze model sample complexity derive general sufficient condition recovery sparse transform matrix single layer take independent gaussian random vector length n input show network response describe use maximum number non-zero weight per node weight learn mathcal log n sample