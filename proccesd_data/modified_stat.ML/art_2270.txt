 sample-efficient nonstationary policy evaluation contextual bandit present prove property new offline policy evaluator exploration learn set superior previous evaluator particular simultaneously correctly incorporate technique importance weighting doubly robust evaluation nonstationary policy evaluation approach addition approach allow generate long history careful control bias-variance tradeoff decrease variance incorporate information randomness target policy empirical evidence synthetic realworld exploration learn problem show new evaluator successfully unify previous approach use information order magnitude efficiently