 understand trainable sparse cod via matrix factorization sparse coding core building block many data analysis machine learning pipeline typically solve rely generic optimization technique optimal class first-order method non-smooth convex function iterative soft thresholding algorithm accelerated version ista fista however method n't exploit particular structure problem hand input data distribution acceleration use neural network propose cite gregor coin lista show empirically one could achieve high quality estimate iteration modify parameter proximal splitting appropriately paper study reason acceleration mathematical analysis reveals relate specific matrix factorization gram kernel dictionary attempt nearly diagonalise kernel basis produce small perturbation ell ball factorization succeed prove result splitting algorithm enjoy improved convergence bound respect non-adaptive version moreover analysis also show condition acceleration occur mostly beginning iterative process consistent numerical experiment validate analysis show dictionary factorization exist adaptive acceleration fails