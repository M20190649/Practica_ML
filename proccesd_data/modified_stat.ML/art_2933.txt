 scalable kernel method via doubly stochastic gradient general perception kernel method scalable neural net method choice nonlinear learning problem simply try hard enough kernel method propose approach scale kernel method use novel concept call doubly stochastic functional gradient approach relies fact many kernel method express convex optimization problem solve problem make two unbiased stochastic approximation functional gradient one use random training point another use random function associate kernel descend use noisy functional gradient show function produce procedure iteration converges optimal function reproducing kernel hilbert space rate achieve generalization performance sqrt doubly stochasticity also allow u avoid keep support vector implement algorithm small memory footprint linear number iteration independent data dimension approach readily scale kernel method regime dominate neural net show method achieve competitive performance neural net datasets million handwritten digit mnist million energy material molecularspace million photo imagenet