 adiabatic persistent contrastive divergence learn paper study problem parameter learning probabilistic graphical model latent variable standard approach expectation maximization algorithm alternate expectation e maximization step however e step computationally intractable high dimensional data substitution one step faster surrogate combat intractability often cause failure convergence propose new learning algorithm computationally efficient provably ensure convergence correct optimum key idea run cycle markov chain mc e step idea run incomplete mc well study step literature call contrastive divergence cd learning know cd-based scheme find approximated gradient log-likelihood via mean-field approach e step propose algorithm exact one via mc algorithm step due multi-time-scale stochastic approximation theory despite theoretical guarantee convergence propose scheme might suffer slow mixing mc e step tackle also propose hybrid approach apply mean-field mc approximation e step hybrid approach outperform bare mean-field cd scheme experiment real-world datasets