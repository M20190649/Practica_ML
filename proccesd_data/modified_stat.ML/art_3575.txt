 functional frank-wolfe boosting general loss function boost generic learning method classification regression yet number base hypothesis becomes large boost lead deterioration test performance overfitting important ubiquitous phenomenon especially regression setting avoid overfitting consider use l regularization propose novel frank-wolfe type boost algorithm fwboost apply general loss function use exponential loss fwboost algorithm rewrite variant adaboost binary classification fwboost algorithm exactly form exist boost method term make call base learn algorithm different weight update direct connection boost frank-wolfe yield new algorithm practical exist boost method new guarantee rate convergence experimental result show test performance fwboost degrade large round boosting consistent theoretical analysis