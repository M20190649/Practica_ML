 stochastic compositional gradient descent algorithm minimize composition expected-value function classical stochastic gradient method well suit minimize expected-value objective function however apply minimization nonlinear function involve expect value composition two expected-value function i.e. problem form min x mathbf e v f v big mathbf e w g w x big order solve stochastic composition problem propose class stochastic compositional gradient descent scgd algorithm view stochastic version quasi-gradient method scgd update solution base noisy sample gradient f v g w use auxiliary variable track unknown quantity mathbf e w g w x prove scgd converge almost surely optimal solution convex optimization problem long solution exist convergence involve interplay two iteration different time scale nonsmooth convex problem scgd achieve convergence rate k general case k strongly convex case take k sample smooth convex problem scgd accelerate converge rate k general case k strongly convex case nonconvex problem prove limit point generate scgd stationary point also provide convergence rate analysis indeed stochastic setting one want optimize composition expected-value function common practice propose scgd method find wide application learning estimation dynamic programming etc