 mini-batch semi-stochastic gradient descent proximal setting propose ms gd method incorporate mini-batching scheme improve theoretical complexity practical performance semi-stochastic gradient descent gd consider problem minimize strongly convex function represent sum average large number smooth convex function simple nonsmooth convex regularizer method first perform deterministic step computation gradient objective function start point follow large number stochastic step process repeat time last iterate become new start point novelty method introduction mini-batching computation stochastic step step instead choose single function sample b function compute gradient compute direction base analyze complexity method show benefit two speedup effect first prove long b certain threshold reach predefined accuracy less overall work without mini-batching second mini-batching scheme admit simple parallel implementation hence suitable acceleration parallelization