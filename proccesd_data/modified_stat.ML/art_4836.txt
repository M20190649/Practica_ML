 combine gradient boost machine collective inference predict continuous value gradient boosting regression tree competitive procedure learn predictive model continuous data fit data additive non-parametric model classic version gradient boost assumes data independent identically distribute however relational data interdependent link instance common dependency data exploit improve predictive performance collective inference one approach exploit relational correlation pattern significantly reduce classification error however much work collective learning inference focus discrete prediction task rather continuous target value get attention term collective inference work investigate combine two paradigm together improve regression relational domain specifically propose boosting algorithm learn collective inference model predict continuous target variable algorithm learn basic relational model collectively infer target value iteratively learn relational model predict residual evaluate propose algorithm real network dataset show outperform alternative boost method however investigation also reveal relational feature interact together produce good prediction