 expect policy gradient propose expected policy gradient epg unify stochastic policy gradient spg deterministic policy gradient dpg reinforcement learning inspire expect sarsa epg integrates across action estimate gradient instead rely action sampled trajectory establish new general policy gradient theorem stochastic deterministic policy gradient theorem special case also prove epg reduce variance gradient estimate without require deterministic policy gaussian case computational overhead finally show optimal certain sense explore gaussian policy covariance proportional exponential scaled hessian critic respect action present empirical result confirm new form exploration substantially outperform dpg ornstein-uhlenbeck heuristic four challenge mujoco domain