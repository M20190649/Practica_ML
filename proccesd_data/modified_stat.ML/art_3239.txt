 privacy free posterior sampling stochastic gradient monte carlo consider problem bayesian learn sensitive datasets present two simple somewhat surprising result connect bayesian learning differential privacy cryptographic approach protect individual-level privacy permit database-level utility specifically show standard assumption get one single sample posterior distribution differentially private free see estimator statistically consistent near optimal computationally tractable whenever bayesian model interest consistent optimal tractable similarly separately show recent line work use stochastic gradient hybrid monte carlo hmc sample also preserve differentially privacy minor modification algorithmic procedure observation lead anytime algorithm bayesian learn privacy constraint demonstrate perform much good state-of-the-art differential private method synthetic real datasets