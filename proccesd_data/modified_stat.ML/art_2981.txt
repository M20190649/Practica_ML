 general framework fast stagewise algorithms forward stagewise regression follow simple strategy construct sequence sparse regression estimate start coefficient equal zero iteratively update coefficient small amount epsilon variable achieve maximal absolute inner product current residual procedure interesting connection lasso condition know sequence forward stagewise estimate exactly coincide lasso path step size epsilon go zero furthermore essentially equivalence hold outside least square regression minimization differentiable convex loss function subject ell norm constraint stagewise algorithm update coefficient correspond maximal absolute component gradient even match ell -constrained analogue stagewise estimate provide useful approximation computationally appeal success sparse modeling motivate question simple effective strategy like forward stagewise apply broadly regularization setting beyond ell norm sparsity current paper attempt present general framework stagewise estimation yield fast algorithms problem group-structured learning matrix completion image denoising