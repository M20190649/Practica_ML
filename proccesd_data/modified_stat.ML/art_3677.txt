 add gradient noise improve learn deep network deep feedforward recurrent network achieve impressive result many perception language processing application success partially attribute architectural innovation convolutional long short-term memory network main motivation architectural innovation capture good domain knowledge importantly easy optimize basic architecture recently complex architecture neural turing machine memory network propose task include question answering general computation create new set optimization challenge paper discuss low-overhead easy-to-implement technique add gradient noise find surprisingly effective train deep architecture technique help avoid overfitting also result low training loss method alone allow fully-connected -layer deep network train standard gradient descent even start poor initialization see consistent improvement many complex model include relative reduction error rate carefully-tuned baseline challenging question-answering task doubling number accurate binary multiplication model learn across random restarts encourage application technique additional complex modern architecture