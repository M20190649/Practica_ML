 yellowfin art momentum tune hyperparameter tuning one time-consuming workload deep learning state-of-the-art optimizers adagrad rmsprop adam reduce labor adaptively tune individual learning rate variable recently researcher show renew interest simple method like momentum sgd may yield good test metric motivate trend ask simple adaptive method base sgd perform well good revisit momentum sgd algorithm show hand-tuning single learning rate momentum make competitive adam analyze robustness learn rate misspecification objective curvature variation base insight design yellowfin automatic tuner momentum learn rate sgd yellowfin optionally use negative-feedback loop compensate momentum dynamic asynchronous setting fly empirically show yellowfin converge few iteration adam resnets lstms image recognition language modeling constituency parsing speedup x synchronous x asynchronous setting