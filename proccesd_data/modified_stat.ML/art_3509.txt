 importance weight autoencoders variational autoencoder vae kingma welling recently propose generative model pair top-down generative network bottom-up recognition network approximate posterior inference typically make strong assumption posterior inference instance posterior distribution approximately factorial parameter approximate nonlinear regression observation show empirically vae objective lead overly simplify representation fail use network 's entire modeling capacity present importance weight autoencoder iwae generative model architecture vae use strictly tight log-likelihood lower bound derive importance weighting iwae recognition network use multiple sample approximate posterior give increase flexibility model complex posterior fit vae model assumption show empirically iwaes learn rich latent space representation vaes lead improve test log-likelihood density estimation benchmark