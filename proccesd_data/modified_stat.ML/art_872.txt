 learn generative model sinkhorn divergence ability compare two degenerate probability distribution i.e two probability distribution support two distinct low-dimensional manifold live much higher-dimensional space crucial problem arise estimation generative model high-dimensional observation arise computer vision natural language know optimal transport metric represent cure problem since specifically design alternative information divergence handle problematic scenario unfortunately train generative machine use ot raise formidable computational statistical challenge computational burden evaluate ot loss ii instability lack smoothness loss iii difficulty estimate robustly loss gradient high dimension paper present first tractable computational method train large scale generative model use optimal transport loss tackle three issue rely two key idea entropic smoothing turn original ot loss one compute use sinkhorn fix point iteration b algorithmic automatic differentiation iteration two approximation result robust differentiable approximation ot loss streamlined gpu execution entropic smooth generate family loss interpolate wasserstein ot maximum mean discrepancy mmd thus allow find sweet spot leverage geometry ot favorable high-dimensional sample complexity mmd come unbiased gradient estimate result computational architecture complement nicely standard deep network generative model stack extra layer implement loss function