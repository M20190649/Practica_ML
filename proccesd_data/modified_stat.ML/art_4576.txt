 faster asynchronous sgd asynchronous distribute stochastic gradient descent method trouble converging stale gradient gradient update send parameter server client stale parameter use calculate gradient since update server approach propose circumvent problem quantify staleness term number elapsed update work propose novel method quantify staleness term move average gradient statistic show method outperform previous method respect convergence speed scalability many client also discuss extension method use dramatically reduce bandwidth cost distributed training context particular method allow reduction total bandwidth usage factor little impact cost convergence also describe link software library use simulate algorithm deterministically single machine