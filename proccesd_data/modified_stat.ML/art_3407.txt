 accelerate stochastic gradient descent minimize finite sum propose optimization method minimize finite sum smooth convex function method incorporate accelerated gradient descent agd stochastic variance reduction gradient svrg mini-batch setting unlike svrg method directly apply non-strongly strongly convex problem show method achieve low overall complexity recently propose method support non-strongly convex problem moreover method fast rate convergence strongly convex problem experiment show effectiveness method