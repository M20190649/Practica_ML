 precondition stochastic gradient descent stochastic gradient descent sgd still workhorse many practical problem however converge slow difficult tune possible precondition sgd accelerate convergence remarkably many attempt direction either aim solve specialized problem result significantly complicated method sgd paper propose new method estimate preconditioner amplitude perturbation preconditioned stochastic gradient match perturbation parameter optimize way comparable newton method deterministic optimization unlike preconditioners base secant equation fitting do deterministic quasi-newton method assume positive definite hessian approximate inverse new preconditioner work equally well convex non-convex optimization exact noisy gradient stochastic gradient use naturally damp gradient noise stabilize sgd efficient preconditioner estimation method develop reasonable simplification applicable large scale problem experimental result demonstrate equip new preconditioner without tune effort precondition sgd efficiently solve many challenge problem like training deep neural network recurrent neural network require extremely long term memory