 efficiently use second order information large l regularization problem propose novel general algorithm lhac efficiently use second-order information train class large-scale l -regularized problem method execute cheap iteration achieve fast local convergence rate exploit special structure low-rank matrix construct via quasi-newton approximation hessian smooth loss function greedy active-set strategy base large violation dual constraint employ maintain working set iteratively estimate complement optimal active set allow small size subproblems eventually identify optimal active set empirical comparison confirm lhac highly competitive several recently propose state-of-the-art specialized solver sparse logistic regression sparse inverse covariance matrix selection