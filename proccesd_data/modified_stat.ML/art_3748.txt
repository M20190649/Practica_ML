 kronecker-factored approximate fisher matrix convolution layer second-order optimization method natural gradient descent potential speed training neural network correct curvature loss function unfortunately exact natural gradient impractical compute large model approximation either require expensive iterative procedure make crude approximation curvature present kronecker factor convolution kfc tractable approximation fisher matrix convolutional network base structured probabilistic model distribution backpropagated derivative similarly recently propose kronecker-factored approximate curvature k-fac block approximate fisher matrix decompose kronecker product small matrix allow efficient inversion kfc capture important curvature information still yield comparably efficient update stochastic gradient descent sgd show update invariant commonly used reparameterizations centering activation experiment approximate natural gradient descent kfc able train convolutional network several time faster carefully tune sgd furthermore able train network time few iteration sgd suggest potential applicability distributed setting