 normalization propagation parametric technique remove internal covariate shift deep network author batch normalization bn identify address important problem involve train deep network -- internal covariate shift -- current solution certain drawback specifically bn depend batch statistic layerwise input normalization train make estimate mean standard deviation input distribution hide layer inaccurate validation due shift parameter value especially initial training epoch also bn use batch-size training address drawback propose non-adaptive normalization technique remove internal covariate shift call normalization propagation approach depend batch statistic rather use data-independent parametric estimate mean standard-deviation every layer thus computationally faster compare bn exploit observation pre-activation rectified linear unit follow gaussian distribution deep network first second order statistic give dataset normalize forward propagate normalization without need recalculate approximate statistic hidden layer