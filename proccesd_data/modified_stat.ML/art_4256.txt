 attack binarized neural network neural network low-precision weight activation offer compelling efficiency advantage full-precision equivalent two frequently discuss benefit quantization reduce memory consumption faster forward pas implement efficient bitwise operation propose third benefit low-precision neural network improved robustness adversarial attack bad case performance par full-precision model focus low-precision case weight activation quantized pm note stochastically quantize weight one layer sharply reduce impact iterative attack observe non-scaled binary neural network exhibit similar effect original defensive distillation procedure lead gradient masking false notion security address conduct black-box white-box experiment binary model artificially mask gradient