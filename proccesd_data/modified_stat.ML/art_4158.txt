 scalable kernel k-means cluster nystrom approximation relative-error bound kernel k -means cluster correctly identify extract far varied collection cluster structure linear k -means cluster algorithm however kernel k -means cluster computationally expensive non-linear feature map high-dimensional many input point kernel approximation e.g. nystr om method apply previous work approximately solve kernel learn problem condition present work analyze application paradigm kernel k -means clustering show apply linear k -means cluster algorithm frac k epsilon feature construct use so-called rank-restricted nystr om approximation result cluster assignment satisfy epsilon approximation ratio term kernel k -means cost function relative guarantee provide algorithm without use nystr om method part analysis work establish novel epsilon relative-error trace norm guarantee low-rank approximation use rank-restricted nystr om approximation empirical evaluation million instance mnist dataset demonstrate scalability usefulness kernel k -means cluster nystr om approximation work argue spectral cluster use nystr om approximation -- -a popular computationally efficient theoretically unsound approach non-linear clustering -- -should replace efficient theoretically sound combination kernel k -means cluster nystr om approximation superior performance latter approach empirically verify