 variational analysis stochastic gradient algorithm stochastic gradient descent sgd important algorithm machine learning constant learning rate stochastic process initial phase convergence generate sample stationary distribution show sgd constant rate effectively use approximate posterior inference algorithm probabilistic modeling specifically show adjust tune parameter sgd match result stationary distribution posterior analysis rest interpret sgd continuous-time stochastic process minimize kullback-leibler divergence stationary distribution target posterior spirit variational inference detail model sgd multivariate ornstein-uhlenbeck process use property process derive optimal parameter theoretical framework also connect sgd modern scalable inference algorithm analyze recently propose stochastic gradient fisher score perspective demonstrate sgd properly choose constant rate give new way optimize hyperparameters probabilistic model