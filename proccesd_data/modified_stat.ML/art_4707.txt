 efficient distribute learn sparsity propose novel efficient approach distributed sparse learning high-dimensions observation randomly partition across machine computationally round method require master machine solve shifted ell regularize m-estimation problem worker compute gradient respect communication propose approach provably match estimation error bound centralized method within constant round communication ignore logarithmic factor conduct extensive experiment simulate real world datasets demonstrate encouraging performance high-dimensional regression classification task