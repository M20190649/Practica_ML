 gradient descent revisit via adaptive online learn rate gradient descent optimization require choose learning rate deep deep model tune learn rate easily become tedious necessarily lead ideal convergence propose variation gradient descent algorithm learning rate fix instead learn learning rate either another gradient descent first-order method newton 's method second-order way gradient descent machine learn algorithm optimize