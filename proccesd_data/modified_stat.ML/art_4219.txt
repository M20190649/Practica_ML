 collaborative deep learning fixed topology network significant recent interest parallelize deep learn algorithm order handle enormous growth data model size advance focus model parallelization engage multiple compute agent via use central parameter server aspect data parallelization along decentralized computation explore sufficiently context paper present new consensus-based distribute sgd cdsgd momentum variant cdmsgd algorithm collaborative deep learning fixed topology network enable data parallelization well decentralize computation framework extremely useful learn agent access local private data communication constrain environment analyze convergence property propose algorithm strongly convex nonconvex objective function fixed diminish step size use concept lyapunov function construction demonstrate efficacy algorithm comparison baseline centralize sgd recently propose federate average algorithm also enable data parallelism base benchmark datasets mnist cifar- cifar-