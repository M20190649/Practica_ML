 gans train two time-scale update rule converge local nash equilibrium generative adversarial network gans excel create realistic image complex model maximum likelihood infeasible however convergence gan training still prove propose two time-scale update rule ttur train gans stochastic gradient descent arbitrary gan loss function ttur individual learning rate discriminator generator use theory stochastic approximation prove ttur converges mild assumption stationary local nash equilibrium convergence carry popular adam optimization prove follow dynamic heavy ball friction thus prefers flat minimum objective landscape evaluation performance gans image generation introduce fr 'echet inception distance fid capture similarity generated image real one well inception score experiment ttur improves learn dcgans improve wasserstein gans wgan-gp outperform conventional gan train celeba cifar- svhn lsun bedroom one billion word benchmark