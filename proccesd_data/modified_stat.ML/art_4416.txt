 invariance weight distribution rectified mlps interesting approach analyze neural network receive renew attention examine equivalent kernel neural network base fact fully connect feedforward network one hidden layer certain weight distribution activation function infinite number neuron view mapping hilbert space derive equivalent kernel mlps relu leaky relu activation rotationally-invariant weight distribution generalize previous result require gaussian weight distribution additionally central limit theorem use show certain activation function kernel correspond layer weight distribution mean finite absolute third moment asymptotically universal well approximate kernel correspond layer spherical gaussian weight deep network depth increase equivalent kernel approach pathological fixed point use argue train randomly initialize network difficult result also implication weight initialization