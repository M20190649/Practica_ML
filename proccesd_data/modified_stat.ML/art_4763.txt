 extend gauss-newton gauss-newton-admm algorithm low-rank matrix optimization develop generic gauss-newton gn framework solve class nonconvex optimization problem involve low-rank matrix variable oppose standard gauss-newton method framework allow one handle general smooth convex cost function via surrogate main complexity-per-iteration consists inverse two rank-size matrix six small matrix multiplication compute closed form gauss-newton direction backtracking linesearch show mild condition propose algorithm globally locally converges stationary point original nonconvex problem also show empirically gauss-newton algorithm achieves much high accurate solution compare well study alternate direction method adm specify gauss-newton framework handle symmetric case prove convergence adm applicable without lift variable next incorporate gauss-newton scheme alternate direction method multiplier admm design gn-admm algorithm solve low-rank optimization problem prove mild condition proper choice penalty parameter gn-admm globally converge stationary point original problem finally apply algorithm solve several problem practice low-rank approximation matrix completion robust low-rank matrix recovery matrix recovery quantum tomography numerical experiment provide encourage result motivate use nonconvex optimization