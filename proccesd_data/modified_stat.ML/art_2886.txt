 simultaneous model selection optimization parameter-free stochastic learning stochastic gradient descent algorithm train linear kernel predictor gain importance thanks scalability various method propose speed convergence model selection phase often ignore fact theoretical work time assumption make example prior knowledge norm optimal solution practical world validation method remain viable approach paper propose new kernel-based stochastic gradient descent algorithm perform model selection training parameter tune form cross-validation algorithm build recent advancement online learning theory unconstrained setting estimate time right regularization data-dependent way optimal rate convergence prove standard smoothness assumption target function use range space fractional integral operator associate kernel