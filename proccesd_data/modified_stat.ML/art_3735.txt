 distribute bayesian learn stochastic natural-gradient expectation propagation posterior server paper make two contribution bayesian machine learn algorithm firstly propose stochastic natural gradient expectation propagation snep novel alternative expectation propagation ep popular variational inference algorithm snep black box variational algorithm require simplifying assumption distribution interest beyond existence monte carlo sampler estimate moment ep tilt distribution oppose ep guarantee convergence snep show convergent even use monte carlo moment estimate secondly propose novel architecture distributed bayesian learn call posterior server posterior server allow scalable robust bayesian learning case data set store distributed manner across cluster compute node contain disjoint subset data independent monte carlo sampler run compute node direct access local data subset target approximation global posterior distribution give data across whole cluster achieve use distribute asynchronous implementation snep pass message across cluster demonstrate snep posterior server distributed bayesian learning logistic regression neural network keywords distribute learning large scale learning deep learning bayesian learn- ing variational inference expectation propagation stochastic approximation natural gradient markov chain monte carlo parameter server posterior server