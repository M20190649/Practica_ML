 scale mixture perspective multiplicative noise neural network corrupt input hidden layer deep neural network dnns multiplicative noise often draw bernoulli distribution 'dropout provide regularization significantly contribute deep learning 's success however understand multiplicative corruption prevent overfitting difficult due complexity dnn 's functional form paper show gaussian prior place dnn 's weight apply multiplicative noise induce gaussian scale mixture reparameterized circumvent problematic likelihood function analysis proceed use type-ii maximum likelihood procedure derive closed-form expression reveal regularization evolve function network 's weight result show multiplicative noise force weight become either sparse invariant rescaling find analysis implication model compression naturally reveal weight pruning rule starkly contrast commonly use signal-to-noise ratio snr snr prune weight large variance see noisy approach recognize robustness retain empirically demonstrate approach strong advantage snr heuristic competitive retrain soft target produce teacher model