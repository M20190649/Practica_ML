 boost logistic loss consistent manuscript provide optimization guarantee generalization bound statistical consistency result adaboost variant replace exponential loss logistic similar loss specifically twice differentiable convex loss lipschitz tend zero one side heart analysis show lieu explicit regularization constraint structure problem fairly rigidly control source distribution first control type separable case distribution-dependent relaxed weak learn rate induces speedy convergence high probability sample otherwise nonseparable case convex surrogate risk exhibit distribution-dependent level curvature consequently algorithm 's output small norm high probability