 randomized mirror descent algorithm large scale multiple kernel learn consider problem simultaneously learn linearly combine large number kernel learn good predictor base learnt kernel number kernel combine large multiple kernel learn method whose computational cost scale linearly intractable propose randomized version mirror descent algorithm overcome issue objective minimize group p -norm penalize empirical risk key achieve required exponential speed-up computationally efficient construction low-variance estimate gradient propose importance sampling base estimate find ideal distribution sample coordinate probability proportional magnitude corresponding gradient show surprising result case learn coefficient polynomial kernel combinatorial structure base kernel combine allow implementation sample distribution run log time make total computational cost method achieve epsilon -optimal solution log epsilon thereby allow method operate large value experiment simulated real data confirm new algorithm computationally efficient state-of-the-art alternative