 variational gaussian dropout bayesian gaussian multiplicative noise commonly use stochastic regularisation technique training deterministic neural network recent paper reinterpret technique specific algorithm approximate inference bayesian neural network several extension ensue show log-uniform prior use publication generally induce proper posterior thus bayesian inference model ill-posed independent log-uniform prior correlated weight noise approximation issue lead either infinite objective high risk overfitting implies reported sparsity obtained solution explain bayesian related minimum description length argument thus study objective non-bayesian perspective provide previously unknown analytical form allow exact gradient evaluation show later propose additive reparametrisation introduces minimum present original multiplicative parametrisation implication future research direction discuss