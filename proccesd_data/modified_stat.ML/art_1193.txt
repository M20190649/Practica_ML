 easily parallelizable distributable class algorithm structured sparsity optimal acceleration many statistical learn problem pose minimization sum two convex function one typically composition non-smooth linear function example include regression structured sparsity assumption popular algorithm solve problem e.g. admm often involve non-trivial optimization subproblems smooth approximation consider two class primal-dual algorithm incur difficulty unify perspective monotone operator theory unification propose continuum preconditioned forward-backward operator split algorithm amenable parallel distributed computing entire region convergence whole continuum algorithm establish rate convergence known instance continuum analysis close gap theory exploit unification propose continuum accelerated algorithm show whole continuum attain theoretically optimal rate convergence scalability propose algorithm well convergence behavior demonstrate million variable distributed implementation