 deep matching autoencoders increasingly many real world task involve data multiple modality view motivate development many effective algorithm learn common latent space relate multiple domain however exist cross-view learn algorithms assume access pair data training applicability thus limit paired data assumption often violate practice many task small subset data available pair annotation even pair data paper introduce deep matching autoencoders dmae learn common latent space pair unpaired multi-modal data specifically formulate cross-domain representation learning object matching problem simultaneously optimise parameter representation learn auto-encoders pairing unpaired multi-modal data framework elegantly span full regime fully supervise semi-supervised unsupervised paired data multi-modal learning show promising result image captioning new task uniquely enable methodology unsupervised classifier learning