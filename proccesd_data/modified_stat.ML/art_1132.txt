 one-vs-each approximation softmax scalable estimation probability softmax representation probability categorical variable play prominent role modern machine learn numerous application area large scale classification neural language modeling recommendation system however softmax estimation expensive large scale inference high cost associate compute normalizing constant introduce efficient approximation softmax probability take form rigorous low bound exact probability bound express product pairwise probability lead scalable estimation base stochastic optimization allow u perform doubly stochastic estimation subsampling training instance class label show new bound interesting theoretical property demonstrate use classification problem