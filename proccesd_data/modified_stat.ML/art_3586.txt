 sgd variance reduction beyond empirical risk minimization introduce doubly stochastic proximal gradient algorithm optimize finite average smooth convex function whose gradient depend numerically expensive expectation main motivation acceleration optimization regularized cox partial-likelihood core model use survival analysis algorithm use different setting well propose algorithm doubly stochastic sense gradient step do use stochastic gradient descent sgd variance reduction inner expectation approximate monte-carlo markov-chain mcmc algorithm derive condition mcmc number iteration guarantee convergence obtain linear rate convergence strong convexity sublinear rate without assumption illustrate fact algorithm improve state-of-the-art solver regularized cox partial-likelihood several datasets survival analysis