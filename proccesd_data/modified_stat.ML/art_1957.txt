 efficient protocol distributed classification optimization distributed learning goal perform learning task data distribute across multiple node minimal expensive communication prior work daume iii et al. propose general model bound communication require learn classifier allow eps train error linearly separable data adversarially distribute across node work develop key improvement extension basic model first result two-party multiplicative-weight-update base protocol use log eps word communication classify distribute data arbitrary dimension eps -optimally readily extend classification k node kd log eps word communication propose protocol simple implement considerably efficient baseline compare demonstrate empirical result addition illustrate general algorithm design paradigm efficient learn distribute data show solve fixed-dimensional high dimensional linear program efficiently distributed setting constraint may distribute across node since many learn problem view convex optimization problem constraint generate individual point model many typical distribute learning scenario technique make use novel connection multipass streaming well adapt multiplicative-weight-update framework generally distribute setting consequence method extend wide range problem solvable use technique