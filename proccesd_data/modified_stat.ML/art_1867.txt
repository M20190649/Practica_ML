 smooth proximal gradient method general structured sparse learning study problem learn high dimensional regression model regularize structured-sparsity-inducing penalty encode prior structural information either input output side consider two widely adopt type penalty motivate example overlap group lasso penalty base l l mixed-norm penalty graph-guided fusion penalty type penalty due non-separability develop efficient optimization method remain challenging problem paper propose general optimization approach call smooth proximal gradient method solve structured sparse regression problem smooth convex loss wide spectrum structured-sparsity-inducing penalty approach base general smoothing technique nesterov achieve convergence rate faster standard first-order method subgradient method much scalable widely use interior-point method numerical result report demonstrate efficiency scalability propose method