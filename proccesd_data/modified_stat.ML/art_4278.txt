 automatic differentiation tensor algebra kjolstad et al propose tensor algebra compiler take expression define tensor element-wise f ij b c exp leave sum k leave ik b jk c ii k right right generate corresponding compute kernel code machine learning especially deep learning often necessary compute gradient loss function l b c l f b c respect parameters b c tensor compiler apply field necessary derive expression derivative element-wise defined tensor i.e expression da ik partial l partial ik mapping function index argument index special attention require function f ij x x derivative loss dx partial l partial x sum j df ij x sum necessary index j appear index f another example f x x ii x matrix dx ij delta ij df x ii kronecker delta necessary derivative zero off-diagonal element another indexing scheme use f ij x exp x j correct derivative dx k sum df k-i exp x k range sum must choose appropriately publication present algorithm handle case index argument arbitrary linear combination index function thus example handle sum range kronecker delta automatically insert derivative necessary additionally index transform require last example algorithm output symbolic expression subsequently feed tensor algebra compiler source code provide