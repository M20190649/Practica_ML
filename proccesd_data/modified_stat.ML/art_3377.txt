 saddle-free hessian-free optimization nonconvex optimization problem one train deep neural network suffer phenomenon call saddle point proliferation mean vast number high error saddle point present loss function second order method tremendously successful widely adopt convex optimization community usefulness deep learning remain limited due two problem computational complexity method drive towards high error saddle point introduce novel algorithm specially design solve two issue provide crucial first step take widely know advantage newton 's method nonconvex optimization community especially high dimensional setting