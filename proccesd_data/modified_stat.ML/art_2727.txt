 structure-aware dynamic scheduler parallel machine learn train large machine learn ml model many variable parameter take long time one employ sequential procedure even stochastic update natural solution turn distribute compute cluster however naive unstructured parallelization ml algorithm usually lead proportional speedup even result divergence dependency model element attenuate computational gain parallelization compromise correctness inference recent effort toward issue benefit exploit static priori block structure reside ml algorithm paper take path explore dynamic block structure workload therein present ml program execution offer new opportunity improve convergence correctness load balancing distributed ml propose showcase general-purpose scheduler strad coordinate distribute update ml algorithm harness aforementioned opportunity systematic way provide theoretical guarantee scheduler demonstrate efficacy versus static block structure lasso matrix factorization