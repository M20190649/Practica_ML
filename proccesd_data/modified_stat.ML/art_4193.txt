 l regularization versus batch weight normalization batch normalization commonly use trick improve training deep neural network neural network use l regularization also call weight decay ostensibly prevent overfitting however show l regularization regularize effect combine normalization instead regularization influence scale weight thereby effective learning rate investigate dependence theory experimentally show popular optimization method adam partially eliminate influence normalization learning rate lead discussion way mitigate issue