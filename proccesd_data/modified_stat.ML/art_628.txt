 safe feature prune sparse high-order interaction model take account high-order interaction among covariates valuable many practical regression problem however computationally challenge task number high-order interaction feature consider would extremely large unless number covariates sufficiently small paper propose novel efficient algorithm lasso-based sparse learning high-order interaction model basic strategy reduce number feature employ idea recently propose safe feature screen sfs rule sfs rule property feature satisfy rule feature guarantee non-active lasso solution mean safely screened-out prior lasso training process large number feature screened-out train lasso computational cost memory requirment dramatically reduce however apply sfs rule extremely large number high-order interaction feature would computationally infeasible key idea solve computational issue exploit underlying tree structure among high-order interaction feature specifically introduce pruning condition call safe feature prune sfp rule property rule satisfy certain node tree high-order interaction feature correspond descendant node guarantee non-active optimal solution algorithm extremely efficient make possible work e.g. rd order interaction original covariates number possible high-order interaction feature great