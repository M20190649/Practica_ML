 fairness beyond disparate treatment disparate impact learning classification without disparate mistreatment automate data-driven decision make system increasingly use assist even replace human many setting system function learn historical decision often take human order maximize utility system classifier training involve minimize error misclassifications give historical data however quite possible optimally trained classifier make decision people belong different social group different misclassification rate e.g. misclassification rate female high male thereby place group unfair disadvantage account avoid unfairness paper introduce new notion unfairness disparate mistreatment define term misclassification rate propose intuitive measure disparate mistreatment decision boundary-based classifier easily incorporate formulation convex-concave constraint experiment synthetic well real world datasets show methodology effective avoid disparate mistreatment often small cost term accuracy