 convergence rate biased stochastic optimization learn sparse ising model study convergence rate stochastic optimization exact np-hard objective bias estimate gradient available motivate problem context learn structure parameter ising model first provide convergence-rate analysis deterministic error forward-backward splitting fbs extend analysis bias stochastic error first characterize family sampler provide high probability bound allow understand fbs also proximal gradient pg method derive interesting conclusion fbs require logarithmically increase number random sample order converge although low rate required number random sample deterministic biased stochastic setting fbs basic pg accelerate pg guarantee converge biased stochastic setting