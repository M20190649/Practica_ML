 train simplification model simplification deep learning minimal effort back propagation method propose simple yet effective technique simplify training result model neural network back propagation small subset full gradient compute update model parameter gradient vector sparsified way top-k element term magnitude keep result k row column depend layout weight matrix modify lead linear reduction computational cost base sparsified gradient simplify model eliminate row column seldom update reduce computational cost training decoding potentially accelerate decoding real-world application surprisingly experimental result demonstrate time need update few weight back propagation pas interestingly accuracy resulting model actually improve rather degrade detailed analysis give model simplification result show could adaptively simplify model could often reduce around x without loss accuracy even improved accuracy code include extension available http github.com lancopku mesimp