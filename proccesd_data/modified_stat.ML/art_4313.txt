 quantize memory-augmented neural network memory-augmented neural network manns refer class neural network model equip external memory neural turing machine memory network neural network outperform conventional recurrent neural network rnns term learn long-term dependency allow solve intrigue ai task would otherwise hard address paper concern problem quantize mann quantization know effective deploy deep model embedded system limited resource furthermore quantization substantially reduce energy consumption inference procedure benefit justify recent development quantized multi layer perceptrons convolutional network rnns however prior work report successful quantization mann in-depth analysis present reveal various challenge appear quantization network without address properly quantized mann would normally suffer excessive quantization error lead degrade performance paper identify memory address specifically content-based addressing main reason performance degradation propose robust quantization method mann address challenge experiment achieve computation-energy gain x -bit fixed-point binary quantization compare floating-point implementation measure babi dataset result model name quantized mann q-mann improve error rate -bit fixed-point binary quantization respectively compare mann quantize use conventional technique