 positive semi-definite embedding dimensionality reduction out-of-sample extension machine learning statistic often desirable reduce dimensionality high dimensional data propose obtain low dimensional embedding coordinate eigenvectors positive semi-definite kernel matrix kernel matrix solution semi-definite program promote low rank solution define help diffusion kernel besides also discuss infinite dimensional analogue semi-definite program practical perspective main feature approach existence non-linear out-of-sample extension formula embed coordinate call projected nystr om approximation extension formula yield extension kernel matrix data-dependent mercer kernel function although semi-definite program may solve directly propose another strategy base rank constrain formulation solve thanks projected power method algorithm follow singular value decomposition strategy allow reduced computational time