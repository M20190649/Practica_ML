 principal polynomial analysis paper present new framework manifold learning base sequence principal polynomial capture possibly nonlinear nature data propose principal polynomial analysis ppa generalizes pca model direction maximal variance mean curve instead straight line contrarily previous approach ppa reduces perform simple univariate regression make computationally feasible robust moreover ppa show number interesting analytical property first ppa volume-preserving map turn guarantee existence inverse second inverse obtain closed form invertibility important advantage learn method permit understand identified feature input domain data physical meaning moreover allow evaluate performance dimensionality reduction sensible input-domain unit volume preservation also allow easy computation information theoretic quantity reduction multi-information transform third analytical nature ppa lead clear geometrical interpretation manifold allow computation frenet-serret frame local feature generalized curvature point space fourth analytical jacobian allow computation metric induce data thus generalize mahalanobis distance property demonstrate theoretically illustrate experimentally performance ppa evaluate dimensionality redundancy reduction synthetic real datasets uci repository