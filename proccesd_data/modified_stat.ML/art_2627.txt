 bennett-type generalization bound large-deviation case faster rate convergence paper present bennett-type generalization bound learning process i.i.d sample show generalization bound fast rate convergence traditional result particular first develop two type bennett-type deviation inequality i.i.d learn process one provide generalization bound base uniform entropy number lead bound base rademacher complexity adopt new method obtain alternative expression bennett-type generalization bound imply bound fast rate n convergence traditional result n additionally find rate bound become faster large-deviation case refer situation empirical risk far away least close expect risk finally analyze asymptotical convergence learning process compare analysis exist result