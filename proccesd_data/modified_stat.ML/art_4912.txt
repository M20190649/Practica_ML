 hierarchically compositional kernel scalable nonparametric learning propose novel class kernel alleviate high computational cost large-scale nonparametric learn kernel method propose kernel define base hierarchical partitioning underlie data domain nystr om method globally low-rank approximation marry locally lossless approximation hierarchical fashion kernel maintain strict positive-definiteness corresponding kernel matrix admit recursively off-diagonal low-rank structure allow fast linear algebra computation suppress factor data dimension memory arithmetic complexity train regression classifier reduce n n nr nr respectively n number training example r rank level hierarchy although randomized approximate kernel entail similar complexity empirical result show propose kernel achieve matching performance small r demonstrate comprehensive experiment show effective use propose kernel data size order million