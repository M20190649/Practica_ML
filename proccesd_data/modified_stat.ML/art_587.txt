 dense distribution sparse sample improve gibbs sample parameter estimator lda introduce novel approach estimate latent dirichlet allocation lda parameter collapse gibbs sample cgs leverage full conditional distribution latent variable assignment efficiently average multiple sample little computational cost draw single additional collapse gibbs sample approach understand adapt soft cluster methodology collapsed variational bayes cvb cgs parameter estimation order get best technique estimator straightforwardly apply output exist implementation cgs include modern accelerate variant perform extensive empirical comparison estimator standard collapse inference algorithm real-world data unsupervised lda prior-lda supervised variant lda multi-label classification result show consistent advantage approach traditional cgs experimental condition cvb inference majority condition broadly result highlight importance average multiple sample lda parameter estimation use efficient computational technique