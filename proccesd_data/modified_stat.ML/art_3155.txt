 deep learning elastic average sgd study problem stochastic optimization deep learning parallel compute environment communication constraint new algorithm propose setting communication coordination work among concurrent process local worker base elastic force link parameter compute center variable store parameter server master algorithm enable local worker perform exploration i.e algorithm allow local variable fluctuate center variable reduce amount communication local worker master empirically demonstrate deep learning setting due existence many local optimum allow exploration lead improved performance propose synchronous asynchronous variant new algorithm provide stability analysis asynchronous variant round-robin scheme compare common parallelized method admm show stability easgd guarantee simple stability condition satisfy case admm additionally propose momentum-based version algorithm apply synchronous asynchronous setting asynchronous variant algorithm apply train convolutional neural network image classification cifar imagenet datasets experiment demonstrate new algorithm accelerate training deep architecture compare downpour common baseline approach furthermore communication efficient