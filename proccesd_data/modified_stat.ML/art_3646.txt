 interplay network structure gradient convergence deep learn regularization output consistency behavior dropout layer-wise pretraining learn deep network fairly well study however understanding asymptotic convergence backpropagation deep architecture relate structural property network design choice like denoising dropout rate less clear time interesting question one may ask whether network architecture input data statistic may guide choice learn parameter vice versa work explore association structural distributional learnability aspect vis- a-vis interaction parameter convergence rate present framework address question base convergence backpropagation general nonconvex objective use first-order information analysis suggest interesting relationship feature denoising dropout build upon result obtain setup provide systematic guidance regard choice learn parameter network size achieve certain level convergence optimization sense often mediate statistical attribute input result support set experimental evaluation well independent empirical observation report group