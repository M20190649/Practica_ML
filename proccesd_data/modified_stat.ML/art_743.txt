 precondition stochastic gradient langevin dynamic deep neural network effective training deep neural network suffers two main issue first parameter space model exhibit pathological curvature recent method address problem use adaptive preconditioning stochastic gradient descent sgd method improve convergence adapt local geometry parameter space second issue overfitting typically address early stopping however recent work demonstrate bayesian model average mitigates problem posterior sample use stochastic gradient langevin dynamic sgld however rapidly changing curvature render default sgld method inefficient propose combine adaptive preconditioners sgld support idea give theoretical property asymptotic convergence predictive risk also provide empirical result logistic regression feedforward neural net convolutional neural net demonstrate preconditioned sgld method give state-of-the-art performance model