 compression-based regularization application multi-task learning paper investigate information theoretic ground learning problem base principle regularity give dataset exploit extract compact feature data i.e. use few bit need fully describe data order build meaningful representation relevant content multiple label begin introduce noisy lossy source cod paradigm log-loss fidelity criterion provide fundamental tradeoff emph cross-entropy loss average risk information rate feature model complexity approach allow information theoretic formulation emph multi-task learn mtl problem supervised learning framework prediction model several related task learn jointly common representation achieve good generalization performance present iterative algorithm compute optimal tradeoff global convergence proven provide condition hold important property algorithm provide natural safeguard overfitting minimize average risk take account penalization induce model complexity remarkably empirical result illustrate exist optimal information rate minimize emph excess risk depend nature amount available training data application hierarchical text categorization also investigate extend previous work