 extreme stochastic variational inference distribute asynchronous stochastic variational inference svi state-of-the-art algorithm scale variational inference large-datasets inherently serial moreover require parameter fit memory single processor problematic number parameter billion paper propose extreme stochastic variational inference esvi asynchronous lock-free algorithm perform variational inference mixture model massive real world datasets esvi overcome limitation svi require processor access subset data subset parameter thus provide data model parallelism simultaneously demonstrate effectiveness esvi run latent dirichlet allocation lda umbc- b dataset vocabulary million token size billion experiment find esvi outperform vi svi wallclock-time also achieve good quality solution addition propose strategy speed computation save memory fit large number topic