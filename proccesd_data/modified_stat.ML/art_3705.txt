 rsg beating subgradient method without smoothness strong convexity paper study efficiency bf r estarted bf ub bf g radient rsg method periodically restart standard subgradient method sg show apply broad class convex optimization problem rsg method find epsilon -optimal solution low complexity sg method particular first show rsg reduce dependence sg 's iteration complexity distance initial solution optimal set epsilon -level set optimal set multiply logarithmic factor moreover show advantage rsg sg solve three different family convex optimization problem problem whose epigraph polyhedron rsg show converge linearly b problem local quadratic growth property epsilon -sublevel set rsg frac epsilon log frac epsilon iteration complexity c problem admit local kurdyka- l ojasiewicz property power constant beta rsg frac epsilon beta log frac epsilon iteration complexity novelty analysis lie exploit low bound first-order optimality residual epsilon -level set novelty allow u explore local property function e.g. local quadratic growth property local kurdyka- l ojasiewicz property generally local error bound condition develop improved convergence rsg also develop practical variant rsg enjoy fast convergence sg method run without know involved parameter local error bound condition demonstrate effectiveness propose algorithm several machine learn task include regression classification matrix completion