 sensitivity lasso number predictor variable lasso computationally efficient regression regularization procedure produce sparse estimator number predictor p large oracle inequality provide probability loss bound lasso estimator deterministic choice regularization parameter bound tend zero p appropriately control thus commonly cite theoretical justification lasso ability handle high-dimensional setting unfortunately practice regularization parameter select deterministic quantity instead choose use random data-dependent procedure address shortcoming previous theoretical work study loss lasso estimator tune optimally prediction assume orthonormal predictor sparse true model prove probability best possible predictive performance lasso deteriorate p increase positive arbitrarily close one give sufficiently high signal noise ratio sufficiently large p. demonstrate empirically amount deterioration performance far bad oracle inequalities suggest provide real data example deterioration observe