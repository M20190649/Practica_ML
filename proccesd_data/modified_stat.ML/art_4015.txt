 joint multimodal learn deep generative model investigate deep generative model exchange multiple modality bi-directionally e.g. generate image correspond text vice versa recently study handle multiple modality deep generative model variational autoencoders vaes however model typically assume modality force condition relation i.e. generate modality one direction achieve objective extract joint representation capture high-level concept among modality exchange bi-directionally describe herein propose joint multimodal variational autoencoder jmvae modality independently condition joint representation word model joint distribution modality furthermore able generate miss modality remain modality properly develop additional method jmvae-kl train reduce divergence jmvae 's encoder prepared network respective modality experiment show propose method obtain appropriate joint representation multiple modality generate reconstruct properly conventional vaes demonstrate jmvae generate multiple modality bi-directionally