 entropy-sgd biasing gradient descent wide valley paper propose new optimization algorithm call entropy-sgd train deep neural network motivate local geometry energy landscape local extremum low generalization error large proportion almost-zero eigenvalue hessian positive negative eigenvalue leverage upon observation construct local-entropy-based objective function favor well-generalizable solution lie large flat region energy landscape avoid poorly-generalizable solution locate sharp valley conceptually algorithm resembles two nested loop sgd use langevin dynamic inner loop compute gradient local entropy update weight show new objective smoother energy landscape show improved generalization sgd use uniform stability certain assumption experiment convolutional recurrent network demonstrate entropy-sgd compare favorably state-of-the-art technique term generalization error training time