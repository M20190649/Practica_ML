 train deep gaussian process use stochastic expectation propagation probabilistic backpropagation deep gaussian process dgps multi-layer hierarchical generalisation gaussian process gps formally equivalent neural network multiple infinitely wide hidden layer dgps probabilistic non-parametric arguably flexible great capacity generalise provide good calibrated uncertainty estimate alternative deep model focus paper scalable approximate bayesian learning network paper develop novel efficient extension probabilistic backpropagation state-of-the-art method train bayesian neural network use train dgps new method leverage recently propose method scale expectation propagation call stochastic expectation propagation method able automatically discover useful input warping expansion compression therefore flexible form bayesian kernel design demonstrate success new method supervised learning several real-world datasets show typically outperform gp regression never much bad