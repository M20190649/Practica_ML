Fast Learning Rate of lp-MKL and its Minimax Optimality
In this paper, we give a new sharp generalization bound of lp-MKL which is a
generalized framework of multiple kernel learning (MKL) and imposes
lp-mixed-norm regularization instead of l1-mixed-norm regularization. We
utilize localization techniques to obtain the sharp learning rate. The bound is
characterized by the decay rate of the eigenvalues of the associated kernels. A
larger decay rate gives a faster convergence rate. Furthermore, we give the
minimax learning rate on the ball characterized by lp-mixed-norm in the product
space. Then we show that our derived learning rate of lp-MKL achieves the
minimax optimal rate on the lp-mixed-norm ball.