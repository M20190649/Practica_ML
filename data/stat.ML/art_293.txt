Sparse Principal Component Analysis for High Dimensional Vector
  Autoregressive Models
We study sparse principal component analysis for high dimensional vector
autoregressive time series under a doubly asymptotic framework, which allows
the dimension $d$ to scale with the series length $T$. We treat the transition
matrix of time series as a nuisance parameter and directly apply sparse
principal component analysis on multivariate time series as if the data are
independent. We provide explicit non-asymptotic rates of convergence for
leading eigenvector estimation and extend this result to principal subspace
estimation. Our analysis illustrates that the spectral norm of the transition
matrix plays an essential role in determining the final rates. We also
characterize sufficient conditions under which sparse principal component
analysis attains the optimal parametric rate. Our theoretical results are
backed up by thorough numerical studies.