Statistics of Robust Optimization: A Generalized Empirical Likelihood
  Approach
We study statistical inference and distributionally robust solution methods
for stochastic optimization problems, focusing on confidence intervals for
optimal values and solutions that achieve exact coverage asymptotically. We
develop a generalized empirical likelihood framework---based on distributional
uncertainty sets constructed from nonparametric $f$-divergence balls---for
Hadamard differentiable functionals, and in particular, stochastic optimization
problems. As consequences of this theory, we provide a principled method for
choosing the size of distributional uncertainty regions to provide one- and
two-sided confidence intervals that achieve exact coverage. We also give an
asymptotic expansion for our distributionally robust formulation, showing how
robustification regularizes problems by their variance. Finally, we show that
optimizers of the distributionally robust formulations we study enjoy
(essentially) the same consistency properties as those in classical sample
average approximations. Our general approach applies to quickly mixing
stationary sequences, including geometrically ergodic Harris recurrent Markov
chains.