Selective Factor Extraction in High Dimensions
This paper studies simultaneous feature selection and extraction in
supervised and unsupervised learning. We propose and investigate selective
reduced rank regression for constructing optimal explanatory factors from a
parsimonious subset of input features. The proposed estimators enjoy sharp
oracle inequalities, and with a predictive information criterion for model
selection, they adapt to unknown sparsity by controlling both rank and row
support of the coefficient matrix. A class of algorithms is developed that can
accommodate various convex and nonconvex sparsity-inducing penalties, and can
be used for rank-constrained variable screening in high-dimensional
multivariate data. The paper also showcases applications in macroeconomics and
computer vision to demonstrate how low-dimensional data structures can be
effectively captured by joint variable selection and projection.