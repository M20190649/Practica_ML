Strictly Proper Kernel Scoring Rules and Divergences with an Application
  to Kernel Two-Sample Hypothesis Testing
We study strictly proper scoring rules in the Reproducing Kernel Hilbert
Space. We propose a general Kernel Scoring rule and associated Kernel
Divergence. We consider conditions under which the Kernel Score is strictly
proper. We then demonstrate that the Kernel Score includes the Maximum Mean
Discrepancy as a special case. We also consider the connections between the
Kernel Score and the minimum risk of a proper loss function. We show that the
Kernel Score incorporates more information pertaining to the projected embedded
distributions compared to the Maximum Mean Discrepancy. Finally, we show how to
integrate the information provided from different Kernel Divergences, such as
the proposed Bhattacharyya Kernel Divergence, using a one-class classifier for
improved two-sample hypothesis testing results.