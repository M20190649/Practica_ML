Learning rates for the risk of kernel based quantile regression
  estimators in additive models
Additive models play an important role in semiparametric statistics. This
paper gives learning rates for regularized kernel based methods for additive
models. These learning rates compare favourably in particular in high
dimensions to recent results on optimal learning rates for purely nonparametric
regularized kernel based quantile regression using the Gaussian radial basis
function kernel, provided the assumption of an additive model is valid.
Additionally, a concrete example is presented to show that a Gaussian function
depending only on one variable lies in a reproducing kernel Hilbert space
generated by an additive Gaussian kernel, but does not belong to the
reproducing kernel Hilbert space generated by the multivariate Gaussian kernel
of the same variance.