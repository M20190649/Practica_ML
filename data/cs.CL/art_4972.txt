Aligning Very Small Parallel Corpora Using Cross-Lingual Word Embeddings
  and a Monogamy Objective
Count-based word alignment methods, such as the IBM models or fast-align,
struggle on very small parallel corpora. We therefore present an alternative
approach based on cross-lingual word embeddings (CLWEs), which are trained on
purely monolingual data. Our main contribution is an unsupervised objective to
adapt CLWEs to parallel corpora. In experiments on between 25 and 500
sentences, our method outperforms fast-align. We also show that our fine-tuning
objective consistently improves a CLWE-only baseline.