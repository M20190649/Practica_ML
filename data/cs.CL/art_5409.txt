A Framework for Decoding Event-Related Potentials from Text
We propose a novel framework for modeling event-related potentials (ERPs)
collected during reading that couples pre-trained convolutional decoders with a
language model. Using this framework, we compare the abilities of a variety of
existing and novel sentence processing models to reconstruct ERPs. We find that
modern contextual word embeddings underperform surprisal-based models but that,
combined, the two outperform either on its own.