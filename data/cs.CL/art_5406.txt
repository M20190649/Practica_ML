Multi-Task Learning with Contextualized Word Representations for
  Extented Named Entity Recognition
Fine-Grained Named Entity Recognition (FG-NER) is critical for many NLP
applications. While classical named entity recognition (NER) has attracted a
substantial amount of research, FG-NER is still an open research domain. The
current state-of-the-art (SOTA) model for FG-NER relies heavily on manual
efforts for building a dictionary and designing hand-crafted features. The
end-to-end framework which achieved the SOTA result for NER did not get the
competitive result compared to SOTA model for FG-NER. In this paper, we
investigate how effective multi-task learning approaches are in an end-to-end
framework for FG-NER in different aspects. Our experiments show that using
multi-task learning approaches with contextualized word representation can help
an end-to-end neural network model achieve SOTA results without using any
additional manual effort for creating data and designing features.