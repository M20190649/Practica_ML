Imbalanced multi-label classification using multi-task learning with
  extractive summarization
Extractive summarization and imbalanced multi-label classification often
require vast amounts of training data to avoid overfitting. In situations where
training data is expensive to generate, leveraging information between tasks is
an attractive approach to increasing the amount of available information. This
paper employs multi-task training of an extractive summarizer and an RNN-based
classifier to improve summarization and classification accuracy by 50% and 75%,
respectively, relative to RNN baselines. We hypothesize that concatenating
sentence encodings based on document and class context increases
generalizability for highly variable corpuses.