Paraphrases as Foreign Languages in Multilingual Neural Machine
  Translation
Paraphrases, the rewordings of the same semantic meaning, are useful for
improving generalization and translation. However, prior works only explore
paraphrases at the word or phrase level, not at the sentence or corpus level.
Unlike previous works that only explore paraphrases at the word or phrase
level, we use different translations of the whole training data that are
consistent in structure as paraphrases at the corpus level. We train on
parallel paraphrases in multiple languages from various sources. We treat
paraphrases as foreign languages, tag source sentences with paraphrase labels,
and train on parallel paraphrases in the style of multilingual Neural Machine
Translation (NMT). Our multi-paraphrase NMT that trains only on two languages
outperforms the multilingual baselines. Adding paraphrases improves the rare
word translation and increases entropy and diversity in lexical choice. Adding
the source paraphrases boosts performance better than adding the target ones.
Combining both the source and the target paraphrases lifts performance further;
combining paraphrases with multilingual data helps but has mixed performance.
We achieve a BLEU score of 57.2 for French-to-English translation using 24
corpus-level paraphrases of the Bible, which outperforms the multilingual
baselines and is +34.7 above the single-source single-target NMT baseline.