What do Language Representations Really Represent?
A neural language model trained on a text corpus can be used to induce
distributed representations of words, such that similar words end up with
similar representations. If the corpus is multilingual, the same model can be
used to learn distributed representations of languages, such that similar
languages end up with similar representations. We show that this holds even
when the multilingual corpus has been translated into English, by picking up
the faint signal left by the source languages. However, just like it is a
thorny problem to separate semantic from syntactic similarity in word
representations, it is not obvious what type of similarity is captured by
language representations. We investigate correlations and causal relationships
between language representations learned from translations on one hand, and
genetic, geographical, and several levels of structural similarity between
languages on the other. Of these, structural similarity is found to correlate
most strongly with language representation similarity, while genetic
relationships---a convenient benchmark used for evaluation in previous
work---appears to be a confounding factor. Apart from implications about
translation effects, we see this more generally as a case where NLP and
linguistic typology can interact and benefit one another.