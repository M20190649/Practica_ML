The Influence of Down-Sampling Strategies on SVD Word Embedding
  Stability
The stability of word embedding algorithms, i.e., the consistency of the word
representations they reveal when trained repeatedly on the same data set, has
recently raised concerns. We here compare word embedding algorithms on three
corpora of different sizes, and evaluate both their stability and accuracy. We
find strong evidence that down-sampling strategies (used as part of their
training procedures) are particularly influential for the stability of
SVDPPMI-type embeddings. This finding seems to explain diverging reports on
their stability and lead us to a simple modification which provides superior
stability as well as accuracy on par with skip-gram embeddings.