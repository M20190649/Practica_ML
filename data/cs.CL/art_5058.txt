Federated Learning for Mobile Keyboard Prediction
We train a recurrent neural network language model using a distributed,
on-device learning framework called federated learning for the purpose of
next-word prediction in a virtual keyboard for smartphones. Server-based
training using stochastic gradient descent is compared with training on client
devices using the Federated Averaging algorithm. The federated algorithm, which
enables training on a higher-quality dataset for this use case, is shown to
achieve better prediction recall. This work demonstrates the feasibility and
benefit of training language models on client devices without exporting
sensitive user data to servers. The federated learning environment gives users
greater control over the use of their data and simplifies the task of
incorporating privacy by default with distributed training and aggregation
across a population of client devices.