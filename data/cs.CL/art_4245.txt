Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and
  Prince (1988) and the Past Tense Debate
Can advances in NLP help advance cognitive modeling? We examine the role of
artificial neural networks, the current state of the art in many common NLP
tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland
famously introduced a neural architecture that learned to transduce English
verb stems to their past tense forms. Shortly thereafter, Pinker & Prince
(1988) presented a comprehensive rebuttal of many of Rumelhart and McClelland's
claims. Much of the force of their attack centered on the empirical inadequacy
of the Rumelhart and McClelland (1986) model. Today, however, that model is
severely outmoded. We show that the Encoder-Decoder network architectures used
in modern NLP systems obviate most of Pinker and Prince's criticisms without
requiring any simplication of the past tense mapping problem. We suggest that
the empirical performance of modern networks warrants a re-examination of their
utility in linguistic and cognitive modeling.