Consistent Dialogue Generation with Self-supervised Feature Learning
Generating responses that are consistent with the dialogue context is one of
the central challenges in building engaging conversational agents. In this
paper, we propose a neural conversation model that generates consistent
responses by maintaining certain features related to topics and personas
throughout the conversation. Unlike past work that requires external
supervision such as user identities, which are often unavailable or classified
as sensitive information, our approach trains topic and persona feature
extractors in a self-supervised way by utilizing the natural structure of
dialogue data. Moreover, we adopt a binary feature representation and introduce
a feature disentangling loss which, paired with controllable response
generation techniques, allows us to promote or demote certain learned topics
and personas features. The evaluation result demonstrates the model's
capability of capturing meaningful topics and personas features, and the
incorporation of the learned features brings significant improvement in terms
of the quality of generated responses on two datasets, even comparing with
model which explicit persona information.