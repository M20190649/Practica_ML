Addressing word-order Divergence in Multilingual Neural Machine
  Translation for extremely Low Resource Languages
Transfer learning approaches for Neural Machine Translation (NMT) train a NMT
model on the assisting-target language pair (parent model) which is later
fine-tuned for the source-target language pair of interest (child model), with
the target language being the same. In many cases, the assisting language has a
different word order from the source language. We show that divergent word
order adversely limits the benefits from transfer learning when little to no
parallel corpus between the source and target language is available. To bridge
this divergence, We propose to pre-order the assisting language sentence to
match the word order of the source language and train the parent model. Our
experiments on many language pairs show that bridging the word order gap leads
to significant improvement in the translation quality.