Context-Aware Learning for Neural Machine Translation
Interest in larger-context neural machine translation, including
document-level and multi-modal translation, has been growing. Multiple works
have proposed new network architectures or evaluation schemes, but potentially
helpful context is still sometimes ignored by larger-context translation
models. In this paper, we propose a novel learning algorithm that explicitly
encourages a neural translation model to take into account additional context
using a multilevel pair-wise ranking loss. We evaluate the proposed learning
algorithm with a transformer-based larger-context translation system on
document-level translation. By comparing performance using actual and random
contexts, we show that a model trained with the proposed algorithm is more
sensitive to the additional context.