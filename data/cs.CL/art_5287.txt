Semantic Relation Classification via Bidirectional LSTM Networks with
  Entity-aware Attention using Latent Entity Typing
Classifying semantic relations between entity pairs in sentences is an
important task in Natural Language Processing (NLP). Most previous models for
relation classification rely on the high-level lexical and syntactic features
obtained by NLP tools such as WordNet, dependency parser, part-of-speech (POS)
tagger, and named entity recognizers (NER). In addition, state-of-the-art
neural models based on attention mechanisms do not fully utilize information of
entity that may be the most crucial features for relation classification. To
address these issues, we propose a novel end-to-end recurrent neural model
which incorporates an entity-aware attention mechanism with a latent entity
typing (LET) method. Our model not only utilizes entities and their latent
types as features effectively but also is more interpretable by visualizing
attention mechanisms applied to our model and results of LET. Experimental
results on the SemEval-2010 Task 8, one of the most popular relation
classification task, demonstrate that our model outperforms existing
state-of-the-art models without any high-level features.