The Lifted Matrix-Space Model for Semantic Composition
Tree-structured neural network architectures for sentence encoding draw
inspiration from the approach to semantic composition generally seen in formal
linguistics, and have shown empirical improvements over comparable sequence
models by doing so. Moreover, adding multiplicative interaction terms to the
composition functions in these models can yield significant further
improvements. However, existing compositional approaches that adopt such a
powerful composition function scale poorly, with parameter counts exploding as
model dimension or vocabulary size grows. We introduce the Lifted Matrix-Space
model, which uses a global transformation to map vector word embeddings to
matrices, which can then be composed via an operation based on matrix-matrix
multiplication. Its composition function effectively transmits a larger number
of activations across layers with relatively few model parameters. We evaluate
our model on the Stanford NLI corpus, the Multi-Genre NLI corpus, and the
Stanford Sentiment Treebank and find that it consistently outperforms TreeLSTM
(Tai et al., 2015), the previous best known composition function for
tree-structured models.