Optimal encoding in stochastic latent-variable Models
We examine the problem of optimal sparse encoding of sensory stimuli by
latent variables in stochastic models. Analyzing restricted Boltzmann machines
with a communications theory approach, we search for the minimal model size
that correctly conveys the correlations in stimulus patterns in an
information-theoretic sense. We show that the Fisher information Matrix (FIM)
reveals the optimal model size. In larger models the FIM reveals that
irrelevant parameters are associated with individual latent variables,
displaying a surprising amount of order. For well-fit models, we observe the
emergence of statistical criticality as diverging generalized susceptibility of
the model. In this case, an encoding strategy is adopted where highly
informative, but rare stimuli selectively suppress variability in the encoding
units. The information content of the encoded stimuli acts as an unobserved
variable leading to criticality. Together, these results can explain the
stimulus-dependent variability suppression observed in sensory systems, and
suggest a simple, correlation-based measure to reduce the size of artificial
neural networks.