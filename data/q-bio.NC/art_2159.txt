Storage capacity of phase-coded patterns in sparse neural networks
We study the storage of multiple phase-coded patterns as stable dynamical
attractors in recurrent neural networks with sparse connectivity. To determine
the synaptic strength of existent connections and store the phase-coded
patterns, we introduce a learning rule inspired to the spike-timing dependent
plasticity (STDP). We find that, after learning, the spontaneous dynamics of
the network replay one of the stored dynamical patterns, depending on the
network initialization. We study the network capacity as a function of
topology, and find that a small- world-like topology may be optimal, as a
compromise between the high wiring cost of long range connections and the
capacity increase.