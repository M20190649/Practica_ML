Recurrent Network Models Of Sequence Generation And Memory
Sequential activation of neurons is a common feature of network activity
during a variety of behaviors, including working memory and decision making.
Previous network models for sequences and memory emphasized specialized
architectures in which a principled mechanism is pre-wired into their
connectivity. Here we demonstrate that, starting from random connectivity and
modifying a small fraction of connections, a largely disordered recur- rent
network can produce sequences and implement working memory efficiently. We use
this process, called Partial In-Network Training (PINning), to model and match
cellular resolution imaging data from the posterior parietal cortex during a
virtual memory- guided two-alternative forced-choice task. Analysis of the
connectivity reveals that sequences propagate by the cooperation between
recurrent synaptic interactions and external inputs, rather than through
feedforward or asymmetric connections. Together our results suggest that neural
sequences may emerge through learning from largely unstructured network
architectures.