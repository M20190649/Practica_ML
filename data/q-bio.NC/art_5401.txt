Low-dimensional Embodied Semantics for Music and Language
Embodied cognition states that semantics is encoded in the brain as firing
patterns of neural circuits, which are learned according to the statistical
structure of human multimodal experience. However, each human brain is
idiosyncratically biased, according to its subjective experience history,
making this biological semantic machinery noisy with respect to the overall
semantics inherent to media artifacts, such as music and language excerpts. We
propose to represent shared semantics using low-dimensional vector embeddings
by jointly modeling several brains from human subjects. We show these
unsupervised efficient representations outperform the original high-dimensional
fMRI voxel spaces in proxy music genre and language topic classification tasks.
We further show that joint modeling of several subjects increases the semantic
richness of the learned latent vector spaces.