Interpreting and improving natural-language processing (in machines)
  with natural language-processing (in the brain)
Neural network models for NLP are typically implemented without the explicit
encoding of language rules and yet they are able to break one performance
record after another. Despite much work, it is still unclear what the
representations learned by these networks correspond to. We propose here a
novel approach for interpreting neural networks that relies on the only
processing system we have that does understand language: the human brain. We
use brain imaging recordings of subjects reading complex natural text to
interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE,
BERT and Transformer-XL. We study how their representations differ across layer
depth, context length, and attention type. Our results reveal differences in
the context-related representations across these models. Further, in the
transformer models, we find an interaction between layer depth and context
length, and between layer depth and attention type. We finally use the insights
from the attention experiments to alter BERT: we remove the learned attention
at shallow layers, and show that this manipulation improves performance on a
wide range of syntactic tasks. Cognitive neuroscientists have already begun
using NLP networks to study the brain, and this work closes the loop to allow
the interaction between NLP and cognitive neuroscience to be a true
cross-pollination.