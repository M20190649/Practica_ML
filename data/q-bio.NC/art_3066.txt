Subsampling scaling: a theory about inference from partly observed
  systems
In real-world applications, observations are often constrained to a small
fraction of a system. Such spatial subsampling can be caused by the
inaccessibility or the sheer size of the system, and cannot be overcome by
longer sampling. Spatial subsampling can strongly bias inferences about a
system's aggregated properties. To overcome the bias, we derive analytically a
subsampling scaling framework that is applicable to different observables,
including distributions of neuronal avalanches, of number of people infected
during an epidemic outbreak, and of node degrees. We demonstrate how to infer
the correct distributions of the underlying full system, how to apply it to
distinguish critical from subcritical systems, and how to disentangle
subsampling and finite size effects. Lastly, we apply subsampling scaling to
neuronal avalanche models and to recordings from developing neural networks. We
show that only mature, but not young networks follow power-law scaling,
indicating self-organization to criticality during development.