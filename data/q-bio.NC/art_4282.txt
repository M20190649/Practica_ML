From Dyson to Hopfield: Processing on hierarchical networks
We consider statistical-mechanical models for spin systems built on
hierarchical structures, which provide a simple example of non-mean-field
framework. We show that the coupling decay with spin distance can give rise to
peculiar features and phase diagrams much richer that their mean-field
counterpart. In particular, we consider the Dyson model, mimicking
ferromagnetism in lattices, and we prove the existence of a number of
meta-stabilities, beyond the ordered state, which get stable in the
thermodynamic limit. Such a feature is retained when the hierarchical structure
is coupled with the Hebb rule for learning, hence mimicking the modular
architecture of neurons, and gives rise to an associative network able to
perform both as a serial processor as well as a parallel processor, depending
crucially on the external stimuli and on the rate of interaction decay with
distance; however, those emergent multitasking features reduce the network
capacity with respect to the mean-field counterpart. The analysis is
accomplished through statistical mechanics, graph theory, signal-to-noise
technique and numerical simulations in full consistency. Our results shed light
on the biological complexity shown by real networks, and suggest future
directions for understanding more realistic models.