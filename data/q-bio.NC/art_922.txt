Linking connectivity, dynamics and computations in low-rank recurrent
  neural networks
Large scale neural recordings have established that the transformation of
sensory stimuli into motor outputs relies on low-dimensional dynamics at the
population level, while individual neurons exhibit complex selectivity.
Understanding how low-dimensional computations on mixed, distributed
representations emerge from the structure of the recurrent connectivity and
inputs to cortical networks is a major challenge. Here, we study a class of
recurrent network models in which the connectivity is a sum of a random part
and a minimal, low-dimensional structure. We show that, in such networks, the
dynamics are low dimensional and can be directly inferred from connectivity
using a geometrical approach. We exploit this understanding to determine
minimal connectivity required to implement specific computations, and find that
the dynamical range and computational capacity quickly increase with the
dimensionality of the connectivity structure. This framework produces testable
experimental predictions for the relationship between connectivity,
low-dimensional dynamics and computational features of recorded neurons.