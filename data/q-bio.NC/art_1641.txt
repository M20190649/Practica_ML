Do Deep Neural Networks Model Nonlinear Compositionality in the Neural
  Representation of Human-Object Interactions?
Visual scene understanding often requires the processing of human-object
interactions. Here we seek to explore if and how well Deep Neural Network (DNN)
models capture features similar to the brain's representation of humans,
objects, and their interactions. We investigate brain regions which process
human-, object-, or interaction-specific information, and establish
correspondences between them and DNN features. Our results suggest that we can
infer the selectivity of these regions to particular visual stimuli using DNN
representations. We also map features from the DNN to the regions, thus linking
the DNN representations to those found in specific parts of the visual cortex.
In particular, our results suggest that a typical DNN representation contains
encoding of compositional information for human-object interactions which goes
beyond a linear combination of the encodings for the two components, thus
suggesting that DNNs may be able to model this important property of biological
vision.