Repeated sequential learning increases memory capacity via effective
  decorrelation in a recurrent neural network
Memories in neural system are shaped through the interplay of neural and
learning dynamics under external inputs. By introducing a simple local learning
rule to a neural network, we found that the memory capacity is drastically
increased by sequentially repeating the learning steps of input-output
mappings. The origin of this enhancement is attributed to the generation of a
Psuedo-inverse correlation in the connectivity. This is associated with the
emergence of spontaneous activity that intermittently exhibits neural patterns
corresponding to embedded memories. Stablization of memories is achieved by a
distinct bifurcation from the spontaneous activity under the application of
each input.