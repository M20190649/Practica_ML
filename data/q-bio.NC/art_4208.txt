Zipf's law and criticality in multivariate data without fine-tuning
The joint probability distribution of many degrees of freedom in biological
systems, such as firing patterns in neural networks or antibody sequence
composition in zebrafish, often follow Zipf's law, where a power law is
observed on a rank-frequency plot. This behavior has recently been shown to
imply that these systems reside near to a unique critical point where the
extensive parts of the entropy and energy are exactly equal. Here we show
analytically, and via numerical simulations, that Zipf-like probability
distributions arise naturally if there is an unobserved variable (or variables)
that affects the system, e. g. for neural networks an input stimulus that
causes individual neurons in the network to fire at time-varying rates. In
statistics and machine learning, these models are called latent-variable or
mixture models. Our model shows that no fine-tuning is required, i.e. Zipf's
law arises generically without tuning parameters to a point, and gives insight
into the ubiquity of Zipf's law in a wide range of systems.