How Information Transfer works: interpretation of Information Contents
  in Bayes Theorem. Understanding Negative Information
In a given space of models or hypothesis the individual information content
of each of them is considered as opposed to the Shannon entropy that measures
the average information content of the mentioned space. In particular
expressing Bayes Theorem in terms of the information contents associated to its
probabilities allows understanding how bits of information, introduced in the
system by an observation, are transferred to each of the models in the space.
It is shown how, from a single observation not one, but two causal information
sources are generated: the Information Content Associated to the Evidence that
always introduces positive information, and the Information Content Associated
to the Bayes Likelihood that always introduces negative bits; therefore the
evidence contributes to increase the probability of occurrence of the model and
the likelihood to decrease it; depending on the net value of the difference
between these two mentioned information contents, the information that arrives
to a given model will be positive or negative. Thus, we propose a novel metric,
given by the difference of the two mentioned information contents called
transfer information content which measures the information transferred to each
of the single models in the space. The resolution of the Monty Hall Problem
(MHP) and some of its variants in the Information Theory framework proposed
allows to confirm the validity of the formulas derived and to understand the
counterintuitive and theoretically problematic concept of negative information.
The implications of the concepts introduced in terms of information transfer to
the emergent field of Local Information Dynamics, to Computational Neuroscience
(particularly to Directed Information Theory and Neural Coding) are proposed as
further work.