Computational principles of biological memory
Memories are stored, retained, and recollected through complex, coupled
processes operating on multiple timescales. To understand the computational
principles behind these intricate networks of interactions we construct a broad
class of synaptic models that efficiently harnesses biological complexity to
preserve numerous memories. The memory capacity scales almost linearly with the
number of synapses, which is a substantial improvement over the square root
scaling of previous models. This was achieved by combining multiple dynamical
processes that initially store memories in fast variables and then
progressively transfer them to slower variables. Importantly, the interactions
between fast and slow variables are bidirectional. The proposed models are
robust to parameter perturbations and can explain several properties of
biological memory, including delayed expression of synaptic modifications,
metaplasticity, and spacing effects.