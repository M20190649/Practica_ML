Neurons as an Information-theoretic Engine
We show that dynamical gain modulation of neurons' stimulus response is
described as an information-theoretic cycle that generates entropy associated
with the stimulus-related activity from entropy produced by the modulation. To
articulate this theory, we describe stimulus-evoked activity of a neural
population based on the maximum entropy principle with constraints on two types
of overlapping activities, one that is controlled by stimulus conditions and
the other, termed internal activity, that is regulated internally in an
organism. We demonstrate that modulation of the internal activity realises gain
control of stimulus response, and controls stimulus information. A cycle of
neural dynamics is then introduced to model information processing by the
neurons during which the stimulus information is dynamically enhanced by the
internal gain-modulation mechanism. Based on the conservation law for entropy
production, we demonstrate that the cycle generates entropy ascribed to the
stimulus-related activity using entropy supplied by the internal mechanism,
analogously to a heat engine that produces work from heat. We provide an
efficient cycle that achieves the highest entropic efficiency to retain the
stimulus information. The theory allows us to quantify efficiency of the
internal computation and its theoretical limit.