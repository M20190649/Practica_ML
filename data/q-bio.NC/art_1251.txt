Time Divergence-Convergence Learning Scheme in Multi-Layer Dynamic
  Synapse Neural Networks
A new learning scheme called time divergence-convergence (TDC) is proposed
for two-layer dynamic synapse neural networks (DSNN). DSNN is an artificial
neural network model, in which the synaptic transmission is modeled by a
dynamic process and the information between neurons are transmitted through
spike timing. In TDC, the intra-layer neurons of a DSNN are trained to map
input spike trains to a higher dimension of spike trains called a
feature-domain, and the output neurons are trained to build the desired spike
trains by processing the spike timing of intralayer neurons. The DSNN
performance was examined in a jittered spike train classification task which
shows more than 92\% accuracy in classifying different spike trains. The DSNN
performance is comparable with the recurrent multi-layer neural networks and
surpasses a single-layer DSNN with a 22\% margin. Synaptic dynamics have been
proposed as the neural substrate for sub-second temporal processing; we can
utilize TDC to train a DSNN to perform diverse forms of sub-second temporal
processing. The TDC learning proposed here is scalable in terms of the synaptic
adaptation of deeper layers of multi-layer DSNNs. The DSNN along with TDC
learning proposed here can be used in to replicate the processing observed in
neural circuitry and in pattern recognition tasks.