A deep learning-inspired model of the hippocampus as storage device of
  the brain extended dataset
The standard model of memory consolidation foresees that memories are
initially recorded in the hippocampus, while features that capture higher-level
generalisations of data are created in the cortex, where they are stored for a
possibly indefinite period of time. Computer scientists have sought inspiration
from nature to build machines that exhibit some of the remarkable properties
present in biological systems. One of the results of this effort is represented
by artificial neural networks, a class of algorithms that represent the state
of the art in many artificial intelligence applications. In this work, we
reverse the inspiration flow and use the experience obtained from neural
networks to gain insight into the design of brain architecture and the
functioning of memory. Our starting observation is that neural networks learn
from data and need to be exposed to each data record many times during
learning: this requires the storage of the entire dataset in computer memory.
Our thesis is that the same holds true for the brain and the main role of the
hippocampus is to store the "brain dataset", from which high-level features are
learned and encoded in cortical neurons.