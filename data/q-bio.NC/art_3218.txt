Inference of topology and the nature of synapses, and the flow of
  information in neuronal networks
The characterisation of neuronal connectivity is one of the most important
matters in neuroscience. In this work, we show that a recently proposed
informational quantity, the causal mutual information, employed with an
appropriate methodology, can be used not only to correctly infer the direction
of the underlying physical synapses, but also to identify their excitatory or
inhibitory nature, considering easy to handle and measure bivariate
time-series. The success of our approach relies on a surprising property found
in neuronal networks by which non-adjacent neurons do "understand" each other
(positive mutual information), however this exchange of information is not
capable of causing effect (zero transfer entropy). Remarkably, inhibitory
connections, responsible for enhancing synchronisation, transfer more
information than excitatory connections, known to enhance entropy in the
network. We also demonstrate that our methodology can be used to correctly
infer directionality of synapses even in the presence of dynamic and
observational Gaussian noise, and is also successful in providing the effective
directionality of inter modular connectivity, when only mean fields can be
measured.