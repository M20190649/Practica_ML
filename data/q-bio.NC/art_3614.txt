Learning to solve the credit assignment problem
Backpropagation is driving today's artificial neural networks (ANNs).
However, despite extensive research, it remains unclear if the brain implements
this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms
are often seen as a realistic alternative: neurons can randomly introduce
change, and use unspecific feedback signals to observe their effect on the cost
and thus approximate their gradient. However, the convergence rate of such
learning scales poorly with the number of involved neurons (e.g. O(N)). Here we
propose a hybrid learning approach. Each neuron uses an RL-type strategy to
learn how to approximate the gradients that backpropagation would provide -- in
this way it learns to learn. We provide proof that our approach converges to
the true gradient for certain classes of networks. In both feed-forward and
recurrent networks, we empirically show that our approach learns to approximate
the gradient, and can match the performance of gradient-based learning.
Learning to learn provides a biologically plausible mechanism of achieving good
performance, without the need for precise, pre-specified learning rules.