Neural mechanism to simulate a scale-invariant future timeline
Predicting future events, and their order, is important for efficient
planning. We propose a neural mechanism to non-destructively translate the
current state of memory into the future, so as to construct an ordered set of
future predictions. This framework applies equally well to translations in time
or in one-dimensional position. In a two-layer memory network that encodes the
Laplace transform of the external input in real time, translation can be
accomplished by modulating the weights between the layers. We propose that
within each cycle of hippocampal theta oscillations, the memory state is swept
through a range of translations to yield an ordered set of future predictions.
We operationalize several neurobiological findings into phenomenological
equations constraining translation. Combined with constraints based on physical
principles requiring scale-invariance and coherence in translation across
memory nodes, the proposition results in Weber-Fechner spacing for the
representation of both past (memory) and future (prediction) timelines. The
resulting expressions are consistent with findings from phase precession
experiments in different regions of the hippocampus and reward systems in the
ventral striatum. The model makes several experimental predictions that can be
tested with existing technology.