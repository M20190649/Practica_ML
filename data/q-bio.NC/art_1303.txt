Inferring Mesoscale Models of Neural Computation
Recent years have seen dramatic progress in the development of techniques for
measuring the activity and connectivity of large populations of neurons in the
brain. However, as these techniques grow ever more powerful---allowing us to
even contemplate measuring every neuron in entire brain---a new problem arises:
how do we make sense of the mountains of data that these techniques produce?
Here, we argue that the time is ripe for building an intermediate or
"mesoscale" computational theory that can bridge between single-cell
(microscale) accounts of neural function and behavioral (macroscale) accounts
of animal cognition and environmental complexity. Just as digital accounts of
computation in conventional computers abstract away the non-essential dynamics
of the analog circuits that implementing gates and registers, so too a
computational account of animal cognition can afford to abstract from the
non-essential dynamics of neurons. We argue that the geometry of neural
circuits is essential in explaining the computational limitations and
technological innovations inherent in biological information processing. We
propose a blueprint for how to employ tools from modern machine learning to
automatically infer a satisfying mesoscale account of neural computation that
combines functional and structural data, with an emphasis on learning and
exploiting regularity and repeating motifs in neuronal circuits. Rather than
suggest a specific theory, we present a new class of scientific instruments
that can enable neuroscientists to design, propose, implement and test
mesoscale theories of neural computation.