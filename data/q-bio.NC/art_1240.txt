One-shot learning and behavioral eligibility traces in sequential
  decision making
In many daily tasks we make multiple decisions before reaching a goal. In
order to learn such sequences of decisions, a mechanism to link earlier actions
to later reward is necessary. Reinforcement learning theory suggests two
classes of algorithms solving this credit assignment problem: In classic
temporal-difference learning, earlier actions receive reward information only
after multiple repetitions of the task, whereas models with eligibility traces
reinforce entire sequences of actions from a single experience (one-shot). Here
we asked whether humans use eligibility traces. We developed a novel paradigm
to directly observe which actions and states along a multi-step sequence are
reinforced after a single reward. By focusing our analysis on those states for
which RL with and without eligibility trace make qualitatively distinct
predictions, we find direct behavioral (choice probability) and physiological
(pupil dilation) signatures of reinforcement learning with eligibility trace
across multiple sensory modalities.