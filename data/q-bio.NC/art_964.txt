Models of Innate Neural Attractors and Their Applications for Neural
  Information Processing
In this work we reveal and explore a new class of attractor neural networks,
based on inborn connections provided by model molecular markers, the molecular
marker based attractor neural networks (MMBANN). We have explored conditions
for the existence of attractor states, critical relations between their
parameters and the spectrum of single neuron models, which can implement the
MMBANN. Besides, we describe functional models (perceptron and SOM) which
obtain significant advantages, while using MMBANN. In particular, the
perceptron based on MMBANN, gets specificity gain in orders of error
probabilities values, MMBANN SOM obtains real neurophysiological meaning, the
number of possible grandma cells increases 1000- fold with MMBANN. Each set of
markers has a metric, which is used to make connections between neurons
containing the markers. The resulting neural networks have sets of attractor
states, which can serve as finite grids for representation of variables in
computations. These grids may show dimensions of d = 0, 1, 2,... We work with
static and dynamic attractor neural networks of dimensions d = 0 and d = 1. We
also argue that the number of dimensions which can be represented by attractors
of activities of neural networks with the number of elements N=104 does not
exceed 8.