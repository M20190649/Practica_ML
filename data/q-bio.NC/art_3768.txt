Thinking about the brain
We all are fascinated by the phenomena of intelligent behavior, as generated
both by our own brains and by the brains of other animals. As physicists we
would like to understand if there are some general principles that govern the
structure and dynamics of the neural circuits that underlie these phenomena. At
the molecular level there is an extraordinary universality, but these
mechanisms are surprisingly complex. This raises the question of how the brain
selects from these diverse mechanisms and adapts to compute "the right thing"
in each context. One approach is to ask what problems the brain really solves.
There are several examples - from the ability of the visual system to count
photons on a dark night to our gestalt recognition of statistical tendencies
toward symmetry in random patterns - where the performance of the system in
fact approaches some fundamental physical or statistical limits. This suggests
that some sort of optimization principles may be at work, and there are
examples where these principles have been formulated clearly and generated
predictions which are confirmed in new experiments; a central theme in this
work is the matching of the coding and computational strategies of the brain to
the statistical structure of the world around us. Extension of these principles
to the problem of learning leads us into interesting theoretical questions
about how to measure the complexity of the data from which we learn and the
complexity of the models that we use in learning, as well as opening some new
opportunities for experiment. This combination of theoretical and experimental
work gives us some new (if still speculative) perspectives on classical
problems and controversies in cognition.